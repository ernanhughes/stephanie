# config/scorer/hf_hrm/default.yaml
name: hf_hrm
model_type: hf
display_name: TinyLlama-1.1B

model_version: v1
evaluator: hf
target_type: conversation_turn
dimensions:
  - reasoning
  - knowledge
  - clarity
  - faithfulness
  - coverage

# Choose a stronger model you can still run locally
# model_name: meta-llama/Llama-3.2-3B-Instruct
# model_alias: hf_hrm
model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
model_alias: hf_TinyLama

# device_map: auto
device_map: cuda
torch_dtype: float16
max_seq_len: 4096
ppl_range: [3.0, 30.0]
tokenizer_name: null
local_files_only: false
offline: true
low_cpu_mem_usage: true
use_compile: false
