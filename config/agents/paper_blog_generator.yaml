paper_blog_generator:
  name: paper_blog_generator
  enabled: true
  save_prompt: true
  save_context: true
  skip_if_completed: false

  # Where we read inputs from the pipeline context
  input_arxiv_key: arxiv_id
  graph_key: paper_graph
  sections_key: paper_sections
  clusters_key: concept_clusters
  documents_key: paper_documents
  report_key: paper_report_markdown

  # Where we write the final blog
  output_key: paper_blog_markdown

  blog:
    audience: "informed engineer"
    target_length_words: 2200
    max_sections: 16
    max_clusters: 6
    max_similar_papers: 4

    # Blog-specific LLM override
    model:
      name: "ollama/llama3.1:8b"
      api_base: "http://localhost:11434"
      api_key: null
      params:
        num_ctx: 16384
        top_p: 0.9
        top_k: 40
        repeat_penalty: 1.05

    # Extra per-call params (override / extend model.params)
    prompt_params:
      temperature: 0.65        # more creative prose
      max_tokens: 3072         # room for a decent-length post

  # Model block kept for consistency (PromptService will actually use its own cfg,
  # but this lets you switch things later if you want the agent to call async_call_llm).
  model:
    name: ollama/qwen3
    api_base: http://localhost:11434
    api_key: null
    params:
      temperature: 0.4
