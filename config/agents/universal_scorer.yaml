universal_scorer:
  name: universal_scorer
  enabled: true
  save_prompt: true
  save_context: false
  skip_if_completed: false

  force_rescore: true
  progress: true
  save_results: false
  include_mars: true
  enabled_scorers:
    - hrm
    - tiny
    - sicql
    # - svm
    # - mrq
    # - ebt
    # - contrastive_ranker

  include_ranking: true
  rank_top_k: 5

  dimensions: 
    - clarity
    - coverage
    - faithfulness
    - knowledge
    - reasoning

  target_types:
    - case_scorable

  dimension_config:
    alignment:
      scale: "0-100"
      trust_references: ["llm"]
      expected_variance: 0.15
    clarity:
      scale: "0-100"
      trust_references: ["llm"]
      expected_variance: 0.15
    implementability:
      scale: "0-100"
      trust_references: ["llm"]
      expected_variance: 0.15
    novelty:
      scale: "0-100"
      trust_references: ["llm"]
      expected_variance: 0.15
    relevance:
      scale: "0-100"
      trust_references: ["llm"]
      expected_variance: 0.15

  model:
    name: ollama/qwen3
    api_base: http://localhost:11434
    api_key: null
  input_keys: ["goal", "hypotheses"]
  input_key: documents
  output_key: universal_scorer
  prompt_mode: file
  prompt_file: universal_scorer.txt

  processor:
    _target_: stephanie.scoring.metrics.scorable_processor.ScorableProcessor

    # Inline is simplest; you can flip to async/rpc later
    offload_mode: inline
    enable_manifest: false
    enable_item_progress: false

    # IMPORTANT: for cohort building we want fresh scores on each row so that
    # metrics_columns/values are populated even if source scorables were bare.
    attach_scores: true
    require_metrics_for_vpm: false

    # ---- per-row features (run on each scorable) ----
    features:
      - metrics
      - frontier_lens  # light per-row summary; the heavy lift happens in visicalc_group

    feature_configs:
      metrics:
        enabled: true
        # Keep all four so we don't accidentally drop Tiny again
        scorers: ["hrm", "sicql", "tiny", "ebt"]
        dimensions: [coverage, reasoning, knowledge, clarity, faithfulness]
        persist: false          # compute on the fly; DB persistence optional
        attach_scores: true

      frontier_lens:
        enabled: true
        # Preferred metric ordering (optional)
        metric_keys:
          - "HRM.coverage.score"
          - "sicql.coverage.score"
          - "tiny.coverage.score"
          - "svm.coverage.score"
        frontier_metric: "HRM.aggregate"
        row_region_splits: 4
        frontier_low: 0.25
        frontier_high: 0.75

        # Case-insensitive mapping (MetricMapper: ignore_case=True by default in your code)
        metric_mapping:
          include: ["HRM.*", "sicql.*", "tiny.*", "ebt Hello.*"]
          exclude: ["*.raw", "*.debug"]

        # Optional tiny previews (heavy VPM happens in group feature)
        vpm_png:
          enabled: true
          mode: "L"
          per_metric_normalize: true
          target_file: "visicalc_targeted_vpm.png"
          baseline_file: "visicalc_baseline_vpm.png"

        # Row-level hints only (cohort selection done in group step)
        top_k_metrics: 50
        min_effect: 0.25
        importance_filter:
          enabled: true
          path: "config/core_metrics.json"
          top_k: 100
          min_effect: 0.1

    # ---- group features (run once after all rows are built) ----
    group_feature_configs:
      metric_filter:
        enabled: true
        top_k: 100
        normalize: true
        include: ["HRM.*", "sicql.*", "Tiny.*"]
        exclude: ["*.raw", "*.debug"]
        alias_strip: true
        short_circuit_if_locked: true
        include_visicalc_core: false
        visicalc_core_names:
        always_include:
          - "tiny.aggregate"
          - "svm.aggregate"
          - "Tiny.faithfulness.attr.scm.aggregate01"

      # 2) Cohort VisiCalc (heavy batch) + feature lock
      frontier_lens_group:
        enabled: true
        # Runs cohort analysis on the filtered metric matrix; attaches per-row
        # visicalc_features / visicalc_feature_names / visicalc_report / visicalc_quality
        episode_id: "critic:${hydra:runtime.run_dir}"  # any unique tag is fine
        frontier_metric: "Tiny.faithfulness.attr.scm.aggregate01"
        row_region_splits: 4
        frontier_low: 0.25
        frontier_high: 0.75
        per_metric_normalize: true
        metric_mapping:
          include: ["HRM.*", "sicql.*", "tiny.*", "svm.*"]
          exclude: ["*.raw", "*.debug"]
        # files written under runs/visicalc/<run_id> by your tool
        out_dir: "runs/critic/scorer"
        json_file: "visicalc_report.json"
        csv_file: "visicalc_report.csv"
