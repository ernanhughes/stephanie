
"co_ai\agents\ats\__init__.py"
---START-OF-FILE---
---END-OF-FILE---


"co_ai\agents\ats\agentic_tree_search.py"
---START-OF-FILE---
# co_ai/agents/ats/agentic_tree_search.py

from typing import List, Optional, Tuple, Dict
from co_ai.agents.ats.solution_node import SolutionNode
from co_ai.agents.base_agent import BaseAgent
import time
import random

class AgenticTreeSearch:
    def __init__(
        self,
        agent: BaseAgent,
        max_iterations: int = 500,
        time_limit: int = 86400,  # 24 hours
        N_init: int = 5,
        H_debug: float = 0.8,
        H_greedy: float = 0.8,
    ):
        self.agent = agent
        self.tree: List[SolutionNode] = []
        self.max_iterations = max_iterations
        self.time_limit = time_limit
        self.N_init = N_init
        self.H_debug = H_debug
        self.H_greedy = H_greedy
        self.iteration = 0
        self.start_time = time.time()
        self.best_node: Optional[SolutionNode] = None

    async def run(self, context: dict):
        task_description = context.get("goal", {}).get("goal_text")
        knowledge = context.get("knowledge", [])

        while not self._should_stop():
            action, parent_node = self.select_action()
            new_plan = await self.generate_plan(task_description, parent_node, action, knowledge)
            new_code = self.generate_code(new_plan)
            result = self.execute_code(new_code)

            verification = self.verify_output(result)
            new_node = SolutionNode(
                plan=new_plan,
                code=new_code,
                metric=verification["metric"],
                output=result.get("stdout"),
                summary=verification["summary"],
                parent_id=parent_node.id if parent_node else None,
                is_buggy=verification["is_bug"]
            )

            self.tree.append(new_node)
            self.update_best_node(new_node)
            self.iteration += 1

        # Final submission
        best_solution = self.get_best_solution()
        context["final_solution"] = best_solution.to_dict()
        return context

    def _should_stop(self) -> bool:
        if self.iteration >= self.max_iterations:
            return True
        if time.time() - self.start_time > self.time_limit:
            return True
        return False

    def select_action(self) -> Tuple[str, Optional[SolutionNode]]:
        """
        Implements Algorithm 1 from AutoMind paper
        Returns (action_type, parent_node)
        """
        if len([n for n in self.tree if "draft" in n.plan]) < self.N_init:
            return "draft", None

        buggy_nodes = [n for n in self.tree if n.is_buggy]
        valid_nodes = [n for n in self.tree if not n.is_buggy and n.metric is not None]

        if random.random() < self.H_debug and buggy_nodes:
            return "debug", random.choice(buggy_nodes)

        if self.best_node and random.random() < self.H_greedy:
            return "improve", self.best_node

        if valid_nodes:
            return "improve", random.choice(valid_nodes)

        return "draft", None

    async def generate_plan(self, task_description, parent_node, action, knowledge):
        # Use prompt templates defined below
        pass

    def generate_code(self, plan):
        # Decide one-pass vs stepwise based on complexity score
        pass

    def execute_code(self, code):
        # Run in sandboxed environment
        pass

    def verify_output(self, output):
        # Check for bugs, overfitting, submission file
        pass

    def update_best_node(self, node: SolutionNode):
        if node.metric is not None:
            if self.best_node is None or node.metric > self.best_node.metric:
                self.best_node = node

    def get_best_solution(self) -> Optional[SolutionNode]:
        return self.best_node---END-OF-FILE---


"co_ai\agents\ats\code_executor.py"
---START-OF-FILE---
# co_ai/agents/ats/code_executor.py

from typing import Optional, Dict, Any
from co_ai.agents.base_agent import BaseAgent


class CodeExecutor:
    def __init__(self, agent: BaseAgent):
        self.agent = agent

    async def score_complexity(self, task_description: str, plan: str) -> float:
        prompt = f"""
        Rate the complexity of this task and plan on a scale of 1â€“5.
        Task: {task_description}
        Plan: {plan}
        """
        response = await self.agent.llm(prompt)
        try:
            return float(response.strip())
        except:
            return 3.0

    async def one_pass_codegen(self, plan: str) -> str:
        prompt = f"""
        Generate Python code for the following plan:
        {plan}
        """
        response = await self.agent.llm(prompt)
        return response.strip()

    async def stepwise_codegen(self, plan: str) -> str:
        prompt = f"""
        Break the following plan into steps:
        {plan}
        """
        response = await self.agent.llm(prompt)
        steps = response.get("decomposed_steps", [])
        full_code = ""
        for step in steps:
            code = await self.one_pass_codegen(step["details"])
            full_code += code + "\n"
        return full_code---END-OF-FILE---


"co_ai\agents\ats\output_verifier.py"
---START-OF-FILE---
# co_ai/agents/ats/output_verifier.py

from typing import Optional


class OutputVerifier:
    def verify(self, output: str, has_submission_file: bool) -> dict[str, any]:
        is_bug = "Error" in output or "Exception" in output
        is_overfitting = "val_loss increasing" in output.lower()
        metric = self.extract_metric(output)

        return {
            "is_bug": is_bug,
            "is_overfitting": is_overfitting,
            "has_csv_submission": has_submission_file,
            "metric": metric,
            "summary": self.summarize(output)
        }

    def extract_metric(self, output: str) -> Optional[float]:
        # Implement logic to parse metric value from logs
        return 0.85  # Example

    def summarize(self, output: str) -> str:
        # Summarize key findings from output
        return "Model trained successfully."---END-OF-FILE---


"co_ai\agents\ats\plan_generator.py"
---START-OF-FILE---
# co_ai/agents/ats/plan_generator.py

from typing import Optional, Dict, Any
from co_ai.agents.base_agent import BaseAgent


class PlanGenerator:
    def __init__(self, agent: BaseAgent):
        self.agent = agent

    async def draft_plan(self, task_description: str, knowledge: list) -> str:
        prompt = f"""
        You are an expert machine learning engineer.
        Create a detailed solution plan for this task:
        {task_description}

        Some relevant tricks from past solutions:
        {" ".join(knowledge)}

        Output only the plan as natural language text.
        """
        response = await self.agent.llm(prompt)
        return response.strip()

    async def improve_plan(self, previous_plan: str, feedback: str, knowledge: list) -> str:
        prompt = f"""
        Improve this ML solution plan:
        {previous_plan}

        Feedback: {feedback}
        Additional knowledge: {" ".join(knowledge)}

        Output only the improved plan.
        """
        response = await self.agent.llm(prompt)
        return response.strip()

    async def debug_plan(self, previous_plan: str, error_log: str) -> str:
        prompt = f"""
        Fix this buggy ML solution plan:
        {previous_plan}

        Error log: {error_log}
        Output only the corrected plan.
        """
        response = await self.agent.llm(prompt)
        return response.strip()---END-OF-FILE---


"co_ai\agents\ats\solution_node.py"
---START-OF-FILE---
# co_ai/agents/ats/solution_node.py

from typing import Optional, Dict, Any
import time


class SolutionNode:
    def __init__(
        self,
        plan: str,
        code: Optional[str] = None,
        metric: Optional[float] = None,
        output: Optional[str] = None,
        summary: Optional[str] = None,
        parent_id: Optional[str] = None,
        is_buggy: bool = False,
        timestamp: float = None
    ):
        self.id = hash(self)
        self.plan = plan
        self.code = code
        self.metric = metric
        self.output = output
        self.summary = summary
        self.parent_id = parent_id
        self.is_buggy = is_buggy
        self.timestamp = timestamp or time.time()

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "plan": self.plan,
            "code": self.code,
            "metric": self.metric,
            "output": self.output,
            "summary": self.summary,
            "parent_id": self.parent_id,
            "is_buggy": self.is_buggy,
            "timestamp": self.timestamp
        }

    @classmethod
    def from_dict(cls, data: dict) -> "SolutionNode":
        return cls(**data)


---END-OF-FILE---


"co_ai\agents\knowledge\__init__.py"
---START-OF-FILE---
from .automind_knowledge_collector import AutoMindKnowledgeCollector

---END-OF-FILE---


"co_ai\agents\knowledge\automind_knowledge_collector.py"
---START-OF-FILE---
# co_ai/agents/knowledge/automind_knowledge_collector.py

from typing import List, Dict
from co_ai.agents.base_agent import BaseAgent
from co_ai.tools import WebSearchTool, WikipediaTool
from co_ai.tools.arxiv_tool import search_arxiv
from co_ai.tools.huggingface_tool import search_huggingface_datasets
from co_ai.tools.cos_sim_tool import get_top_k_similar


LABEL_HIERARCHY = {
    "Computer Vision": ["Image Classification", "Object Detection", "Segmentation"],
    "NLP": ["Text Classification", "NER", "Summarization"],
    "Tabular Data": ["Classification", "Regression", "Anomaly Detection"],
    "Graph Learning": ["Node Classification", "Link Prediction"]
}


class AutoMindKnowledgeCollector:
    def __init__(self, agent: BaseAgent):
        self.agent = agent
        self.memory = agent.memory
        self.logger = agent.logger
        self.cfg = agent.cfg

    async def collect_papers(self, query: str) -> List[Dict]:
        context = {
            "goal": {
                "id": "paper_search",
                "goal_text": query,
                "goal_type": "model_review"
            },
            "search_queries": [{"goal_text": query}]
        }
        result = await self.agent.run(context)
        return result.get("search_results", [])

    async def collect_kaggle_solutions(self, task_description: str) -> List[Dict]:
        query = f"top {task_description} kaggle solution"
        context = {
            "goal": {
                "id": "kaggle_search",
                "goal_text": query,
                "goal_type": "data_search"
            },
            "search_queries": [{"goal_text": query}]
        }
        result = await self.agent.run(context)
        return result.get("search_results", [])

    def assign_labels_to_document(self, doc_title: str, doc_summary: str) -> List[str]:
        combined_text = f"{doc_title} {doc_summary}".lower()
        matched_labels = []

        for category, subcategories in LABEL_HIERARCHY.items():
            if any(kw in combined_text for kw in category.lower().split()):
                matched_labels.append(category)
                for subcat in subcategories:
                    if any(kw in combined_text for kw in subcat.lower().split()):
                        matched_labels.append(subcat)

        # Fallback using similarity
        if not matched_labels:
            all_labels = [label for cat in LABEL_HIERARCHY.values() for label in cat]
            top = get_top_k_similar(combined_text, all_labels, self.memory, top_k=2)
            matched_labels = [label for label, _ in top]

        return list(set(matched_labels))

    async def retrieve_knowledge(self, task_description: str) -> List[Dict]:
        papers = await self.collect_papers(task_description)
        kaggle_tricks = await self.collect_kaggle_solutions(task_description)
        all_docs = papers + kaggle_tricks

        labeled_docs = [
            {**doc, "labels": self.assign_labels_to_document(doc["title"], doc["summary"])}
            for doc in all_docs
        ]

        # Filter by relevance to task description
        relevant_docs = self.filter_by_similarity(task_description, labeled_docs)

        # Re-rank by label priority
        reranked_docs = self.rerank_by_label_priority(relevant_docs)

        return reranked_docs

    def filter_by_similarity(self, query: str, docs: List[Dict]) -> List[Dict]:
        titles_and_summaries = [f"{doc['title']} {doc['summary']}" for doc in docs]
        scores = get_top_k_similar(query, titles_and_summaries, self.memory, top_k=len(docs))
        ranked_indices = [i for i, _ in scores]
        return [docs[i] for i in ranked_indices]

    def rerank_by_label_priority(self, docs: List[Dict]) -> List[Dict]:
        label_priority = {
            "Computer Vision": 5,
            "NLP": 5,
            "Tabular Data": 4,
            "Image Classification": 3,
            "Text Classification": 3,
            "Classification": 2
        }

        def score_doc(doc):
            return sum(label_priority.get(label, 0) for label in doc.get("labels", []))

        return sorted(docs, key=lambda x: score_doc(x), reverse=True)---END-OF-FILE---


"co_ai\agents\mixins\prompt_evolver_mixin.py"
---START-OF-FILE---
from co_ai.compiler.prompt_evolver import PromptEvolver


class PromptEvolverMixin:
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.prompt_evolver = None  # Will be initialized on first use

    def init_evolver(self, llm, logger=None):
        self.prompt_evolver = PromptEvolver(llm, logger=logger)

    def evolve_prompts(self, examples: list[dict], context: dict = {}, sample_size: int = 10) -> list[str]:
        return self.prompt_evolver.evolve(examples, context=context, sample_size=sample_size)
---END-OF-FILE---


"co_ai\agents\mixins\scoring_mixin.py"
---START-OF-FILE---
from co_ai.analysis.score_evaluator import ScoreEvaluator
from co_ai.scoring.base_evaluator import BaseEvaluator


class ScoringMixin:
    """
    A generic scoring mixin that supports dynamic, stage-aware evaluation using ScoreEvaluator.

    Supports any configured scoring stage (e.g., review, reasoning, reflection).
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._scorers = {}  # Caches ScoreEvaluator instances per stage

    def get_scorer(self, stage: str) -> ScoreEvaluator:
        """
        Lazily loads and returns a ScoreEvaluator for the given stage.
        Config path is read from e.g., cfg['review_score_config'].
        """
        if stage not in self._scorers:
            config_key = f"{stage}_score_config"
            config_path = self.cfg.get(config_key, f"config/scoring/{stage}.yaml")
            self._scorers[stage] = ScoreEvaluator.from_file(
                filepath=config_path,
                prompt_loader=self.prompt_loader,
                cfg=self.cfg,
                logger=self.logger,
                memory=self.memory
            )
        return self._scorers[stage]

    def score_hypothesis(
        self,
        hypothesis: dict,
        context: dict,
        metrics: str = "review",
        evaluator: BaseEvaluator = None,
    ) -> dict:
        """
        Score a hypothesis for a given evaluation stage.

        Args:
            hypothesis:
            hyp (dict): Hypothesis object with a "text" key.
            context (dict): Pipeline context, must include 'goal'.
            metrics (str): Evaluation metrics (e.g., "review", "reasoning", "reflection").
            evaluator (callable): Optional evaluator override (e.g., a parser function).

        Returns:
            dict: {
                "id": hypothesis_id,
                "score": float,
                "scores": {dimension_name: {score, rationale, weight}, ...},
                "metrics": metrics
            }
        """
        if evaluator:
            result = evaluator.evaluate(hypothesis, context)
            final_score = result["score"]
            dimension_scores = result.get("dimensions", {
                metrics: {
                    "score": final_score,
                    "weight": 1.0,
                    "rationale": result.get("rationale", "")
                }
            })
        else:
            scorer = self.get_scorer(metrics)
            dimension_scores = scorer.evaluate(
                hypothesis=hypothesis,
                context=context,
                llm_fn=self.call_llm
            )

        weighted_total = sum(
            s["score"] * s.get("weight", 1.0)
            for s in dimension_scores.values()
        )
        weight_sum = sum(s.get("weight", 1.0) for s in dimension_scores.values())
        final_score = round(weighted_total / weight_sum, 2) if weight_sum > 0 else 0.0

        self.logger.log("HypothesisScoreComputed", {
            "score": final_score,
            "dimension_scores": dimension_scores,
            "hypothesis": hypothesis,
            "metrics": metrics
        })

        return {
            "id": hypothesis.get("id"),
            "score": final_score,
            "scores": dimension_scores,
            "metrics": metrics
        }
---END-OF-FILE---


"co_ai\agents\__init__.py"
---START-OF-FILE---
"""
Agents responsible for core reasoning steps:
- base
- generation
- reflection
- ranking
- evolution
- meta review
- proximity
- debate
- literature
- generic
- refiner
"""
from .base import BaseAgent
from .debate import DebateAgent
from .dots_planner import DOTSPlannerAgent
from .evolution import EvolutionAgent
from .generation import GenerationAgent
from .generic import GenericAgent
from .judge import JudgeAgent
from .literature import LiteratureAgent
from .lookahead import LookaheadAgent
from .meta_review import MetaReviewAgent
from .proximity import ProximityAgent
from .ranking import RankingAgent
from .refiner import RefinerAgent
from .reflection import ReflectionAgent
from .sharpening import SharpeningAgent
---END-OF-FILE---


"co_ai\agents\adaptive_reasoner.py"
---START-OF-FILE---
from typing import Union

from co_ai.agents import BaseAgent
from co_ai.constants import GOAL
from co_ai.dataloaders import ARMDataLoader
from co_ai.evaluator import ARMReasoningSelfEvaluator, LLMJudgeEvaluator


class AdaptiveReasonerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        self.modes = ["adaptive", "instruction_guided", "consensus_guided"]
        self.mode = self.cfg.get("mode", "adaptive")
        self.format_list = self.cfg.get(
            "format_list", ["direct", "short_cot", "code", "long_cot"]
        )
        self.judge = self._init_judge()

    async def run(self, context: dict):
        goal = context.get(GOAL) 


        self.judge.train_from_database(goal.get("goal_text"), self.cfg)

        prompt = goal.get("goal_text")

        response = ""
        if self.mode == "instruction_guided":
            format_name = self.cfg.get("format", "long_cot")
            response = self._generate_with_format(format_name, context)
        elif self.mode == "consensus_guided":
            response = self._run_consensus_mode(context)
        else:  # default to adaptive
            response = self._run_adaptive_mode(prompt, context)

        self.logger.log("AdaptiveReasoningResponse", response)

        context[self.output_key] = response
        return context

    def _generate_with_format(self, fmt, context):
        prompt = self.prompt_loader.from_file(fmt, self.cfg, context)
        response = self.call_llm(prompt, context)
        return {
            "prompt": prompt,
            "response": response,
            "format_used": ARMDataLoader.detect_format(response) or fmt,
        }

    def _run_consensus_mode(self, context:dict):
        outputs = {}
        for fmt in ["direct", "short_cot", "code"]:
            outputs[fmt] = self._generate_with_format(fmt, context)["response"]

        responses = list(outputs.values())
        unique_responses = set(responses)

        if len(unique_responses) == 1:
            return {
                "response": responses[0],
                "format": "consensus-simple",
                "source_formats": list(outputs.keys()),
            }
        else:
            long_cot_response = self._generate_with_format("long_cot", context)
            return {
                "response": long_cot_response["response"],
                "format": "long_cot",
                "source_formats": list(outputs.keys()),
                "fallback_reason": "no_consensus",
            }

    def _run_adaptive_mode(self, prompt:str, context:dict) -> dict[str, Union[str, float]]:
        prioritized_formats = ["direct", "short_cot", "code", "long_cot"]

        scores = {}
        for fmt in prioritized_formats:
            dict_response = self._generate_with_format(fmt, context)
            response = dict_response["response"]
            base_score = self.judge.score(prompt, response)

            token_len = len(response.split())
            rarity_bonus = 1.0 / (1 + self.judge.format_freq.get(fmt, 0))

            final_score = base_score - 0.01 * token_len + rarity_bonus
            scores[fmt] = final_score
            self.judge._update_format_stats(fmt, final_score)

        best_format = max(scores, key=scores.get)
        chosen_response = self._generate_with_format(best_format, context)
        # Log decision
        self.logger.log(
            "AdaptiveModeDecision",
            {"goal": prompt, "scores": scores, "chosen": best_format},
        )

        return {
            "response": chosen_response,
            "format_used": best_format,
            "scores": scores,
        }

    def get_format_for_goal(self, goal: dict):
        if "preferred_format" in goal:
            return goal["preferred_format"]
        goal_type = goal.get("goal_type", "default")
        if goal_type == "math":
            return "code"
        elif goal_type == "commonsense":
            return "short_cot"
        else:
            return "long_cot"

    def _get_prioritized_formats(self, context:dict):
        if "preferred_format" in context:
            return [context["preferred_format"]]

        priority_map = self.cfg.get("format_priority_by_difficulty", {})
        difficulty = context.get("difficulty", "default").lower()
        return priority_map.get(difficulty, priority_map.get("default", ["long_cot"]))

    def _init_judge(self):
        judge_strategy = self.cfg.get("judge", "mrq")
        if judge_strategy == "llm":
            llm = self.cfg.get("judge_model", self.cfg.get("model"))
            prompt_file = self.cfg.get(
                "judge_prompt_file", "judge_pairwise_comparison.txt"
            )
            self.logger.log(
                "EvaluatorInit", {"strategy": "LLM", "prompt_file": prompt_file}
            )
            return LLMJudgeEvaluator(
                self.cfg, llm, prompt_file, self.call_llm, self.logger
            )
        else:
            self.logger.log("EvaluatorInit", {"strategy": "ARM"})
            return ARMReasoningSelfEvaluator(self.cfg, self.memory, self.logger)

---END-OF-FILE---


"co_ai\agents\auto_tuner.py"
---START-OF-FILE---
from datetime import datetime

from co_ai.agents.base_agent import BaseAgent
from co_ai.analysis.rule_effect_analyzer import RuleEffectAnalyzer
from co_ai.constants import PIPELINE_RUN_ID
from co_ai.models import SymbolicRuleORM


class AutoTunerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.eval_threshold = cfg.get("eval_threshold", 50)
        self.max_rules = cfg.get("max_rules_to_consider", 10)
        self.tune_mode = cfg.get("tune_mode", False)  # Only write changes if True

    async def run(self, context: dict) -> dict:
        self.logger.log("AutoTunerStart", {"tune_mode": self.tune_mode})

        analyzer = RuleEffectAnalyzer(self.memory.session, logger=self.logger)
        summary = analyzer.analyze(context.get(PIPELINE_RUN_ID))

        underperforming_rules = [
            (rule_id, data) for rule_id, data in summary.items()
            if data.get("avg_score", 100) < self.eval_threshold
        ]
        underperforming_rules = sorted(underperforming_rules, key=lambda x: x[1]["avg_score"])[:self.max_rules]

        for rule_id, data in underperforming_rules:
            rule = self.memory.symbolic_rules.get_by_id(rule_id)
            if not rule:
                continue

            self.logger.log("RuleUnderperforming", {
                "rule_id": rule_id,
                "avg_score": data["avg_score"],
                "context_hash": rule.context_hash,
                "attributes": rule.attributes
            })

            suggestions = self.suggest_rule_edits(rule, data)
            for suggestion in suggestions:
                self.logger.log("AutoRuleSuggestion", {
                    "rule_id": rule_id,
                    "suggested_attributes": suggestion,
                    "reason": "AutoTuner based on score analysis"
                })

                if self.tune_mode:
                    new_rule = SymbolicRuleORM(
                        target=rule.target,
                        filter=rule.filter,
                        attributes=suggestion,
                        source="auto_tuner",
                        created_at=datetime.utcnow(),
                        context_hash=SymbolicRuleORM.compute_context_hash(suggestion, rule.filter),
                        description=f"Auto-tuned from rule {rule_id}"
                    )
                    self.memory.symbolic_rules.insert(new_rule)
                    self.logger.log("AutoRuleInserted", {"new_rule_id": new_rule.id})

        self.logger.log("AutoTunerEnd", {"rules_checked": len(underperforming_rules)})
        return context

    def suggest_rule_edits(self, rule: SymbolicRuleORM, data: dict) -> list[dict]:
        """
        Heuristic placeholder: try tweaking `temperature`, `max_tokens`, `adapter`, etc.
        Could be replaced by LLM-based or learned tuner later.
        """
        original = rule.attributes or {}

        candidates = []

        if "temperature" in original:
            try:
                temp = float(original["temperature"])
                new_temp = round(min(temp + 0.2, 1.0), 2)
                candidates.append({**original, "temperature": new_temp})
            except:
                pass

        if "adapter" in original:
            candidates.append({**original, "adapter": "default"})

        if "max_tokens" in original:
            try:
                new_tokens = max(int(original["max_tokens"]) - 100, 100)
                candidates.append({**original, "max_tokens": new_tokens})
            except:
                pass

        # Default fallback: add a hint flag
        candidates.append({**original, "hint": "reviewed by tuner"})

        return candidates
---END-OF-FILE---


"co_ai\agents\automind.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.components.coding_strategy import SelfAdaptiveCoder
from co_ai.components.search_policy import TreeSearchPolicy
from co_ai.agents.judge import JudgeAgent
from co_ai.memory.embedding_store import EmbeddingStore
from co_ai.analysis.score_evaluator import ScoreEvaluator
from co_ai.agents.ats.solution_node import SolutionNode
from co_ai.components.solution_tree import SolutionTree


class AutoMindAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        self.embedding_store = EmbeddingStore(cfg.embedding)
        self.judge = JudgeAgent(cfg.judge) if cfg.judge else None
        self.scorer = ScoreEvaluator(cfg.scoring)
        self.tree = SolutionTree()
        self.policy = TreeSearchPolicy(cfg.policy)
        self.coder = SelfAdaptiveCoder(cfg.coder)
        self.max_iters = cfg.get("max_iters", 25)

    def run(self, goal):
        self.logger.log({"goal": goal})
        self.tree.initialize(goal)

        for step in range(self.max_iters):
            parent_node, action = self.policy.select(self.tree)

            if action == "draft":
                plan = self._generate_plan(goal)
            elif action == "improve":
                plan = self._improve_plan(parent_node)
            elif action == "debug":
                plan = self._debug_plan(parent_node)
            else:
                continue

            code = self.coder.generate_code(plan)
            output, metric = self._execute(code)

            valid = self.judge.validate(plan, code, output, metric) if self.judge else True
            node = SolutionNode(plan, code, metric, output, valid)
            self.tree.add_node(node)

            self.logger.log({"step": step, "plan": plan, "code": code, "metric": metric, "valid": valid})

        best = self.tree.get_best()
        return best.code

    def _generate_plan(self, goal):
        context = self._retrieve_knowledge(goal)
        return self.prompt_loader.render("draft_plan", goal=goal, context=context)

    def _improve_plan(self, node):
        context = self._retrieve_knowledge(node.plan)
        return self.prompt_loader.render("improve_plan", plan=node.plan, output=node.output, context=context)

    def _debug_plan(self, node):
        return self.prompt_loader.render("debug_plan", plan=node.plan, output=node.output)

    def _retrieve_knowledge(self, query):
        return self.embedding_store.query(query, top_k=3)

    def _execute(self, code):
        try:
            # Placeholder: execute code in sandboxed environment
            metric, output = 0.0, "Execution output"
        except Exception as e:
            return str(e), None
        return output, metric
---END-OF-FILE---


"co_ai\agents\base.py"
---START-OF-FILE---
# co_ai/agents/base.py
import random
import re
from abc import ABC, abstractmethod
from collections import defaultdict
from datetime import datetime, timezone

import litellm

from co_ai.constants import (AGENT, API_BASE, API_KEY, BATCH_SIZE, CONTEXT,
                             GOAL, HYPOTHESES, INPUT_KEY, MODEL, NAME,
                             OUTPUT_KEY, PROMPT_MATCH_RE, PROMPT_PATH,
                             SAVE_CONTEXT, SAVE_PROMPT, SOURCE, STRATEGY)
from co_ai.logs import JSONLogger
from co_ai.prompts import PromptLoader
from co_ai.rules import SymbolicRuleApplier
from co_ai.models import PromptORM


def remove_think_blocks(text: str) -> str:
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()


class BaseAgent(ABC):
    def __init__(self, cfg, memory=None, logger=None):
        self.cfg = cfg
        agent_key = self.__class__.__name__.replace(AGENT, "").lower()
        self.name = cfg.get(NAME, agent_key)
        self.memory = memory
        self.logger = logger or JSONLogger()
        self.rule_applier = SymbolicRuleApplier(cfg, memory, logger)
        self.model_config = cfg.get(MODEL, {})
        self.prompt_loader = PromptLoader(memory=self.memory, logger=self.logger)
        self.prompt_match_re = cfg.get(PROMPT_MATCH_RE, "")
        self.llm = self.init_llm()  # TODO do we need to init here?
        self.strategy = cfg.get(STRATEGY, "default")
        self.model_name = self.llm.get(NAME, "")
        self.source = cfg.get(SOURCE, CONTEXT)
        self.batch_size = cfg.get(BATCH_SIZE, 6)
        self.save_context = cfg.get(SAVE_CONTEXT, False)
        self.input_key = cfg.get(INPUT_KEY, HYPOTHESES)
        self.preferences = cfg.get("preferences", {})
        self.remove_think = cfg.get("remove_think", True)
        self.output_key = cfg.get(OUTPUT_KEY, self.name)
        self._goal_id_cache = {}
        self._prompt_id_cache = {}
        self._hypothesis_id_cache = {}
        self.logger.log(
            "AgentInitialized",
            {
                "agent_key": agent_key,
                "class": self.__class__.__name__,
                "config": self.cfg,
            },
        )

    def init_llm(self, cfg=None):
        config = cfg or self.cfg
        model_cfg = config.get(MODEL, {})
        required_keys = [NAME, API_BASE]
        for key in required_keys:
            if key not in model_cfg:
                self.logger.log(
                    "MissingLLMConfig", {"agent": self.name, "missing_key": key}
                )
        return {
            NAME: model_cfg.get(NAME),
            API_BASE: model_cfg.get(API_BASE),
            API_KEY: model_cfg.get(API_KEY),
        }
    
    def get_or_save_prompt(self, prompt_text: str, context:dict) -> PromptORM:
        prompt = self.memory.prompt.get_from_text(prompt_text)
        if prompt is None:
            self.memory.prompt.save(
                context.get("goal"),
                agent_name=self.name,
                prompt_key=self.cfg.get(PROMPT_PATH, ""),
                prompt_text=prompt_text,
                strategy=self.cfg.get(STRATEGY, ""),
                pipeline_run_id=context.get("pipeline_run_id"),
                version=self.cfg.get("version", 1),
            )
            prompt = self.memory.prompt.get_from_text(prompt_text)
        if prompt is None:
            raise ValueError(
                f"Please check this prompe: {prompt_text}. "
                "Ensure it is saved before use."
            )
        return prompt

    def call_llm(self, prompt: str, context: dict, llm_cfg: dict = None) -> str:
        updated_cfg = self.rule_applier.apply_to_prompt(self.cfg, context)
        if self.llm is None:
            # ðŸ” Apply rules here (now that goal is known)
            updated_cfg = self.rule_applier.apply_to_agent(self.cfg, context)
            self.llm = self.init_llm(cfg=updated_cfg)  # initialize with updated config

        """Call the default or custom LLM, log the prompt, and handle output."""
        props = llm_cfg or self.llm  # Use passed-in config or default

        agent_name = self.name

        strategy = updated_cfg.get(STRATEGY, "")
        prompt_key = updated_cfg.get(PROMPT_PATH, "")
        use_memory_for_fast_prompts = updated_cfg.get(
            "use_memory_for_fast_prompts", True
        )

        # ðŸ” Check cache
        if self.memory and use_memory_for_fast_prompts:
            previous = self.memory.prompt.find_similar_prompt(
                agent_name=agent_name, prompt_text=prompt, strategy=strategy, similarity_threshold=0.8
            )
            if previous:
                chosen = random.choice(previous)
                cached_response = chosen.get("response_text")
                self.logger.log(
                    "LLMCacheHit",
                    {
                        "agent": agent_name,
                        "strategy": strategy,
                        "prompt_key": prompt_key,
                        "cached": True,
                        "count": len(previous),
                        "emoji": "ðŸ“¦ðŸ”ðŸ’¬",
                    },
                )
                return cached_response

        messages = [{"role": "user", "content": prompt}]
        try:
            response = litellm.completion(
                model=props[NAME],
                messages=messages,
                api_base=props[API_BASE],
                api_key=props.get(API_KEY, ""),
            )
            output = response["choices"][0]["message"]["content"]

            # Save prompt and response if enabled
            if updated_cfg.get(SAVE_PROMPT, False) and self.memory:
                self.memory.prompt.save(
                    context.get("goal"),
                    agent_name=self.name,
                    prompt_key=updated_cfg.get(PROMPT_PATH, ""),
                    prompt_text=prompt,
                    response=output,
                    strategy=updated_cfg.get(STRATEGY, ""),
                    pipeline_run_id=context.get("pipeline_run_id"),
                    version=updated_cfg.get("version", 1),
                )

            # Remove [THINK] blocks if configured
            response_cleaned = (
                remove_think_blocks(output) if self.remove_think else output
            )

            # Optionally add to context history
            if updated_cfg.get("add_prompt_to_history", True):
                self.add_to_prompt_history(
                    context, prompt, {"response": response_cleaned}
                )

            return response_cleaned

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("LLMCallError", {"exception": str(e)})
            raise

    @abstractmethod
    async def run(self, context: dict) -> dict:
        pass

    def add_to_prompt_history(self, context: dict, prompt: str, metadata: dict = None):
        """
        Appends a prompt record to the context['prompt_history'] under the agent's name.

        Args:
            context (dict): The context dict to modify
            prompt (str): prompt to store
            metadata (dict): any extra info
        """
        if "prompt_history" not in context:
            context["prompt_history"] = {}
        if self.name not in context["prompt_history"]:
            context["prompt_history"][self.name] = []
        entry = {
            "prompt": prompt,
            "agent": self.name,
            "preferences": self.preferences,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
        if metadata:
            entry.update(metadata)
        context["prompt_history"][self.name].append(entry)

    def get_hypotheses(self, context: dict) -> list[dict]:
        try:
            if self.source == "context":
                hypothesis_dicts = context.get(self.input_key, [])
                if not hypothesis_dicts:
                    self.logger.log("NoHypothesesInContext", {"agent": self.name})
                return hypothesis_dicts

            elif self.source == "database":
                goal = context.get(GOAL)
                hypotheses = self.get_hypotheses_from_db(goal.get("goal_text"))
                if not hypotheses:
                    self.logger.log(
                        "NoHypothesesInDatabase", {"agent": self.name, "goal": goal}
                    )
                return [h.to_dict() for h in hypotheses] if hypotheses else []

            else:
                self.logger.log(
                    "InvalidSourceConfig", {"agent": self.name, "source": self.source}
                )
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log(
                "HypothesisFetchError",
                {"agent": self.name, "source": self.source, "error": str(e)},
            )

        return []

    def get_hypotheses_from_db(self, goal_text: str):
        return self.memory.hypotheses.get_latest(goal_text, self.batch_size)

    @staticmethod
    def extract_goal_text(goal):
        return goal.get("goal_text") if isinstance(goal, dict) else goal

    def get_goal_id(self, goal: dict):
        if not isinstance(goal, dict):
            raise ValueError(
                f"Expected goal to be a dict, got {type(goal).__name__}: {goal}"
            )
        goal_text = goal.get("goal_text", "")
        if goal_text in self._goal_id_cache:
            return self._goal_id_cache[goal_text][0]
        goal = self.memory.goals.get_from_text(goal_text)
        self._goal_id_cache[goal_text] = (goal.id, goal)
        return goal.id

    def get_hypothesis_id(self, hypothesis_dict: dict):
        if not isinstance(hypothesis_dict, dict):
            raise ValueError(
                f"Expected hypothesis_text to be a dict, got {type(hypothesis_dict).__name__}: {hypothesis_dict}"
            )
        text = hypothesis_dict.get("text")
        if text in self._hypothesis_id_cache:
            return self._hypothesis_id_cache[text][0]
        hypothesis = self.memory.hypotheses.get_from_text(text)
        self._hypothesis_id_cache[text] = (hypothesis.id, hypothesis)
        return hypothesis.id


    def _log_timing_diagram(self):
        """Log timing breakdown as Mermaid diagram"""
        timing_data = self.logger.get_logs_by_type("FunctionTiming")
        function_times = defaultdict(float)
        
        for log in timing_data:
            key = f"{log['class']}.{log['function']}"
            function_times[key] += log["duration_ms"]

        mermaid = ["```mermaid\ngraph TD"]
        total = sum(function_times.values())
        
        for func, duration in function_times.items():
            percent = (duration / total) * 100
            mermaid.append(f"A{func.replace('.', '_')}[{func} | {percent:.1f}%]")
        
        mermaid.append("```")
        self.logger.log("TimingBreakdown", {"diagram": "\n".join(mermaid)})


    def _analyze_performance(self):
        """Print performance summary"""
        from tabulate import tabulate
        
        timing_logs = self.logger.get_logs_by_type("FunctionTiming")
        function_times = defaultdict(list)
        for log in timing_logs:
            data = log["data"]
            key = f"{data['class']}.{data['function']}"
            function_times[key].append(data["duration_ms"])
        
        table = []
        for key, durations in function_times.items():
            table.append([
                key,
                f"{sum(durations)/len(durations):.2f}ms",
                len(durations),
                f"{max(durations):.2f}ms"
            ])
        
        print("\nâ±ï¸ Performance Breakdown")
        print(tabulate(table, headers=["Function", "Avg Time", "Calls", "Max Time"]))---END-OF-FILE---


"co_ai\agents\compiler_optimizer.py"
---START-OF-FILE---
import statistics
from collections import defaultdict

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL, PIPELINE_RUN_ID


class CompilerOptimizerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.score_threshold = cfg.get("score_threshold", 5.0)  # optional tuning param

    async def run(self, context: dict) -> dict:
        pipeline_run_id = context.get(PIPELINE_RUN_ID)
        goal = context.get(GOAL)

        # Step 1: Fetch all hypotheses and their scores for this run
        hypotheses = self.memory.hypotheses.get_by_pipeline_run(pipeline_run_id)
        all_scores = self.memory.evaluations.get_by_pipeline_run(pipeline_run_id)

        # Group scores by prompt_id
        prompt_scores = defaultdict(list)
        for score in all_scores:
            if score.prompt_id:
                prompt_scores[score.prompt_id].append(score)

        summary = []

        # Step 2: Analyze performance per prompt
        for prompt_id, scores in prompt_scores.items():
            prompt = self.memory.prompt.get(prompt_id)
            raw_scores = [s.score for s in scores if s.score is not None]

            if not raw_scores:
                continue

            avg = statistics.mean(raw_scores)
            std = statistics.stdev(raw_scores) if len(raw_scores) > 1 else 0
            count = len(raw_scores)
            high_score_rate = sum(s >= self.score_threshold for s in raw_scores) / count

            summary.append({
                "prompt_id": prompt_id,
                "prompt_text": prompt.prompt_text[:100] if prompt else "<unknown>",
                "avg_score": avg,
                "std_dev": std,
                "count": count,
                "high_score_rate": round(high_score_rate * 100, 2)
            })

        # Step 3: Log or save insights
        top_prompts = sorted(summary, key=lambda x: x["avg_score"], reverse=True)[:5]
        self.logger.log("CompilerOptimizerSummary", {
            "goal": goal.get("goal_text"),
            "top_prompts": top_prompts,
            "pipeline_run_id": pipeline_run_id
        })

        print("\n=== Top Performing Compiled Prompts ===")
        for i, p in enumerate(top_prompts):
            print(f"[{i+1}] Avg Score: {p['avg_score']:.2f} | Used {p['count']}x | Prompt: {p['prompt_text']}")

        # Step 4 (future): Update strategy weights, rules, DSPy prior preferences

        context["compiler_optimization_summary"] = summary
        return context
---END-OF-FILE---


"co_ai\agents\cot_dspy_generator.py"
---START-OF-FILE---
"""
ChainOfThoughtDSPyGeneratorAgent

This agent uses DSPy to generate a chain-of-thought reasoning trace in response to a given research goal or question.
It loads a goal from the context, uses a DSPy Module configured with a local Ollama model (e.g., qwen3), and returns
a structured reasoning output. The result is stored as a hypothesis and linked to the goal, prompt, and pipeline run
for later evaluation or training (e.g., via MR.Q or LLM-based scoring).

Key features:
- DSPy integration with structured signature (CoTGenerationSignature)
- Local model inference via Ollama (e.g., qwen3)
- Hypothesis logging and memory storage (via HypothesisORM)
- Prompt metadata management and linking
- Designed for use in self-improving Co AI pipelines

Intended to be paired with self-judging evaluators and symbolic optimizers.
"""

from abc import ABC, abstractmethod

import dspy
from dspy import InputField, OutputField, Signature

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import (GOAL, GOAL_TEXT, PIPELINE, PIPELINE_RUN_ID,
                             PROMPT_PATH, STRATEGY)
from co_ai.models import HypothesisORM

# DSPy signature for generating Chains of Thought
class CoTGenerationSignature(Signature):
    question = InputField(desc="A scientific or reasoning question")
    references = InputField(desc="Optional reference material to inform the reasoning")
    preferences = InputField(desc="Optional reasoning preferences or style constraints")
    answer = OutputField(desc="Chain-of-thought reasoning that addresses the question")


class CoTGeneratorModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generator = dspy.Predict(CoTGenerationSignature)

    def forward(self, question, references="", preferences=""):
        return self.generator(
            question=question, references=references, preferences=preferences
        )


# Simple evaluation result class to return from evaluator
class EvaluationResult:
    def __init__(self, score: float, reason: str):
        self.score = score
        self.reason = reason


# Base evaluator interface (not used directly, but useful for future extensions)
class BaseEvaluator(ABC):
    @abstractmethod
    def evaluate(
        self, original: str, proposal: str, metadata: dict = None
    ) -> EvaluationResult:
        pass


# Main agent class responsible for training and tuning prompts using DSPy
class ChainOfThoughtDSPyGeneratorAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        # Setup DSPy
        lm = dspy.LM(
            "ollama_chat/qwen3",
            api_base="http://localhost:11434",
            api_key="",
        )
        dspy.configure(lm=lm)

        self.module = CoTGeneratorModule()

    async def run(self, context: dict):
        goal = context.get(GOAL)
        references = context.get("references", "")
        preferences = context.get("preferences", "")

        result = self.module(
            question=goal.get("goal_text"), references=references, preferences=preferences
        )

        cot = result.answer.strip()
        self.logger.log("CoTGenerated", {"goal": goal, "cot": cot})

        prompt_text = goal.get(GOAL_TEXT)
        prompt = self.get_or_save_prompt(prompt_text, context)
        goal_id = self.get_goal_id(goal)
        hyp = HypothesisORM(
            goal_id=goal_id,
            prompt_id=prompt.id,
            text=cot,
            features={"source": "cot_dspy"},
            pipeline_signature=context.get(PIPELINE),
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
        )
        self.memory.hypotheses.insert(hyp)

        context[self.output_key] = [hyp.to_dict()]
        return context
---END-OF-FILE---


"co_ai\agents\cot_generator.py"
---START-OF-FILE---
from co_ai.agents import BaseAgent
from co_ai.analysis.rubric_classifier import RubricClassifierMixin
from co_ai.constants import GOAL, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator.llm_judge_evaluator import LLMJudgeEvaluator
from co_ai.evaluator.mrq_self_evaluator import MRQSelfEvaluator
from co_ai.models import HypothesisORM


class ChainOfThoughtGeneratorAgent(BaseAgent, RubricClassifierMixin): 
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)
        self.logger.log("AgentInit", {"agent": "ChainOfThoughtGeneratorAgent"})
        self.evaluator = self._init_evaluator()
        self.num_candidates = cfg.get("num_candidates", 2)

    async def run(self, context: dict):
        goal = context.get(GOAL)
        self.logger.log("AgentRunStarted", {"goal": goal})

        if isinstance(self.evaluator, MRQSelfEvaluator):
            self.logger.log("MRQTraining", {"type": "MRQ"})
            self.evaluator.train_from_database(goal=goal.goal_text, cfg=self.cfg)

        prompt_text = self.prompt_loader.load_prompt(self.cfg, context)
        self.logger.log("PromptGenerated", {"prompt": prompt_text[:200]})

        # Step 1: Generate candidates
        self.logger.log("GenerationStarted", {"num_candidates": self.num_candidates})
        candidates = [
            self.call_llm(prompt_text, context) for _ in range(self.num_candidates)
        ]
        self.logger.log(
            "GenerationCompleted", {"candidates": [c[:100] for c in candidates]}
        )

        # Step 2: Evaluate pairwise
        best = candidates[0]
        scores = {}
        for candidate in candidates[1:]:
            best, scores = self.evaluator.judge(
                prompt=prompt_text,
                output_a=best,
                output_b=candidate, 
                context=context
            )
        self.logger.log("EvaluationCompleted", {"best_output": best[:100], **scores})

        # Step 3: Store hypothesis and patterns
        value_a = scores.get("value_a", 0)
        value_b = scores.get("value_b", 0)
        score = max(value_a, value_b)
        features = {
            "prompt": prompt_text,
            "best_output": best,
            "candidates": candidates,
        }

        prompt = self.get_or_save_prompt(prompt_text, context)
 
        best_orm = HypothesisORM(
            goal_id=self.get_goal_id(goal),
            text=best,
            confidence=score,
            features=features,
            prompt_id=prompt.id,
            pipeline_signature=context.get(PIPELINE),
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
        )
        self.memory.hypotheses.insert(best_orm)
        self.logger.log("HypothesisStored", {"text": best[:100], "confidence": score})

        self.classify_and_store_patterns(
            hypothesis=best_orm.to_dict(),
            context=context,
            prompt_loader=self.prompt_loader,
            cfg=self.cfg,
            memory=self.memory,
            logger=self.logger,
            agent_name=self.name,
            score=score,
        )

        context[self.output_key] = [best_orm.to_dict()]
        self.logger.log("AgentRunCompleted", {"output_key": self.output_key})
        return context

    def _init_evaluator(self):
        if self.cfg.get("evaluator", "mrq") == "llm":
            llm = self.cfg.get("evaluator_model", self.cfg.get("model"))
            prompt_file = self.cfg.get("evaluator_prompt_file", "evaluation.txt")
            self.logger.log(
                "EvaluatorInit", {"strategy": "LLM", "prompt_file": prompt_file}
            )
            return LLMJudgeEvaluator(
                self.cfg, llm, prompt_file, self.call_llm, self.logger
            )
        else:
            self.logger.log("EvaluatorInit", {"strategy": "MRQ"})
            return MRQSelfEvaluator(self.memory, self.logger)

    def _summarize_pattern(self, pattern: dict):
        stats = {}
        for dimension, label in pattern.items():
            if label not in stats:
                stats[label] = 0
            stats[label] += 1
        return stats
---END-OF-FILE---


"co_ai\agents\debate.py"
---START-OF-FILE---
# co_ai/agents/debate.py
from co_ai.agents.base_agent import BaseAgent


class OptimistDebater(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        reviews = []

        for h in hypotheses:
            prompt = (
                f"As an optimistic analyst, critique the following hypothesis:\n\n"
                f"{h}\n\n"
                f"Focus on strengths, positive implications, and reasons it might be valid."
            )
            review = self.call_llm(prompt, context)
            reviews.append({"hypotheses": h, "review": review, "persona": "Optimist"})

        return {"reviews": reviews}

class SkepticDebater(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        reviews = []

        for h in hypotheses:
            prompt = (
                f"As a skeptical analyst, critique the following hypothesis:\n\n"
                f"{h}\n\n"
                f"Focus on weaknesses, uncertainties, or reasons it might be flawed."
            )
            review = self.call_llm(prompt, context)
            reviews.append({"hypotheses": h, "review": review, "persona": "Skeptic"})

        return {"reviews": reviews}

class BalancedDebater(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        reviews = []

        for h in hypotheses:
            prompt = (
                f"As a balanced analyst, critique the following hypothesis:\n\n"
                f"{h}\n\n"
                f"Provide both positive and negative aspects."
            )
            review = self.call_llm(prompt, context)
            reviews.append({"hypotheses": h, "review": review, "persona": "Balanced"})

        return {"reviews": reviews}
    
class DebateAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.optimist = OptimistDebater(cfg, memory, logger)
        self.skeptic = SkepticDebater(cfg, memory, logger)
        self.balanced = BalancedDebater(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        optimist_reviews = await self.optimist.run({"hypotheses": hypotheses})
        skeptic_reviews = await self.skeptic.run({"hypotheses": hypotheses})
        balanced_reviews = await self.balanced.run({"hypotheses": hypotheses})

        return {
            "optimist_reviews": optimist_reviews,
            "skeptic_reviews": skeptic_reviews,
            "balanced_reviews": balanced_reviews
        }
    
---END-OF-FILE---


"co_ai\agents\dots_planner.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL, STRATEGY
from co_ai.utils.goal_classifier import classify_goal_strategy  # See below


class DOTSPlannerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy_map = cfg.get("strategy_routes", {})
        self.default_strategy = cfg.get("default_strategy", "default")

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        strategy = classify_goal_strategy(goal)

        pipeline = self.strategy_map.get(strategy, self.strategy_map[self.default_strategy])

        context["strategy"] = strategy
        context["suggested_pipeline"] = pipeline

        self.logger.log("DOTSPlanGenerated", {
            "strategy": strategy,
            "pipeline": pipeline
        })

        return context
---END-OF-FILE---


"co_ai\agents\evolution.py"
---START-OF-FILE---
# co_ai/agents/evolution.py
import itertools
import re

import numpy as np

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import (EVOLVED, GOAL, HYPOTHESES, PIPELINE,
                             PIPELINE_RUN_ID, RANKING)
from co_ai.models import HypothesisORM
from co_ai.tools.embedding_tool import get_embedding


class EvolutionAgent(BaseAgent):
    """
    The Evolution Agent refines hypotheses iteratively using several strategies:

    - Grafting similar hypotheses into unified statements
    - Feasibility improvement through LLM reasoning
    - Out-of-the-box hypothesis generation
    - Inspiration from top-ranked ideas
    - Simplification and clarity enhancement

    These improvements are based on the paper:
    "The Evolution agent continuously refines and improves existing hypotheses..."
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.use_grafting = cfg.get("use_grafting", False)
        self.preferences = cfg.get("preferences", ["novelty", "feasibility"])

    async def run(self, context: dict) -> dict:
        """
        Evolve top-ranked hypotheses individually.

        Args:
            context: Dictionary with keys:
                - ranked: list of (hypotheses, score) tuples
                - hypotheses: list of unranked hypotheses (fallback)
                - preferences: override criteria for refinement
        """
        ranked = context.get(RANKING, [])
        fallback_hypotheses = context.get(HYPOTHESES, [])
        preferences = context.get("preferences", self.preferences)

        # Decide which hypotheses to evolve
        if ranked:
            top_texts = [hyp for hyp, _ in ranked[:3]]
        elif fallback_hypotheses:
            top_texts = fallback_hypotheses
        else:
            self.logger.log(
                "NoHypothesesToEvolve", {"reason": "no_ranked_or_unranked_input"}
            )
            context[EVOLVED] = []
            return context

        evolved = []
        for h in top_texts:
            try:
                prompt = self.prompt_loader.load_prompt(
                    {**self.cfg, **{HYPOTHESES: h}}, context
                )
                raw_output = self.call_llm(prompt, context)
                refined_list = self.extract_hypothesis(raw_output)
                self.logger.log(
                    "EvolvedParsedHypotheses",
                    {"raw_response_snippet": raw_output[:300], "parsed": refined_list},
                )

                if refined_list:
                    for r in refined_list:
                        goal = context.get(GOAL)
                        hyp = HypothesisORM(
                            goal=goal,
                            text=h,
                            pipeline_signature=context.get(PIPELINE),
                            pipeline_run_id=context.get(PIPELINE_RUN_ID),
                        )
                        self.memory.hypotheses.insert(hyp)
                        evolved.append(r)
                else:
                    self.logger.log(
                        "EvolutionFailed",
                        {"original": h[:100], "response_snippet": raw_output[:200]},
                    )

            except Exception as e:
                print(f"âŒ Exception: {type(e).__name__}: {e}")
                self.logger.log(
                    "EvolutionError", {"error": str(e), "hypotheses": h}
                )

        context["evolved"] = evolved
        self.logger.log(
            "EvolutionCompleted",
            {"evolved_count": len(evolved), "preferences": preferences},
        )

        return context

    async def graft_similar(self, context: dict, threshold: float = 0.90) -> list[str]:
        """
        Graft pairs of highly similar hypotheses into unified versions.
        """
        hypotheses = self.get_hypotheses(context)
        # TODO: use memory
        embeddings = [get_embedding(h, self.cfg) for h in hypotheses]
        used = set()
        grafted = []

        for (i, h1), (j, h2) in itertools.combinations(enumerate(hypotheses), 2):
            if i in used or j in used:
                continue
            sim = self.cosine_similarity(embeddings[i], embeddings[j])
            if sim >= threshold:
                self.logger.log(
                    "GraftingPair",
                    {"similarity": sim, "h1": h1[:60] + "...", "h2": h2[:60] + "..."},
                )
                prompt = (
                    f"Combine the following hypotheses into a clearer, unified statement:\n\n"
                    f"A: {h1}\nB: {h2}"
                )
                graft = self.call_llm(prompt, context)
                grafted.append(graft)
                used.update([i, j])

        # Add ungrafted hypotheses back
        hypotheses = context.get(HYPOTHESES, [])
        for i, h in enumerate(hypotheses):
            if i not in used:
                grafted.append(h)

        return grafted

    def cosine_similarity(self, vec1, vec2):
        """Compute cosine similarity between two vectors."""
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

    def extract_hypothesis(self, text: str) -> list[str]:
        """
        Extracts hypothesis from markdown-style output with '**Hypothesis:**' section.
        Returns a list of simplified hypotheses.
        """
        pattern = re.compile(r"\*\*Hypothesis:\*\*\s*(.*?)\n", re.IGNORECASE)
        matches = pattern.findall(text)

        if len(matches) == 0:
            return [text]
        return [match.strip() for match in matches if match.strip()]
---END-OF-FILE---


"co_ai\agents\general_reasoner.py"
---START-OF-FILE---
from itertools import combinations
from typing import Optional

from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin
from co_ai.analysis.rubric_classifier import RubricClassifierMixin
from co_ai.constants import GOAL, GOAL_TEXT, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator import LLMJudgeEvaluator, MRQSelfEvaluator
from co_ai.models import EvaluationORM, HypothesisORM
from co_ai.prompts import PromptLoader


class GeneralReasonerAgent(ScoringMixin, RubricClassifierMixin, BaseAgent):
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)

        self.logger.log("AgentRunStarted", {"goal": goal})

        # Generate hypotheses (if needed)
        if self.cfg.get("thinking_mode") == "generate_and_judge":
            hypotheses = self.generate_hypotheses(context)
        else:
            hypotheses = self.get_hypotheses(context)

        context["hypotheses"] = hypotheses
        context["scoring"] = []

        dimension_scores = []
        for hyp in hypotheses:
            scored = self.score_hypothesis(hyp, context, metrics="reasoning_cor")
            hyp["final_score"] = scored["score"]
            hyp["dimension_scores"] = scored["scores"]
            dimension_scores.append(scored)
        context["dimension_scores"] = dimension_scores


        best_hypothesis = max(hypotheses, key=lambda h: h["final_score"])
        
        # Classify with rubrics and store pattern stats
        pattern = self.classify_with_rubrics(
            hypothesis=best_hypothesis,
            context=context,
            prompt_loader=self.prompt_loader,
            cfg=self.cfg,
            logger=self.logger
        )


        summarized = self._summarize_pattern(pattern)
        context["pattern"] = summarized

        pattern_stats = self.generate_pattern_stats(
            goal=goal,
            hypothesis=best_hypothesis,
            pattern_dict=summarized,
            cfg=self.cfg,
            agent_name=self.name,
            confidence_score=best_hypothesis.get("confidence")
        )

        self.memory.pattern_stats.insert(pattern_stats)
        context["pattern_stats"] = summarized

        context[self.output_key] = best_hypothesis
        context["ranked_hypotheses"] = sorted(hypotheses, key=lambda h: h["final_score"], reverse=True)

        return context

    def generate_hypotheses(self, context: dict) -> list[dict]:
        """Generates multiple hypotheses using different strategies"""
        goal = context.get(GOAL)
        question = goal.get(GOAL_TEXT)

        strategies = self.cfg.get("generation_strategy_list", ["cot"])
        merged = {**context, "question": question}

        hypotheses = []
        goal_id = self.get_goal_id(goal)
        for strategy in strategies:
            prompt = self.prompt_loader.from_file(
                f"strategy_{strategy}.txt", self.cfg, merged
            )
            response = self.call_llm(prompt, merged)
            hypothesis = HypothesisORM(
                text=response,
                goal_id=goal_id,
                strategy=strategy,
                features={"strategy": strategy},
                source=self.name,
                pipeline_signature=context.get(PIPELINE),
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
            )
            self.memory.hypotheses.insert(hypothesis)
            hypotheses.append(hypothesis.to_dict())

        return hypotheses

    def _summarize_pattern(self, pattern: dict):
        stats = {}
        for dimension, label in pattern.items():
            stats[label] = stats.get(label, 0) + 1
        return stats
---END-OF-FILE---


"co_ai\agents\generation.py"
---START-OF-FILE---
# co_ai/agents/generation.py

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import (FEEDBACK, GOAL, GOAL_TEXT, HYPOTHESES, LITERATURE,
                             PIPELINE, PIPELINE_RUN_ID)
from co_ai.parsers import extract_hypotheses
from co_ai.tools.huggingface_tool import recommend_similar_papers


class GenerationAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        from co_ai.models import HypothesisORM

        papers = recommend_similar_papers()


        goal = context.get(GOAL)

        self.logger.log("GenerationStart", {GOAL: goal})

        # Load literature if available
        literature = context.get(LITERATURE, {})

        # Build context for prompt
        render_context = {
            GOAL: goal.get(GOAL_TEXT),
            LITERATURE: literature,
            FEEDBACK: context.get(FEEDBACK, {}),
            HYPOTHESES: context.get(HYPOTHESES, []),
        }
        merged = {**context, **render_context}

        # Load prompt based on strategy
        prompt_text = self.prompt_loader.load_prompt(self.cfg, merged)
        response = self.call_llm(prompt_text, context)

        # Extract hypotheses
        hypotheses = extract_hypotheses(response)
        hypotheses_saved = []
        prompt = self.memory.prompt.get_from_text(prompt_text)
        for h in hypotheses:
            hyp = HypothesisORM(
                goal_id=goal.get("id"),
                text=h,
                prompt_id=prompt.id,
                pipeline_signature=context.get(PIPELINE),
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
            )
            self.memory.hypotheses.insert(hyp)
            hypotheses_saved.append(hyp.to_dict())

        # Update context with new hypotheses
        context[self.output_key] = hypotheses_saved

        # Log event
        self.logger.log(
            "GeneratedHypotheses",
            {
                GOAL: goal,
                HYPOTHESES: hypotheses,
                "prompt_snippet": prompt_text[:100],
                "response_snippet": response[:200],
            },
        )

        return context
---END-OF-FILE---


"co_ai\agents\generic.py"
---START-OF-FILE---
# co_ai/agents/generic_agent.py
import re

from co_ai.agents.base_agent import BaseAgent


class GenericAgent(BaseAgent):
    def __init__(self, cfg: dict, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.name = cfg.get("name")
        self.cfg = cfg
        self.memory = memory
        self.logger = logger

        # Input/output mapping
        self.strategy = cfg.get("strategy", "default")

        # Regex pattern to extract result
        self.extraction_regex = cfg.get("extraction_regex", r"response:(.*)")

        # Optional refinement
        self.refine_prompts = cfg.get("refine_prompts", False)

    async def run(self, context: dict) -> dict:
        """Run agent based on config-defined behavior"""
        try:
            # Build prompt from template and context
            prompt = self.prompt_loader.load_prompt(self.cfg, context)

            # Call LLM
            response = self.call_llm(prompt, context)

            # Extract result using regex
            match = re.search(self.extraction_regex, response, re.DOTALL)
            result = match.group(1).strip() if match else response

            # Store in context
            context[self.output_key] = {
                "title": self.name,
                "content": result,
                "prompt_used": prompt[:300],
                "strategy": self.strategy
            }

            self.logger.log("AgentRanSuccessfully", {
                "agent": self.name,
                "input_key": self.input_key,
                "output_key": self.output_key,
                "prompt_snippet": prompt[:200],
                "response_snippet": result[:300]
            })

            return context

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("AgentFailed", {
                "agent": self.name,
                "error": str(e),
                "context_snapshot": {k: len(str(v)) for k, v in context.items()}
            })
            return context
---END-OF-FILE---


"co_ai\agents\idea_evaluator.py"
---START-OF-FILE---
# co_ai/agents/idea_evaluator.py
from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL
from co_ai.evaluator.llm_judge_evaluator import LLMJudgeEvaluator
from co_ai.evaluator.mrq_self_evaluator import MRQSelfEvaluator


class IdeaEvaluatorAgent(BaseAgent):
    """
    Evaluates research ideas and hypotheses using multiple strategies:

    - LLM-based pairwise comparison (like DPO)
    - Preference learning via MR.Q Self Evaluator
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy = cfg.get("strategy", "llm")  # llm | mrq
        self.evaluator = self._init_evaluator()
        self.top_k = cfg.get("top_k", 5)

    async def run(self, context: dict) -> dict:
        hypotheses = self.get_hypotheses(context)
        goal = context.get(GOAL)
        baseline = context.get("baseline_hypotheses", {})

        if not hypotheses:
            self.logger.log("NoHypothesesToEvaluate", {})
            context["scored_hypotheses"] = []
            return context

        scored_results = []
        for hyp in hypotheses:
            preferred, scores = self.evaluator.judge(
                prompt=hyp,
                output_a=baseline or hyp,
                output_b=hyp,
                context=context,
            )
            scored_results.append(
                {
                    "hypothesis": hyp,
                    "preferred": preferred,
                    "scores": scores,
                    "source": "llm-judge",
                    "score": scores.get("score_b", 0),
                    "reasoning": scores.get("reason", ""),
                }
            )

        scored_results.sort(key=lambda x: x["score"], reverse=True)
        context["scored_hypotheses"] = scored_results
        context["top_hypothesis"] = scored_results[0]
        return context

    def get_top_k(self, context: dict, k: int = 5):
        return sorted(
            context.get("scored_hypotheses", []), key=lambda x: x["score"], reverse=True
        )[:k]

    def _init_evaluator(self):
        if self.cfg.get("evaluator", "llm") == "llm":
            llm_model = self.cfg.get("evaluator_model", self.cfg.get("model"))
            prompt_file = self.cfg.get("evaluator_prompt_file", "evaluator.txt")
            return LLMJudgeEvaluator(
                self.cfg,
                llm_cfg=llm_model,
                prompt_file=prompt_file,
                llm=self.call_llm,
                logger=self.logger,
            )
        else:
            return MRQSelfEvaluator(
                memory=self.memory,
                logger=self.logger,
                device=self.cfg.get("device", "cpu"),
            )
---END-OF-FILE---


"co_ai\agents\idea_evolution.py"
---START-OF-FILE---
# co_ai/agents/evolution.py
import itertools

import numpy as np

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import EVOLVED, GOAL, HYPOTHESES, PIPELINE, RANKING
from co_ai.models import HypothesisORM
from co_ai.parsers import extract_hypotheses


class IdeaEvolutionAgent(BaseAgent):
    """
    The Evolution Agent refines hypotheses iteratively using several strategies:

    - Grafting similar hypotheses into unified statements
    - Feasibility improvement through LLM reasoning
    - Out-of-the-box hypothesis generation
    - Inspiration from top-ranked ideas
    - Simplification and clarity enhancement

    These improvements are based on the paper:
    "NOVELSEEK: When Agent Becomes the Scientist â€“ Building Closed-Loop System from Hypothesis to Verification"
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.use_grafting = cfg.get("use_grafting", False)
        self.max_variants_per_idea = cfg.get("max_variants", 3)
        self.max_evolution_rounds = cfg.get("evolution_rounds", 4)
        self.selection_top_k = cfg.get("select_top_k", 5)
        self.preferences = cfg.get("preferences", ["novelty", "feasibility"])

    async def run(self, context: dict) -> dict:
        """
        Evolve top-ranked hypotheses across multiple rounds.
        """
        # Get input hypotheses
        ranked_hypotheses = context.get(RANKING, [])
        fallback_hypotheses = context.get(HYPOTHESES, [])
        preferences = context.get("preferences", self.preferences)
        current_round = context.get("evolution_round", 0)

        if not ranked_hypotheses and not fallback_hypotheses:
            self.logger.log("NoHypothesesToEvolve", {"reason": "no_ranked_or_unranked_input"})
            context[EVOLVED] = []
            return context

        # Decide which hypotheses to evolve
        top_texts = [h.get("text") for h, _ in ranked_hypotheses[:3]] if ranked_hypotheses else fallback_hypotheses

        # Run evolution strategies
        all_variants = await self._mutate_all(top_texts, context, preferences)

        # Optionally use grafting
        if self.use_grafting:
            all_variants += await self.graft_similar(context)

        # Score and select top K
        scored_variants = self._score_variants(all_variants, context)
        top_variants = scored_variants[:self.selection_top_k]

        # Save to DB
        self._save_evolved(top_variants, context)

        # Update context
        context["evolved"] = top_variants
        context["evolution_round"] = current_round + 1
        context["evolved_count"] = len(top_variants)

        self.logger.log(
            "EvolutionCompleted",
            {
                "evolved_count": len(top_variants),
                "preferences": preferences,
                "round": current_round + 1
            }
        )

        return context

    async def _mutate_all(self, hypotheses: list, context: dict, preferences: list) -> list:
        """Generate multiple variants for each hypothesis"""
        all_mutants = []

        for h in hypotheses:
            prompt_context = {
                "hypothesis": h,
                "literature_summary": context.get("knowledge_base_summaries", []),
                "critique": context.get("scores", {}),
                "focus_area": context.get(GOAL, {}).get("focus_area"),
                "preferences": ", ".join(preferences)
            }

            prompt = self.prompt_loader.load_prompt(self.cfg, prompt_context)
            raw_output = self.call_llm(prompt, context)

            mutants = extract_hypotheses(raw_output)
            self.logger.log("HypothesisMutated", {
                "original": h[:60],
                "mutations": mutants[:2]
            })

            all_mutants.extend(mutants)

        return all_mutants

    async def graft_similar(self, context: dict, threshold: float = 0.85) -> list:
        """
        Graft pairs of highly similar hypotheses into unified versions.
        """
        hypotheses = self.get_hypotheses(context)
        embeddings = [await self.memory.embedding.get_or_create(h.get("text")) for h in hypotheses]
        used = set()
        grafted = []

        for (i, h1), (j, h2) in itertools.combinations(enumerate(hypotheses), 2):
            if i in used or j in used:
                continue

            sim = self.cosine_similarity(embeddings[i], embeddings[j])
            if sim >= threshold:
                self.logger.log("GraftingPair", {
                    "similarity": sim,
                    "h1": h1[:60] + "...",
                    "h2": h2[:60] + "..."
                })
                prompt = (
                    f"Combine the following hypotheses into a clearer, more innovative statement:\n\n"
                    f'A: {h1.get("text")}\nB: {h2.get("text")}'
                )
                try:
                    response = self.call_llm(prompt, context)
                    combined = extract_hypotheses(response)
                    grafted.extend(combined)
                    used.update([i, j])
                except Exception as e:
                    self.logger.log("GraftingFailed", {"error": str(e)})
                    continue

        # Add ungrafted hypotheses back
        hypotheses = context.get(HYPOTHESES, [])
        for i, h in enumerate(hypotheses):
            if i not in used:
                grafted.append(h)

        return grafted

    def _score_variants(self, variants: list, context: dict) -> list:
        """
        Score variants using ScorerAgent logic and sort by total score
        """
        scorer = self.memory.scores
        scored = []

        for v in variants:
            score_data = scorer.score(v, context)
            score_data["text"] = v
            scored.append(score_data)

        # Sort by composite score
        scored.sort(key=lambda x: x["score"], reverse=True)
        return scored

    def _save_evolved(self, variants: list, context: dict):
        """
        Save evolved hypotheses to database with lineage info
        """
        goal_text = self.extract_goal_text(context.get(GOAL))
        pipeline_sig = context.get(PIPELINE)

        for v in variants:
            hyp = HypothesisORM(
                goal=goal_text,
                text=v["text"],
                pipeline_signature=pipeline_sig,
                parent=context.get("current_hypothesis", None),
                evolution_level=context.get("evolution_round", 0)
            )
            self.db.add(hyp)
        self.db.commit()

    def cosine_similarity(self, vec1, vec2):
        """Compute cosine similarity between two vectors."""
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

---END-OF-FILE---


"co_ai\agents\idea_innovation.py"
---START-OF-FILE---
# co_ai/agents/idea_innovation.py
from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL
from co_ai.memory import IdeaStore
from co_ai.models.idea import IdeaORM


class IdeaInnovationAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        survey_results = context.get("survey_results", [])
        search_results = context.get("search_results", [])

        # Build prompt context
        prompt_context = {
            "goal_text": goal.get("goal_text"),
            "focus_area": goal.get("focus_area"),
            "goal_type": goal.get("goal_type"),
            "strategy": goal.get("strategy"),
            "survey_summary": self._summarize_results(survey_results),
            "search_result_summaries": self._summarize_results(search_results),
            "preferences": self.cfg.get("preferences", []),
        }

        merged = {**context, **prompt_context}

        # Load and render prompt
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)

        # Call LLM to generate ideas
        raw_ideas = self.call_llm(prompt, merged)

        # Parse and structure ideas
        ideas = self._parse_raw_ideas(raw_ideas, goal)

        # Store generated ideas
        stored_ideas = self.memory.ideas.bulk_add_ideas(ideas)

        # Update context with results
        context["ideas"] = [idea.to_dict() for idea in stored_ideas]
        context["idea_ids"] = [idea.id for idea in stored_ideas]

        return context

    def _summarize_results(self, results: list) -> str:
        """Converts list of result dicts into a summary string"""
        if not results:
            return "No prior research found."
        summaries = []
        for r in results[:5]:  # limit to top 5 for brevity
            title = r.get("title", "")
            summary = r.get("summary", "")[:200] + "..." if len(r.get("summary", "")) > 200 else ""
            url = r.get("url", "")
            summaries.append(f"- {title}: {summary} ({url})")
        return "\n".join(summaries)

    def _parse_raw_ideas(self, raw_text: str, goal: dict) -> list:
        """Parses raw LLM response into structured idea objects"""
        lines = [line.strip() for line in raw_text.splitlines() if line.strip()]
        ideas = []

        for line in lines:
            ideas.append({
                "idea_text": line,
                "parent_goal": goal.get("goal_text"),
                "focus_area": goal.get("focus_area"),
                "strategy": goal.get("strategy"),
                "source": "generated_by_IdeaInnovationAgent",
                "origin": "llm",
                "extra_data": {}
            })

        return ideas---END-OF-FILE---


"co_ai\agents\idea_sharpening.py"
---START-OF-FILE---
# co_ai/agents/idea_sharpening.py

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL, HYPOTHESES, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator import MRQSelfEvaluator
from co_ai.models import HypothesisORM


class IdeaSharpeningAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.target = cfg.get("target", "generation")
        self.device = cfg.get("device", "cpu")
        self.evaluator = MRQSelfEvaluator(memory, logger, device=self.device)
        self.templates = cfg.get("templates", ["critic"])
        self.save_count = cfg.get("save_count", 3)


    async def run(self, context: dict) -> dict:
        """
        Main execution loop for IdeaSharpeningAgent.

        Takes a list of ideas, sharpens them using templates,
        judges against baseline using evaluator, and logs results.
        """
        goal = context.get(GOAL, {})
        ideas = context.get("ideas", [])

        if not ideas:
            self.logger.log("NoIdeasToSharpen", {"reason": "empty_input"})
            return context

        sharpened_results = []
        for idea in ideas:
            idea_text = idea.get("idea_text")
            result = await self._sharpen_and_evaluate(idea_text, goal, context)
            sharpened_results.append(result)

        # Sort by score
        sharpened_results.sort(key=lambda x: x["score"], reverse=True)

        # Update context
        context["sharpened_ideas"] = [r["sharpened_hypothesis"] for r in sharpened_results]
        context["scored_ideas"] = sharpened_results
        best_idea = sharpened_results[0]["sharpened_hypothesis"]
        context["top_idea"] = best_idea

        hypotheses = context.get(HYPOTHESES, [])
        if hypotheses:
            # Find the hypothesis with the maximum confidence value
            sorted_hyps = sorted(
                hypotheses, key=lambda h: h.get("confidence", 0.0), reverse=True
            )

            # Keep only the top hypothesis
            context[HYPOTHESES] = sorted_hyps[:self.save_count]
            # For scoring later
            context["baseline_hypotheses"] = sorted_hyps[-1]

        return context

    async def _sharpen_and_evaluate(self, idea: str, goal: dict, context: dict) -> dict:
        # Build prompt for refinement
        focus_area = goal.get("focus_area", "")
        baselines = self.cfg.get("baselines")
        baseline = baselines.get(focus_area, baselines.get("default"))
        merged = {
            "goal": goal,
            "idea": idea,
            "baseline": baseline,
            "literature_summary": context.get("knowledge_base_summaries", []),
            "examples": self.memory.hypotheses.get_similar(idea, limit=3),
            "strategy": goal.get("strategy", "default"),
        }

        improved = None
        winner = "original"
        scores = {}

        for name in self.templates:
            prompt_template = self.prompt_loader.from_file(name, self.cfg, merged)
            sharpened = self.call_llm(prompt_template, merged)

            try:
                preferred_output, scores = self.evaluator.score_single(
                    prompt=idea,
                    output=sharpened,
                    context=merged,
                )
                improved = preferred_output
                winner = "b" if improved == sharpened else "a"
            except Exception as e:
                self.logger.log("IdeaSharpeningFailed", {"error": str(e)})
                improved = idea
                winner = "a"
                scores = {"value_a": 5.0, "value_b": 5.0}

            result = {
                "template_used": name,
                "original_idea": idea,
                "sharpened_hypothesis": improved,
                "winner": winner,
                "improved": winner == "b",
                "scores": scores,
                "score": max(scores.values()),
                "pipeline_stage": context.get(PIPELINE),
                "prompt_template": prompt_template,
            }

            saved_hyp = self.save_improved(goal, idea, result, context)
            if saved_hyp:
                context.setdefault(HYPOTHESES, []).append(saved_hyp.to_dict())
            return result

    def save_improved(self, goal: dict, original_idea: str, result: dict, context: dict):
        if not result["improved"]:
            return None
        sharpened = result["sharpened_hypothesis"]
        prompt_id = self.memory.prompt.get_id_from_response(sharpened)

        # Save to HypothesisORM
        hyp = HypothesisORM(
            goal_id=goal.get("id"),
            text=sharpened,
            prompt_id=prompt_id,
            pipeline_signature=context.get(PIPELINE),
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
            source="idea_sharpening_agent",
            confidence=result["score"]
        )
        self.memory.hypotheses.insert(hyp)

        self.logger.log(
            "IdeaSharpenedAndSaved",
            {
                "prompt_snippet": original_idea[:100],
                "response_snippet": sharpened[:100],
                "score": result["score"],
            },
        )

        return hyp
---END-OF-FILE---


"co_ai\agents\judge.py"
---START-OF-FILE---
import re

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL, REFLECTION


class JudgeAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = self.extract_goal_text(context.get(GOAL))
        hypotheses = self.get_hypotheses(context)

        self.logger.log("JudgeRunStarted", {"goal": goal[:100], "hypothesis_count": len(hypotheses)})

        if len(hypotheses) < 2:
            self.logger.log("NotEnoughHypotheses", {"count": len(hypotheses)})
            return context

        reflections = context.get(REFLECTION, [])

        length = len(hypotheses)
        if length < 7:
            self.logger.log("JudgeStrategy", {"method": "all_pair"})
            rankings = await self._run_all_pair(context, hypotheses, reflections)
        elif length < 11:
            self.logger.log("JudgeStrategy", {"method": "tournament"})
            rankings = await self._run_tournament(context, hypotheses, reflections)
        elif length < 16:
            self.logger.log("JudgeStrategy", {"method": "top_k"})
            rankings = await self._run_top_k(context, hypotheses, reflections)
        else:
            self.logger.log("JudgeStrategy", {"method": "default"})
            rankings = await self._run_default(context, hypotheses, reflections)

        self._log_rankings(goal, rankings)

        best_idx = rankings.index(max(rankings, key=lambda x: x["score"]))
        context["best_hypothesis"] = hypotheses[best_idx]
        context[self.output_key] = rankings

        self.logger.log("JudgeRunCompleted", {"best_index": best_idx, "best_score": rankings[best_idx]["score"]})

        return context

    async def _run_all_pair(self, context, hypotheses, reflections):
        scores = [0] * len(hypotheses)

        for i in range(len(hypotheses)):
            for j in range(i + 1, len(hypotheses)):
                result = await self._compare_pair(
                    context,
                    hypotheses[i],
                    hypotheses[j],
                    reflections[i],
                    reflections[j],
                )
                self.logger.log("PairJudged", result)
                if result["winner"] == "A":
                    scores[i] += 1
                elif result["winner"] == "B":
                    scores[j] += 1

        return [
            {"index": idx, "score": score, "text": hypotheses[idx]}
            for idx, score in enumerate(scores)
        ]

    async def _run_tournament(self, context, hypotheses, reflections):
        current_round = list(range(len(hypotheses)))
        scores = [0] * len(hypotheses)

        while len(current_round) > 1:
            next_round = []

            for i in range(0, len(current_round) - 1, 2):
                a = current_round[i]
                b = current_round[i + 1]

                result = await self._compare_pair(
                    context,
                    hypotheses[a],
                    hypotheses[b],
                    reflections[a],
                    reflections[b],
                )
                self.logger.log("TournamentPairJudged", result)

                winner = a if result["winner"] == "A" else b
                loser = b if result["winner"] == "A" else a

                scores[winner] += 1
                next_round.append(winner)

            if len(current_round) % 2 == 1:
                next_round.append(current_round[-1])

            current_round = next_round

        return [
            {"index": idx, "score": scores[idx], "text": hypotheses[idx]}
            for idx in current_round
        ]

    async def _run_top_k(self, context, hypotheses, reflections, k=3):
        scored = []
        for i, h in enumerate(hypotheses):
            score = self._compute_composite_score(reflections[i])
            scored.append((i, score))
            self.logger.log("TopKScoreComputed", {"index": i, "score": score})

        scored.sort(key=lambda x: x[1], reverse=True)
        top_indices = [x[0] for x in scored[:k]]
        top_hypotheses = [hypotheses[i] for i in top_indices]
        top_reflections = [reflections[i] for i in top_indices]

        self.logger.log("TopKSelected", {"indices": top_indices})

        rankings = await self._run_all_pair(context, top_hypotheses, top_reflections)

        return rankings

    async def _run_default(self, context, hypotheses, reflections):
        scores = [0] * len(hypotheses)
        for i in range(0, len(hypotheses), 2):
            try:
                result = await self._compare_pair(
                    context,
                    hypotheses[i],
                    hypotheses[i + 1],
                    reflections[i],
                    reflections[i + 1],
                )
                self.logger.log("DefaultPairJudged", result)

                winner = i if result["winner"] == "A" else i + 1
                scores[winner] += 1
            except IndexError:
                scores[-1] += 1
                break

        return [
            {"index": idx, "score": scores[idx], "text": hypotheses[idx]}
            for idx in range(len(hypotheses))
        ]

    async def _compare_pair(
        self, context, hypothesis_a, hypothesis_b, reflection_a, reflection_b
    ):
        to_merge = {
            "hypothesis_a": hypothesis_a,
            "hypothesis_b": hypothesis_b,
            "reflection_a": reflection_a,
            "reflection_b": reflection_b,
            "notes": context.get("comparison_notes", ""),
        }
        merged = {**context, **to_merge}

        prompt = self.prompt_loader.load_prompt(
            self.cfg, merged
        )
        response = self.call_llm(prompt, context)

        winner_match = re.search(r"better hypothesis:<([AB])>", response, re.IGNORECASE)
        reason_match = re.search(r"reason:<(.+)>", response, re.DOTALL)

        winner = winner_match.group(1).upper() if winner_match else "A"
        reason = reason_match.group(1).strip() if reason_match else "No clear winner"

        return {
            "winner": winner,
            "reason": reason,
            "prompt_used": prompt[:500] + "...",
            "hypothesis_a_snippet": hypothesis_a[:200],
            "hypothesis_b_snippet": hypothesis_b[:200],
        }

    def _compute_composite_score(self, reflection):
        base_score = reflection.get("elo_rating", 1000) / 10

        correctness = reflection.get("correctness_score", 3) * 10
        novelty = reflection.get("novelty_score", 3) * 10
        feasibility = reflection.get("feasibility_score", 3) * 10

        total = base_score + correctness + novelty + feasibility

        return total

    def _log_rankings(self, goal: str, rankings: list):
        for item in rankings:
            self.logger.log(
                "HypothesisRanked",
                {
                    "goal_snippet": goal[:60],
                    "hypothesis_snippet": item["text"][:100],
                    "score": item["score"],
                },
            )
---END-OF-FILE---


"co_ai\agents\knowledge_loader.py"
---START-OF-FILE---
# co_ai/agents/knowledge_loader.py
from co_ai.agents.base_agent import BaseAgent
from co_ai.models import SearchResultORM


class KnowledgeLoaderAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = context.get("goal")
        goal_id = goal.get("id")

        # Try to load from DB
        knowledge_base = self._load_from_db(goal_id)

        if knowledge_base:
            self.logger.log("LoadedFromDB", {"count": len(knowledge_base)})
            context[self.output_key] = knowledge_base
        else:
            self.logger.log("NoResultsFoundInDB", {})

        return context

    def _load_from_db(self, goal_id: int) -> list:
        """
        Load processed search results (with key concepts, insights, etc.)
        that are linked to this goal.
        """

        results = self.memory.search_results.get_by_goal_id(goal_id)
        return [
            {
                "title": r.title,
                "summary": r.summary,
                "refined_summary": r.refined_summary,
                "url": r.url,
                "source": r.source,
                "key_concepts": r.key_concepts,
                "technical_insights": r.technical_insights,
                "relevance_score": r.relevance_score,
                "related_ideas": r.related_ideas,
                "extracted_methods": r.extracted_methods,
                "domain_knowledge_tags": r.domain_knowledge_tags
            }
            for r in results
        ]---END-OF-FILE---


"co_ai\agents\lats_dspy.py"
---START-OF-FILE---
import json
import math
import re
from collections import defaultdict

import dspy
from dspy import (BootstrapFewShot, Example, InputField, OutputField, Predict,
                  Signature)
from dspy.signatures import InputField, OutputField

from co_ai.agents import BaseAgent
from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin
from co_ai.agents.proximity import ProximityAgent
from co_ai.agents.rule_tuner import RuleTunerAgent
from co_ai.agents.unified_mrq import UnifiedMRQAgent
from co_ai.constants import GOAL, PIPELINE_RUN_ID
from co_ai.models import EvaluationORM, HypothesisORM
from co_ai.utils.graph_tools import (analyze_graph_impact, build_mermaid_graph,
                                     compare_graphs, save_mermaid_to_file)


class TraceStep(Signature):
    """
    A reasoning step in the LATS framework.

    Inputs:
        - state: Current problem state (e.g., goal + history)
        - trace: Sequence of previous thoughts/actions

    Outputs:
        - next_step: Next thought/action to explore
    """

    state = InputField(desc="Current problem state")
    trace = InputField(desc="History of thoughts/actions taken so far")
    next_step = OutputField(desc="Next reasoning step (thought or action)")


class ReflectionPrompt(Signature):
    """
    Self-reflection module to analyze failed reasoning paths.

    Inputs:
        - state: Final state after failed attempt
        - trace: Full reasoning path
        - goal: Original goal text

    Outputs:
        - rationale: Explanation of failure
        - improvement_plan: Suggested improvements
    """

    state = InputField(desc="Final state after failed attempt")
    trace = InputField(desc="Full reasoning path")
    goal = InputField(desc="Original goal text")

    rationale = OutputField(desc="Why the attempt failed")
    improvement_plan = OutputField(desc="Concrete steps to improve")


class ValueEstimator(Signature):
    """
    Evaluates a reasoning path using a hybrid value function.

    Inputs:
        - state: Current problem state
        - trace: Reasoning steps taken
        - goal: Goal text

    Outputs:
        - score: Normalized score (0â€“1)
        - rationale: Explanation of the score
    """

    state = InputField(desc="Current problem state")
    trace = InputField(desc="Sequence of thoughts/actions")
    goal = InputField(desc="Goal text")

    score = OutputField(desc="Hybrid score (LM + self-consistency)")
    rationale = OutputField(desc="Explanation of score")


class SharpeningPrompt(Signature):
    """
    Sharpens hypotheses using dimensional feedback.

    Inputs:
        - hypothesis: Original hypothesis text
        - feedback: Dimensional scores and rationales
        - goal: Original goal

    Outputs:
        - refined_hypothesis: Improved version
        - changes: Summary of changes made
    """

    hypothesis = InputField(desc="Original hypothesis")
    feedback = InputField(desc="Dimensional scores and rationales")
    goal = InputField(desc="Goal text")

    refined_hypothesis = OutputField(desc="Improved hypothesis")
    changes = OutputField(desc="Summary of changes made")


class LATSProgram(dspy.Module):
    def __init__(self, cfg, agent):
        super().__init__()
        self.cfg = cfg
        self.agent = agent
        self.generator = Predict(TraceStep)
        self.value_estimator = Predict(ValueEstimator)
        self.reflector = Predict(ReflectionPrompt)
        self.sharpener = Predict(SharpeningPrompt)
        self.max_depth = cfg.get("max_depth", 3)

    def _estimate_value(self, state, trace):
        """Estimate value using LM-powered scorer"""
        result = self.value_estimator(state=state, trace=trace, goal=state)
        try:
            score = float(result.score)
        except:
            score = 0.5
        return score, result.rationale

    def forward(self, state, trace, depth=0):
        if depth >= self.max_depth:
            return trace, self._estimate_value(state, trace)[0]

        prediction = self.generator(state=state, trace=trace)
        if not prediction or not prediction.next_step:
            return trace, 0.0

        next_step = prediction.next_step.strip()
        new_state = self.agent._update_state(state, next_step)
        new_trace = trace + [next_step]

        child_trace, child_score = self.forward(new_state, new_trace, depth + 1)

        if child_score < self.cfg.get("threshold", 0.7):
            reflection = self.reflector(state=new_state, trace=child_trace, goal=state)
            sharpened = self.sharpener(
                hypothesis=next_step, feedback=reflection.rationale, goal=state
            )
            child_trace[-1] = sharpened.refined_hypothesis
            new_state = self.agent._update_state(state, child_trace[-1])
            score, _ = self._estimate_value(new_state, child_trace)
            return child_trace, score

        return child_trace, child_score


class LATSDSPyAgent(ScoringMixin, BaseAgent):
    """
    Enhanced LATS agent with:
    - Tree search (MCTS + UCT)
    - Multi-dimensional scoring
    - Proximity-based reuse
    - Reflection/refinement
    - Rule tuning
    - DSPy optimization
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.max_depth = cfg.get("max_depth", 5)
        self.branching_factor = cfg.get("branching_factor", 3)
        self.ucb_weight = cfg.get("ucb_weight", 1.41)
        self.num_simulations = cfg.get("num_simulations", 50)
        self.lambda_weight = cfg.get("lambda", 0.5)

        # Node tracking
        self.nodes = []
        self.N = defaultdict(int)  # visit count
        self.W = defaultdict(float)  # total reward
        self.children = dict()  # node -> children

        # Initialize sub-agents
        self.proximity_agent = ProximityAgent(
            cfg.get("proximity", {}), memory=memory, logger=logger
        )
        self.rule_tuner = RuleTunerAgent(
            cfg.get("rule_tuner", {}), memory=memory, logger=logger
        )
        self.mrq_agent = UnifiedMRQAgent(
            cfg.get("mrq", {}), memory=memory, logger=logger
        )

        # Setup DSPy
        lm = dspy.LM(
            "ollama_chat/qwen3",
            api_base="http://localhost:11434",
            api_key="",
        )
        dspy.configure(lm=lm)

        # Initialize DSPy program
        self.lats_program = LATSProgram(cfg, self)

        # Symbolic impact analyzer
        self.impact_analyzer = SymbolicImpactAnalyzer(self._get_score)

    async def run(self, context: dict) -> dict:
        """Main LATS search loop"""
        goal = context[GOAL]
        root_state = root_state = {
            "goal": goal["goal_text"],
            "goal_id": goal["id"],
            "current": goal["goal_text"],
            "trace": [],
        }

        # 1. Initialize root node
        root = self.create_node(state=root_state, trace=[], parent=None)

        # 2. Run MCTS simulations
        for sim_num in range(self.num_simulations):
            # Selection
            node = self.select(root)

            # Expansion
            if not self.is_terminal(node):
                node = await self.expand(node, context)

            # Simulation & Evaluation
            reward, trace_data = self.simulate_and_evaluate(node, context)

            # Backpropagation
            self.backpropagate(node, reward, trace_data)

            # âœ… Log Mermaid graph after each simulation
            if sim_num % 5 == 0:  # Every 5 simulations
                percent_complete = (sim_num + 1) / self.num_simulations * 100
                self.logger.log(
                    "Progress",
                    {
                        "simulation": sim_num + 1,
                        "percent_complete": f"{percent_complete:.1f}%",
                        "best_score": self._get_best_score(root),
                    },
                )
                mermaid_lines = build_mermaid_graph(root, max_depth=3)
                mermaid_diagram = "\n".join(mermaid_lines)
                self.logger.log("SearchTree", {"diagram": mermaid_diagram})

            # Optional: Periodic refinement
            if sim_num % 10 == 0:
                await self._refine_system(context)

        # 3. Get best path
        best_child = self.best_uct(node=root, ucb_weight=0)  # Greedy selection
        best_trace = best_child["trace"]

        # âœ… Final Mermaid visualization
        mermaid_lines = build_mermaid_graph(best_child, max_depth=5)
        mermaid_diagram = "\n".join(mermaid_lines)
        self.logger.log("FinalSearchTree", {"diagram": mermaid_diagram})
        save_mermaid_to_file(mermaid_diagram, "final_search_tree.mmd")

        # Reconstruct merged context for prompt
        merged_for_prompt = {
            "state": best_child["state"]["current"]
            if isinstance(best_child["state"], dict)
            else best_child["state"],
            "trace": best_trace,
            "mode": "reason",  # Or use context.get("mode", "reason")
        }
        prompt_text = self.prompt_loader.load_prompt(self.cfg, merged_for_prompt)
        prompt_id = self.memory.prompt.get_id_from_response(prompt_text)

        # Safely extract scores
        dimension_scores = best_child.get("dimension_scores", {})

        # 4. Create final hypothesis
        hypothesis = HypothesisORM(
            goal_id=goal["id"],
            prompt_id=prompt_id,
            source=self.name,
            text="\n".join(best_trace),
            metadata={
                "trace": best_trace,
                "path": [n["id"] for n in best_trace],
                "scores": {
                    dim: data["score"] for dim, data in dimension_scores.items()
                },
                "score": best_child.get("score", 0.0),
            },
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
        )

        self.memory.hypotheses.insert(hypothesis)
        context.setdefault("lats_result", []).append(hypothesis.to_dict())
        context.setdefault("hypotheses", []).append(hypothesis.to_dict())
        return context

    def create_node(self, state, trace, parent=None):
        """Create a new node in the search tree"""

        # Ensure trace is always a list
        if isinstance(trace, str):
            print("Trace is not right")
            trace = trace.split("\n")  # Convert string to list
        elif not isinstance(trace, list):
            print("Trace is not right")
            trace = [str(trace)]  # Fallback

        node = {
            "id": len(self.nodes) + 1,
            "state": state,
            "trace": trace,
            "parent": parent,
            "visits": 0,
            "reward": 0.0,
            "children": [],
            "is_terminal": False,
            "dimension_scores": {},
            "final_score": 0.0,
        }
        self.nodes.append(node)
        return node

    def select(self, node):
        self._log_node(node, level="debug")
        """Select node for expansion using UCT"""
        while self.children.get(id(node)) and self.children[id(node)]:
            unvisited = [c for c in self.children[id(node)] if c["visits"] == 0]
            if unvisited:
                return unvisited[0]
            node = self.best_uct(node)
        return node

    def best_uct(self, node, ucb_weight=None):
        """Select best child using UCT formula"""
        ucb_weight = ucb_weight or self.ucb_weight

        def uct(child):
            if child["visits"] == 0:
                return float("inf")
            return (child["reward"] / child["visits"]) + ucb_weight * math.sqrt(
                math.log(node["visits"]) / child["visits"]
            )

        return max(self.children[id(node)], key=uct)

    async def expand(self, node, context: dict):
        """Generate new children nodes from current node"""
        self._log_node(node, level="debug")

        # Build prompt with context
        merged = {
            **context,
            "state": node["state"]["current"],  # Only pass the current reasoning state
            "trace": node["trace"],
            "mode": "reason",
        }

        # 1. Get similar hypotheses
        proximity_context = await self._run_proximity(context)
        self.logger.log(
            "ProximityContext",
            {
                "most_similar": proximity_context.get("most_similar"),
                "all": proximity_context,
            },
        )
        merged["similar_hypotheses"] = proximity_context.get("most_similar", "")

        # 2. Generate completions with DSPy
        completions, steps = self.lats_program.forward(
            state=node["state"], trace=node["trace"], depth=0
        )

        # 3. Apply proximity-based refinement
        refined_completions = []
        for comp in completions:
            refined = self._apply_proximity_guidance(comp, proximity_context)
            refined_completions.append(refined)

        # 4. Score and build children
        children = []
        for comp in refined_completions:
            new_state = self._update_state(node["state"], comp)
            new_trace = node["trace"] + [comp]

            # Ensure scoring context includes mode
            scoring_context = {
                **context,
                "mode": "reason",  # Required for CoR templates
            }

            # Score using dimensional scorers
            hyp = {"text": comp, "goal_id": context[GOAL]["id"]}

            score_result = self.score_hypothesis(
                hyp, scoring_context, metrics="lats_node"
            )

            # Create child node with metadata
            child = self.create_node(state=new_state, trace=new_trace, parent=node)
            child["score"] = score_result["score"]
            child["dimension_scores"] = score_result["scores"]
            child["action"] = comp

            children.append(child)
            self._log_node(child, level="debug")

        # Store children
        self.children[id(node)] = children

        # Log all children at once
        for child in children:
            self._log_node(child, level="info")  # â† Add here

        return children[0] if children else node

    def simulate_and_evaluate(self, node, context):
        """Simulate until terminal state and return final reward"""
        current = node
        while not self.is_terminal(current) and len(current["trace"]) < self.max_depth:
            # Build prompt
            merged = {
                **context,
                "state": current["state"],
                "trace": current["trace"],
                "mode": "simulate",
            }
            prompt = self.prompt_loader.load_prompt(self.cfg, merged)
            response = self.call_llm(prompt, context=merged)

            # Parse completions
            completions = self._parse_completions(response)
            if not completions:
                break

            action = completions[0]  # Take first completion
            new_state = self._update_state(current["state"], action)
            new_trace = current["trace"] + [action]

            # Create new node
            current = self.create_node(state=new_state, trace=new_trace, parent=current)

        # Evaluate final node
        reward, trace_data = self.evaluate(current, context)
        return reward, trace_data

    def evaluate(self, node, context):
        """Evaluate node using hybrid LM + self-consistency scoring"""
        if self.cfg.get("use_environment", False):
            obs = self.env.step(node["state"])
            return obs["reward"], {"trace": node["trace"], "environment": obs}

        # Fallback: dimensional scoring
        print(node)
        hyp = {
            "text": "\n".join(node["trace"]),
            "goal_id": node["state"].get("goal_id"),
        }

        score_result = self.score_hypothesis(hyp, context, metrics="lats_reflection")
        return score_result["score"] / 100, score_result

    def backpropagate(self, node, reward, trace_data=None):
        """Update node statistics up the tree"""
        while node:
            self._log_node(node, level="debug")
            node["visits"] += 1
            node["reward"] += reward

            # Store trace data for analysis
            if trace_data:
                node.setdefault("history", []).append(
                    {
                        "visits": node["visits"],
                        "reward": reward,
                        "trace_data": trace_data,
                    }
                )

            node = node["parent"]

    def is_terminal(self, node):
        """
        Check if node is terminal state
        Works with both string and dict state formats
        """
        state = node["state"]

        # Handle structured state dict
        if isinstance(state, dict):
            current_text = state.get("current", "")
            return (
                "success" in current_text.lower()
                or len(node["trace"]) >= self.max_depth
            )

        # Fallback for string-based state
        return "success" in state.lower() or len(node["trace"]) >= self.max_depth

    def _update_state(self, state_dict, action):
        """
        Updates structured state dictionary with new action
        """
        if isinstance(state_dict, dict):
            new_state = state_dict.copy()
            new_state["current"] = f"{state_dict['current']}\n{action}"
            # Ensure trace stays as list
            new_state["trace"] = state_dict.get("trace", []) + [action]
            return new_state
        # Fallback for string state
        return {
            "current": f"{state_dict}\n{action}",
            "trace": [action],  # Start with action as list
        }

    def _parse_completions(self, response: str) -> list:
        """Parse multiple thoughts/actions from response"""
        thought_pattern = r"([Tt]hought\s*\d+|[Aa]ction\s*\d+|[-â€¢])\s*(.*?)(?=\n(?:[Tt]hought\s*\d+|[Aa]ction\s*\d+|[-â€¢])\s|\Z)"
        matches = re.findall(thought_pattern, response.strip(), re.DOTALL)

        if not matches:
            return [response.strip()]

        completions = [match[-1].strip() for match in matches if match[-1].strip()]
        return completions[: self.branching_factor]

    async def _run_proximity(self, context):
        """Run proximity agent to find similar hypotheses"""
        try:
            return await self.proximity_agent.run(context)
        except Exception as e:
            self.logger.log("ProximityAgentFailed", {"error": str(e)})
            return {}

    def _apply_proximity_guidance(self, comp, proximity_data):
        """Enhance completion using proximity feedback"""
        if not proximity_data.get("most_similar"):
            return comp

        # Use LLM to refine action with proximity info
        prompt = self.prompt_loader.load_prompt(
            "proximity_guidance",
            {
                "current_action": comp,
                "similar_hypotheses": proximity_data["most_similar"],
            },
        )

        response = self.call_llm(prompt, {})
        return response.strip()

    def _apply_reflection_to_prompt(self, prompt, reflection):
        """Inject reflection into prompt for future steps"""
        if not reflection:
            return prompt

        reflection_prompt = self.prompt_loader.load_prompt(
            "reflection_injection", {"prompt": prompt, "reflection": reflection}
        )

        return self.call_llm(reflection_prompt, {})

    def _get_score(self, node, source="graph1"):
        """Get score for node in impact analysis"""
        resolved = self.resolve_node(node)
        if not resolved:
            return 0.0

        # Safely extract trace (always a list)
        trace = resolved.get("trace", [])
        if isinstance(trace, str):
            trace = trace.split("\n")  # Fallback if trace is string
        elif not isinstance(trace, list):
            trace = []

        # Safely extract state (ensure dict)
        state = resolved.get("state", {})  # âœ… Fallback to empty dict
        goal_text = state.get("goal", "Unknown goal")

        # Build hypothesis for scoring
        hyp = {
            "text": "\n".join(trace),
        }

        # Score using dimensional scorers
        score_result = self.score_hypothesis(
            hyp,
            {"goal": {"goal_text": goal_text}},  # Always a dict
            metrics="lats_reflection",
        )

        return score_result["score"] / 100  # Normalize

    def resolve_node(self, node):
        """Converts node ID or string to full node dict"""
        if isinstance(node, dict):
            return node

        # Try match by ID
        matches = [n for n in self.nodes if str(n["id"]) == str(node)]
        if matches:
            return matches[0]

        # Last resort: node is a trace string
        if isinstance(node, str):
            return {"trace": node.split("\n")}

        # Default
        return {"trace": []}

    async def _refine_system(self, context):
        if len(self.nodes) > 1:
            # Pass full node dicts to analyzer
            analysis = self.impact_analyzer.analyze(
                self.nodes[: len(self.nodes) // 2],  # First half of nodes
                self.nodes[len(self.nodes) // 2 :],  # Second half
            )
            context["graph_analysis"] = analysis

        # Train MR.Q on high-quality traces
        high_scoring = [n for n in self.nodes if n.get("score", 0) > 0.8]
        if high_scoring:
            await self.mrq_agent.run({"traces": high_scoring})

        # Tune rules based on analysis
        if context.get("graph_analysis"):
            await self.rule_tuner.run(context)

        return context

    def _get_value(self, node):
        """Calculate value using hybrid LM + self-consistency"""
        lm_score = node.get("score", 0.5)
        sc_score = self._self_consistency(node)
        return self.lambda_weight * lm_score + (1 - self.lambda_weight) * sc_score

    def _self_consistency(self, node):
        """Calculate self-consistency score for node"""
        if not node["trace"]:
            return 0.0

        # Use LLM to evaluate consistency
        prompt = self.prompt_loader.load_prompt(
            "self_consistency", {"trace": node["trace"], "state": node["state"]}
        )
        response = self.call_llm(prompt, {})

        # Parse numerical score
        score_match = re.search(r"(\d+)", response)
        return int(score_match.group(1)) / 100 if score_match else 0.5

    def _get_dimension_score(self, trace):
        """Get dimensional scores for trace"""
        # Build hypothesis
        hyp = {"text": "\n".join(trace), "id": f"hyp_{len(self.nodes)}"}

        # Score across dimensions
        score_result = self.score_hypothesis(hyp, {}, metrics="lats_reflection")
        return score_result["score"] / 100  # Normalize

    def _train_on_traces(self, traces):
        """Train DSPy module on high-quality traces"""
        # Convert traces to examples
        examples = [
            Example(
                state=trace["state"],
                trace=trace["trace"],
                next_step=trace["last_action"],
            )
            for trace in traces
        ]

        # Use dimensional scores as weights
        weighted_examples = [
            example.with_score(self._get_dimension_score(example.trace))
            for example in examples
        ]

        # Compile with BootstrapFewShot
        tuner = BootstrapFewShot(metric=self._dimension_aware_metric)
        self.lats_program.generator = tuner.compile(
            student=Predict(TraceStep), trainset=weighted_examples
        )

    def _dimension_aware_metric(self, example, pred):
        """Use dimensional scores for training metric"""
        scores = self._get_dimension_scores(pred.trace)
        return sum(s["score"] * s.get("weight", 1.0) for s in scores.values())

    def _get_dimension_scores(self, trace):
        """Get scores across all dimensions"""
        hyp = {"text": "\n".join(trace)}
        return self.score_hypothesis(hyp, {}, metrics="lats_node")

    def _generate_reflection(self, node):
        """Generate reflection for failed trajectory"""
        prompt = self.prompt_loader.load_prompt(
            "reflection",
            {
                "trace": node["trace"],
                "state": node["state"],
                "goal": node["state"],  # Use state as goal proxy
            },
        )
        response = self.call_llm(prompt, {})
        return response.strip()

    def _build_prompt(self, node):
        """Build prompt for node evaluation"""
        merged = {"state": node["state"], "trace": node["trace"], "mode": "evaluate"}
        return self.prompt_loader.load_prompt(self.cfg, merged)

    def _choose_action(self, response):
        """Choose best action from response"""
        completions = self._parse_completions(response)
        return completions[0] if completions else ""

    def _self_consistency_check(self, node):
        """Validate consistency of reasoning path"""
        prompt = self.prompt_loader.load_prompt(
            "self_consistency", {"trace": node["trace"], "state": node["state"]}
        )
        response = self.call_llm(prompt, {})

        # Parse consistency score
        score_match = re.search(r"(\d+)", response)
        return int(score_match.group(1)) / 100 if score_match else 0.5

    def _should_prune(self, node):
        """Determine if node should be pruned"""
        return node.get("score", 0) < self.cfg.get("prune_threshold", 0.4)

    def _get_node_path(self, node):
        """Get full path from root to node"""
        path = []
        while node:
            path.append(node)
            node = node["parent"]
        return path[::-1]  # Reverse to get root-first

    def _log_simulation(self, sim_num, node, reward):
        """Log simulation results for analysis"""
        self.logger.log(
            "LATSIteration",
            {
                "simulation": sim_num,
                "node_id": node["id"],
                "reward": reward,
                "trace": node["trace"][-3:] if node["trace"] else [],
                "depth": len(node["trace"]),
            },
        )

    def _log_node(self, node, level="debug"):
        """
        Logs a structured representation of a node for debugging
        """
        # Safely extract parent ID
        parent_info = node.get("parent")
        parent_id = "none"
        if parent_info is not None:
            parent_id = (
                getattr(parent_info, "id", parent_info.get("id", "none"))
                if isinstance(parent_info, (dict, object))
                else "none"
            )

        # Safely extract state string
        state_data = node.get("state", {})
        if isinstance(state_data, dict):
            state_str = state_data.get("current", "")
        else:
            state_str = str(state_data)

        # Safely extract trace
        trace = node.get("trace", [])

        # Build node info for logging
        node_info = {
            "id": node.get("id", "unknown"),
            "visits": node.get("visits", 0),
            "reward": round(float(node.get("reward", 0.0)), 2),
            "depth": len(trace),
            "is_terminal": node.get("is_terminal", False),
            "trace_preview": trace[-3:] if trace else [],
            "state_preview": state_str[:200],  # Now safe for both string and dict
            "parent_id": parent_id,
            "child_count": len(node.get("children", [])),
            "score": round(float(node.get("score", 0.0)), 2),
            "dimension_scores": {
                dim: {
                    "score": round(float(score.get("score", 0)), 2),
                    "rationale": score.get("rationale", "")[:100],
                }
                for dim, score in node.get("dimension_scores", {}).items()
            }
            if "dimension_scores" in node
            else {},
        }

        # Log based on level
        if level == "debug":
            self.logger.log(
                "NodeDebug", {"node_id": node.get("id"), "node_info": node_info}
            )
        elif level == "info":
            self.logger.log(
                "NodeSummary",
                {
                    "node_id": node.get("id"),
                    "summary": self._format_node_summary(node_info),
                },
            )
        return node_info

    def _format_node_summary(self, node_info):
        """Formats node info into a single-line summary"""
        return (
            f"ID:{node_info['id']} "
            f"V:{node_info['visits']} "
            f"R:{node_info['reward']} "
            f"S:{node_info['score']} "
            f"D:{node_info['depth']} "
            f"Pâ†’{node_info['parent_id']} "
            f"Childs:{node_info['child_count']} "
            f"State:'{node_info['state_preview'][:30]}...'"
            f"Dimensions:{self._format_dimension_scores(node_info['dimension_scores'])}"
        )

    def _format_dimension_scores(self, dimension_scores):
        return " ".join(
            f"{dim}={data['score']}" for dim, data in dimension_scores.items()
        )


class SymbolicImpactAnalyzer:
    """
    Analyzes structural overlap and divergence between two graph representations (e.g., symbolic vs. LATS)
    and attributes score delta to divergent paths.
    """

    def __init__(self, score_lookup_fn):
        self.score_lookup_fn = (
            score_lookup_fn  # Function to get scores for a given node or trace
        )

    def analyze(self, graph1, graph2):
        matches, only_1, only_2 = compare_graphs(graph1, graph2)
        results = []

        for node in matches:
            score_1 = self.score_lookup_fn(node, source="graph1")
            score_2 = self.score_lookup_fn(node, source="graph2")
            results.append(
                {"node": node, "type": "converged", "delta": score_2 - score_1}
            )

        for node in only_1 + only_2:
            score = self.score_lookup_fn(node, source="graph1")
            results.append({"node": node, "type": "diverged", "score": score})

        return results
---END-OF-FILE---


"co_ai\agents\lats_original.py"
---START-OF-FILE---
import json
import math
import re
from collections import defaultdict
from typing import Any, Dict, List, Optional, Union

from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin
from co_ai.agents.proximity import ProximityAgent
from co_ai.agents.rule_tuner import RuleTunerAgent
from co_ai.agents.unified_mrq import UnifiedMRQAgent
from co_ai.analysis.symbolic_impact_analyzer import SymbolicImpactAnalyzer
from co_ai.constants import GOAL, PIPELINE_RUN_ID
from co_ai.models import EvaluationORM, HypothesisORM
from co_ai.utils import compute_similarity_matrix
from co_ai.utils.graph_tools import build_mermaid_graph, save_mermaid_to_file


class LATSAgent(ScoringMixin, BaseAgent):
    """
    Enhanced LATS agent with:
    - Tree search (MCTS + UCT)
    - Multi-dimensional scoring
    - Proximity-based reuse
    - Reflection/refinement
    - Rule tuning
    - Mermaid visualization
    """
    
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        # Configuration
        self.max_depth = cfg.get("max_depth", 5)
        self.branching_factor = cfg.get("branching_factor", 3)
        self.ucb_weight = cfg.get("ucb_weight", 1.41)
        self.num_simulations = cfg.get("num_simulations", 50)
        self.lambda_weight = cfg.get("lambda", 0.5)
        self.min_score_threshold = cfg.get("min_score_threshold", 0.7)
        self.prune_threshold = cfg.get("prune_threshold", 0.4)
        self.similarity_threshold = cfg.get("similarity_threshold", 0.75)

        # Node tracking
        self.nodes = []
        self.N = defaultdict(int)  # visit count
        self.W = defaultdict(float)  # total reward
        self.children = dict()  # node -> children

        # Initialize sub-agents
        self.proximity_agent = ProximityAgent(
            cfg.get("proximity", {}), memory=memory, logger=logger
        )
        self.rule_tuner = RuleTunerAgent(
            cfg.get("rule_tuner", {}), memory=memory, logger=logger
        )
        self.mrq_agent = UnifiedMRQAgent(
            cfg.get("mrq", {}), memory=memory, logger=logger
        )
        self.impact_analyzer = SymbolicImpactAnalyzer(self._get_score)

    async def run(self, context: dict) -> dict:
        """
        Main LATS search loop
        """
        goal = context[GOAL]
        root_state = {
            "goal": goal["goal_text"],
            "current": goal["goal_text"],
            "trace": []
        }
        
        # Initialize root node
        root = self.create_node(state=root_state, trace=[], parent=None)
        
        # Run MCTS simulations
        best_scores = []
        for sim_num in range(self.num_simulations):
            # Selection
            node = self.select(root)

            # Expansion
            if not self.is_terminal(node):
                node = await self.expand(node, context)
            reward = self.simulate(node)
            self.backpropagate(node, reward)
            
            # Track progress
            current_best = self._get_best_score(root)
            best_scores.append(current_best)
            
            # âœ… Log Mermaid graph after each simulation
            if sim_num % 5 == 0:  # Every 5 simulations
                percent_complete = (sim_num + 1) / self.num_simulations * 100
                self.logger.log("Progress", {
                    "simulation": sim_num + 1,
                    "percent_complete": f"{percent_complete:.1f}%",
                    "best_score": self._get_best_score(root)
                })
                mermaid_lines = build_mermaid_graph(root, max_depth=3)
                mermaid_diagram = "\n".join(mermaid_lines)
                self.logger.log("SearchTree", {"diagram": mermaid_diagram})
    
            # Optional: Periodic refinement
            if sim_num % 10 == 0:
                await self._refine_system(context)

        # 3. Get best path
        best_child = self.best_uct(node=root, ucb_weight=0)  # Greedy selection
        best_trace = best_child["trace"]

        # âœ… Final Mermaid visualization
        mermaid_lines = build_mermaid_graph(best_child, max_depth=5)
        mermaid_diagram = "\n".join(mermaid_lines)
        self.logger.log("FinalSearchTree", {"diagram": mermaid_diagram})
        save_mermaid_to_file(mermaid_diagram, "final_search_tree.mmd")

        # Reconstruct merged context for prompt
        merged_for_prompt = {
            "state": best_child["state"]["current"]
            if isinstance(best_child["state"], dict)
            else best_child["state"],
            "trace": best_trace,
            "mode": "reason",  # Or use context.get("mode", "reason")
        }
        prompt_text = self.prompt_loader.load_prompt(self.cfg, merged_for_prompt)
        prompt_id = self.memory.prompt.get_id_from_response(prompt_text)

        # Safely extract scores
        dimension_scores = best_child.get("dimension_scores", {})

        # 4. Create final hypothesis
        hypothesis = HypothesisORM(
            goal_id=goal["id"],
            prompt_id=prompt_id,
            source=self.name,
            text="\n".join(best_trace),
            metadata={
                "trace": best_trace,
                "path": [n["id"] for n in best_trace],
                "scores": {
                    dim: data["score"] for dim, data in dimension_scores.items()
                },
                "score": best_child.get("score", 0.0),
            },
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
        )

        self.memory.hypotheses.insert(hypothesis)
        context.setdefault("lats_result", []).append(hypothesis.to_dict())
        context.setdefault("hypotheses", []).append(hypothesis.to_dict())
        # Refine system using analysis
        await self._refine_system(context)
        
        return context

    def create_node(self, state, trace, parent=None):
        """Create a new node in the search tree"""

        # Ensure trace is always a list
        if isinstance(trace, str):
            print("Trace is not right")
            trace = trace.split("\n")  # Convert string to list
        elif not isinstance(trace, list):
            print("Trace is not right")
            trace = [str(trace)]  # Fallback

        node = {
            "id": len(self.nodes) + 1,
            "state": state,
            "trace": trace,
            "parent": parent,
            "visits": 0,
            "reward": 0.0,
            "children": [],
            "is_terminal": False,
            "dimension_scores": {},
            "final_score": 0.0,
        }
        self.nodes.append(node)
        return node

    def select(self, node):
        self._log_node(node, level="debug")
        """Select node for expansion using UCT"""
        while self.children.get(id(node)) and self.children[id(node)]:
            unvisited = [c for c in self.children[id(node)] if c["visits"] == 0]
            if unvisited:
                return unvisited[0]
            node = self.best_uct(node)
        return node

    def best_uct(self, node, ucb_weight=None):
        """Select best child using UCT formula"""
        ucb_weight = ucb_weight or self.ucb_weight

        def uct(child):
            if child["visits"] == 0:
                return float("inf")
            return (child["reward"] / child["visits"]) + ucb_weight * math.sqrt(
                math.log(node["visits"]) / child["visits"]
            )

        return max(self.children[id(node)], key=uct)

    async def expand(self, node, context: dict):
        """
        Generate new children nodes from current node
        """
        # Build prompt with context
        merged = {
            **context,
            "state": node["state"]["current"],
            "trace": node["trace"],
            "mode": "reason"
        }
        
        # Get similar hypotheses
        proximity_context = await self._run_proximity(context)
        merged["similar_hypotheses"] = proximity_context.get("most_similar", "")
        
        # Load prompt
        prompt_text = self.prompt_loader.load_prompt(self.cfg, merged)
        response = self.call_llm(prompt_text, context=merged)

        prompt = self.memory.prompt.get_from_text(prompt_text)
        if not prompt:
            goal = context.get("goal")
            prompt_id = self.memory.prompt.save(goal, self.name, self.name, prompt_text, response)
        else:
            prompt_id = prompt.id
        # Parse completions
        completions = self._parse_completions(response)
        if not completions:
            self.logger.log("NoCompletions", {"prompt": prompt_text, "response": response})
            return node

        # Create child nodes
        children = []
        for comp in completions:
            new_state = self._update_state(node['state'], comp)
            new_trace = node['trace'] + [comp]
            
            # Score hypothesis
            hyp = HypothesisORM(
                goal_id=context[GOAL]["id"],
                prompt_id=prompt_id,
                strategy="lats",
                source=self.name,
                text=comp,
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
            )
            self.memory.hypotheses.insert(hyp)

            hyp_dict = hyp.to_dict()

            # Add mode to context
            score_context = {
                **context,
                "mode": "reason"
            }
            context.setdefault("hypotheses", []).append(hyp_dict)
            # Score using dimensional scorers
            score_result = self.score_hypothesis(hyp_dict, score_context, metrics="lats_node")
            
            # Create child node with metadata
            child = self.create_node(new_state, new_trace, parent=node)
            child["score"] = score_result["score"]
            child["dimension_scores"] = score_result["scores"]
            child["action"] = comp
            
            children.append(child)

        # Store children
        self.children[id(node)] = children
        return children[0]  # Return first child

    async def _run_proximity(self, context):
        """
        Run proximity agent to find similar hypotheses
        """
        try:
            return await self.proximity_agent.run(context)
        except Exception as e:
            self.logger.log("ProximityAgentFailed", {"error": str(e)})
            return {}

    def _parse_completions(self, response: str) -> list:
        """
        Safely parse completions from LLM response
        """
        thought_pattern = r"(?:[Tt]hought\s*\d+|[Aa]ction\s*\d+|[-â€¢])\s*(.*?)(?=\n(?:[Tt]hought\s*\d+|[Aa]ction\s*\d+|[-â€¢])\s|\Z)"
        matches = re.findall(thought_pattern, response.strip(), re.DOTALL)
        
        if not matches:
            return [response.strip()]
            
        completions = [match.strip() for match in matches if match.strip()]
        return completions[:self.branching_factor]

    def _update_state(self, state_dict, action: str) -> dict:
        """
        Updates state dictionary with new action
        """
        new_state = state_dict.copy()
        new_state["current"] = state_dict["current"] + "\n" + action
        new_state["trace"] = state_dict["trace"] + [action]
        return new_state

    def is_terminal(self, node):
        """
        Check if node is terminal state
        Works with both string and dict state formats
        """
        state = node["state"]

        # Handle structured state dict
        if isinstance(state, dict):
            current_text = state.get("current", "")
            return (
                "success" in current_text.lower()
                or len(node["trace"]) >= self.max_depth
            )

        # Fallback for string-based state
        return "success" in state.lower() or len(node["trace"]) >= self.max_depth

    def simulate(self, node):
        """
        Simulate until terminal state
        """
        current = node
        while not self.is_terminal(current) and len(current['trace']) < self.max_depth:
            prompt = self._build_prompt(current, mode="simulate", context=current)
            response = self.call_llm(prompt, context=current)
            completions = self._parse_completions(response)
            
            if not completions:
                break
                
            action = completions[0]  # Take first completion
            new_state = self._update_state(current["state"], action)
            current = self.create_node(
                new_state, current["trace"] + [action], parent=current
            )
        
        # Evaluate final node
        return self._get_value(current)

    def _build_prompt(self, node, mode="reason", context=None):
        """
        Build prompt for node evaluation
        """
        merged = {
            **context,
            "state": node["state"]["current"],
            "trace": node["trace"],
            "mode": mode
        }
        prompt_text = self.prompt_loader.load_prompt(self.cfg, merged)
        return prompt_text

    async def _run_proximity(self, context):
        """Run proximity agent to find similar hypotheses"""
        try:
            return await self.proximity_agent.run(context)
        except Exception as e:
            self.logger.log("ProximityAgentFailed", {"error": str(e)})
            return {}

    def _apply_proximity_guidance(self, comp, proximity_data):
        """Enhance completion using proximity feedback"""
        if not proximity_data.get("most_similar"):
            return comp

        # Use LLM to refine action with proximity info
        prompt = self.prompt_loader.load_prompt(
            "proximity_guidance",
            {
                "current_action": comp,
                "similar_hypotheses": proximity_data["most_similar"],
            },
        )

        response = self.call_llm(prompt, {})
        return response.strip()


    def _get_value(self, node):
        """
        Hybrid value function: LLM score + self-consistency
        """
        # Use LLM-powered scorer
        score_result = self._evaluate_node(node)
        
        # Self-consistency check
        sc_score = self._self_consistency(node)
        
        return self.lambda_weight * score_result["score"] + (1 - self.lambda_weight) * sc_score

    def _evaluate_node(self, node):
        """
        Evaluate node using dimensional scorers
        """
        hyp = {
            "text": "\n".join(node["trace"]),
            "id": f"hyp_{node['id']}"
        }
        return self.score_hypothesis(
            hyp,
            {"goal": {"goal_text": node["state"]["goal"]}},
            metrics="lats_reflection"
        )

    def _self_consistency(self, node):
        """
        Calculate self-consistency score
        """
        prompt = self.prompt_loader.load_prompt("self_consistency", {
            "trace": node["trace"],
            "state": node["state"]["current"]
        })
        response = self.call_llm(prompt, {})
        
        # Parse numerical score
        score_match = re.search(r'(\d+)', response)
        return int(score_match.group(1)) / 100 if score_match else 0.5

    def backpropagate(self, node, reward):
        """
        Update node statistics up the tree
        """
        while node:
            node['visits'] += 1
            node['reward'] += reward
            
            # Store trace data for analysis
            if "history" not in node:
                node["history"] = []
            node["history"].append({
                "visits": node["visits"],
                "reward": reward
            })
            
            node = node['parent']

    async def _refine_system(self, context):
        if len(self.nodes) > 1:
            # Pass full node dicts to analyzer
            analysis = self.impact_analyzer.analyze(
                self.nodes[:len(self.nodes)//2],  # First half of nodes
                self.nodes[len(self.nodes)//2:]  # Second half
            )
            context["graph_analysis"] = analysis
        
        # Train MR.Q on high-quality traces
        high_scoring = [n for n in self.nodes if n.get("score", 0) > 0.8]
        if high_scoring:
            await self.mrq_agent.run({"traces": high_scoring})
        
        # Tune rules based on analysis
        if context.get("graph_analysis"):
            await self.rule_tuner.run(context)
        
        return context

    def _get_score(self, node, source="graph1"):
        """
        Score node using hypothesis-based evaluator
        """
        trace = node.get("trace", [])
        if isinstance(trace, str):
            trace = trace.split("\n")
        elif not isinstance(trace, list):
            trace = []
        
        hyp = {
            "text": "\n".join(trace),
            "goal_id": node["state"]["goal_id"]
            if isinstance(node["state"], dict)
            else "unknown",
        }

        goal_text = (
            node["state"]["goal"] if isinstance(node["state"], dict) else node["state"]
        )
        score_result = self.score_hypothesis(
            hyp,
            {"goal": {"goal_text": goal_text}},  # Always a dict
            metrics="lats_reflection",
        )

        return score_result["score"] / 100  # Normalize

    def _log_progress(self, sim_num, best_score, best_trace):
        percent_complete = (sim_num + 1) / self.num_simulations * 100
        self.logger.log(
            "LATSProgress",
            {
                "simulation": sim_num + 1,
                "percent_complete": f"{percent_complete:.1f}%",
                "best_score": best_score,
                "best_trace": [step[:20] + "..." for step in best_trace],
                # "node_id": best_node.get("id", "unknown")
            },
        )

    def _should_stop_early(self, score_history):
        if len(score_history) < 5:
            return False

        # Check for score plateau
        recent_scores = score_history[-5:]
        score_variance = max(recent_scores) - min(recent_scores)
        if score_variance < 0.01:
            return True

        # Check for high score
        if score_history[-1] >= self.cfg.get("threshold", 0.95):
            return True

        return False

    def _log_node(self, node, level="debug"):
        """
        Logs a structured representation of a node for debugging
        """
        # Safely extract parent ID
        parent_info = node.get("parent")
        parent_id = "none"
        if parent_info is not None:
            parent_id = (
                getattr(parent_info, "id", parent_info.get("id", "none"))
                if isinstance(parent_info, (dict, object))
                else "none"
            )

        # Safely extract state string
        state_data = node.get("state", {})
        if isinstance(state_data, dict):
            state_str = state_data.get("current", "")
        else:
            state_str = str(state_data)

        # Safely extract trace
        trace = node.get("trace", [])

        # Build node info for logging
        node_info = {
            "id": node.get("id", "unknown"),
            "visits": node.get("visits", 0),
            "reward": round(float(node.get("reward", 0.0)), 2),
            "depth": len(trace),
            "is_terminal": node.get("is_terminal", False),
            "trace_preview": trace[-3:] if trace else [],
            "state_preview": state_str[:200],  # Now safe for both string and dict
            "parent_id": parent_id,
            "child_count": len(node.get("children", [])),
            "score": round(float(node.get("score", 0.0)), 2),
            "dimension_scores": {
                dim: {
                    "score": round(float(score.get("score", 0)), 2),
                    "rationale": score.get("rationale", "")[:100],
                }
                for dim, score in node.get("dimension_scores", {}).items()
            }
            if "dimension_scores" in node
            else {},
        }

        # Log based on level
        if level == "debug":
            self.logger.log("NodeDebug", {
                "node_id": node.get("id"),
                "node_info": node_info
            })
        elif level == "info":
            self.logger.log("NodeSummary", {
                "node_id": node.get("id"),
                "summary": self._format_node_summary(node_info)
            })
        return node_info

    def _apply_reflection_to_prompt(self, prompt, reflection):
        """
        Inject reflection into prompt for future steps
        """
        if not reflection:
            return prompt
            
        reflection_prompt = self.prompt_loader.load_prompt("reflection_injection", {
            "prompt": prompt,
            "reflection": reflection
        })
        
        return self.call_llm(reflection_prompt, {})

    def _train_on_traces(self, traces):
        """
        Train agent on high-quality traces
        """
        examples = [
            {
                "state": trace["state"],
                "trace": trace["trace"],
                "next_step": trace["last_action"]
            }
            for trace in traces
        ]
        
        # Use dimensional scores as weights
        weighted_examples = [
            example for example in examples
            if self._is_high_quality(example)
        ]
        
        if not weighted_examples:
            return

        # Re-train on high-quality traces
        self._retrain_on_examples(weighted_examples)

    def _is_high_quality(self, trace):
        """
        Check if trace meets quality threshold
        """
        return trace["score"] > self.min_score_threshold

    def _retrain_on_examples(self, examples):
        """
        Retrain agent on high-quality examples
        """
        # Implement your own training logic here
        pass
        ---END-OF-FILE---


"co_ai\agents\lats.py"
---START-OF-FILE---
import json
import math
import re
import uuid

import requests

from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin
from co_ai.constants import GOAL
from co_ai.utils.timing import time_function


class LATSNode:
    """Unified node structure for MCTS"""
    def __init__(self, state, trace, parent=None):
        self.id = str(uuid.uuid4())
        self.state = state
        self.trace = trace
        self.parent = parent
        self.children = []
        self.visits = 0
        self.reward = 0.0
        self.score = None
        self.dimension_scores = {}
        self.is_terminal = False

    def is_leaf(self):
        return len(self.children) == 0


class LATSAgent(ScoringMixin, BaseAgent): 
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        self.root = None
        self.nodes = []
        self.max_depth = cfg.get("max_depth", 5)
        self.branching_factor = cfg.get("branching_factor", 3)
        self.ucb_weight = cfg.get("ucb_weight", 1.41)
        self.num_simulations = cfg.get("num_simulations", 50)
        self.prune_threshold = cfg.get("prune_threshold", 0.4)
        self.children = {}

        self.max_steps = self.cfg.get("max_steps", 10)
        self.branching_factor = self.cfg.get("branching_factor", 3)

    async def run(self, context: dict):
        """Main MCTS loop"""
        # Initialize root node
        goal = context["goal"]
        self.root = self.create_node(goal["goal_text"], [])
        
        # Run simulations
        for _ in range(self.num_simulations):
            node = self._select(self.root)
            if not self._is_terminal(node):
                self.expand(node, context)
            reward = self._simulate(node, context)
            self._backpropagate(node, reward)
        
        # Return best path
        best_child = self._best_uct(self.root)
        final_score, best_node, best_trace = self._get_best_score(self.root)

        context["best_trace"] = best_trace
        context["best_score"] = final_score

        # Log final result
        self.logger.log("FinalResult", {
            "trace": best_trace,
            "score": final_score,
            "dimension_scores": best_node.dimension_scores
        })

        # Get timing logs
        timing_logs = self.logger.get_logs_by_type("FunctionTiming")
        for log in timing_logs:
            print(f"{log['timestamp']} - {log['data']['function']}: {log['data']['duration_ms']}ms")
        return {
            "trace": best_child.trace,
            "score": best_child.score,
            "dimension_scores": best_child.dimension_scores
        }

    def expand(self, node, context: dict):
        completions = self._generate_completions(node, context)
        if not completions:
            self.logger.log("NoCompletions", {"context": context})
            return node

        children = []
        for i, comp in enumerate(completions):
            new_state = self._update_state(node.state, comp["action"])
            new_trace = node.trace + [comp["step"]]
            
            hyp = {"text": comp["action"], "goal_id": context[GOAL]["id"]}

            print(f"Node State:=====\n{node.state}\n===")

            if isinstance(node.state, dict):
                goal_text = node.state.get("goal", "Unknown goal")
            else:
                goal_text = str(node.state)

            score_result = self._score_hypothesis(hyp, {"goal": {"goal_text": goal_text}}, metrics="lats_node")
            
            child = self.create_node(new_state, new_trace, parent=node)
            child.score = score_result["score"]
            child.dimension_scores = score_result["scores"]
            child.action = comp["action"]
            children.append(child)
        
        self.children[id(node)] = children
        return children[0] if children else node

    async def _run_proximity(self, context:dict):
        from co_ai.agents.proximity import ProximityAgent
        proximity_agent = ProximityAgent(
            self.cfg.get("proximity", {}), memory=self.memory, logger=self.logger
        )
        return await proximity_agent.run(context)

    # Set up LATSComponent
    @time_function(logger=None)
    def _generate_completions(self, node: LATSNode, context: dict):
        merged = {
            **context,
            "state": node.state["current"] if isinstance(node.state, dict) else node.state,
            "trace": node.trace,
            "mode": "reason",
            "branching_factor": self.branching_factor
        }
        
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)
        response = self.call_llm(prompt, context=merged)
        
        # First try structured parsing
        completions = self._parse_thought_sections(response)
        
        # Fallback to basic parsing
        if not completions:
            completions = self._fallback_parsing(response)
        
        return completions

    def _parse_thought_sections(self, response: str):
        """
        Extract structured thoughts from LLM response
        Format: ### Thought N\n**Rationale**: ...\n**Action**: ...
        """
        thought_pattern = r"###\s*Thought\s*\d+[\s\S]*?\n\*\*Rationale\*\*:\s*(.*?)(?:\n\*\*Action\*\*:\s*(.*?))?(\n###|\Z)"
        matches = re.findall(thought_pattern, response.strip(), re.DOTALL)
        
        completions = []
        for i, (rationale, action, _) in enumerate(matches):
            completions.append({
                "step": f"Thought {i+1}: {rationale[:50]}...",
                "rationale": rationale.strip(),
                "action": action.strip()
            })
        
        return completions[:self.branching_factor]

    def _fallback_parsing(self, response: str):
        """Fallback parser for raw text responses"""
        # Match Thought N: patterns
        thought_pattern = r"[Tt]hought\s*\d+:\s*(.*?)(?=\n(?:[Tt]hought\s*\d+:\s|\Z))"
        matches = re.findall(thought_pattern, response.strip(), re.DOTALL)

        # Clean and extract
        completions = [{"step": match.strip()} for match in matches if match.strip()]

        # If no thoughts found, use entire response as one hypothesis
        if not completions and response.strip():
            completions = [{"step": response.strip()}]

        return completions[:self.branching_factor]

    @time_function(logger=None)
    def _fallback_parsing(self, response: str):
        """Fallback parser for raw text responses"""
        # Match Thought N: patterns
        thought_pattern = r"[Tt]hought\s*\d+:\s*(.*?)(?=\n(?:[Tt]hought\s*\d+:\s|\Z))"
        matches = re.findall(thought_pattern, response.strip(), re.DOTALL)
        
        completions = []
        for i, match in enumerate(matches[:self.branching_factor]):
            step = match.strip()
            completions.append({
                "step": f"Thought {i+1}: {step}",
                "rationale": "Fallback step due to parsing failure",
                "action": step  # Use step as action
            })
        
        # If no matches found, wrap the whole response
        if not completions and response.strip():
            completions.append({
                "step": "Fallback reasoning step",
                "rationale": "No structured thoughts found",
                "action": response.strip()
            })
        
        return completions

    @time_function(logger=None)
    def _update_state(self, state, action: str):
        """Update state with new action"""
        if isinstance(state, dict):
            new_state = state.copy()
            new_state["current"] = f"{state['current']}\n{action}"
            new_state["trace"] = state.get("trace", []) + [action]
            return new_state
        
        # Fallback: string-based state â†’ wrap into dict
        return {
            "goal": state,
            "current": f"{state}\n{action}",
            "trace": [action]
        }

    @time_function(logger=None)
    def _score_hypothesis(self, hypothesis: dict, context: dict, metrics: str = "lats_node"):
        """Use dimensional scoring system"""
        return super().score_hypothesis(hypothesis, context, metrics)

    @time_function(logger=None)
    def _simulate(self, node: LATSNode, context: dict):
        """Simulate until terminal state"""
        current = node
        
        while not self._is_terminal(current) and len(current.trace) < self.max_depth:
            prompt = self._build_prompt(current, context, mode="simulate")
            response = self.call_llm(prompt, context)
            completions = self._fallback_parsing(response)
            
            if completions:
                action = completions[0]
                new_state = self._update_state(current.state, action)
                current = self.create_node(new_state, current.trace + [action], parent=current)
        
        # Evaluate final node
        return self._get_value(current)

    @time_function(logger=None)
    def _get_value(self, node: LATSNode):
        """Hybrid value function using LM + self-consistency"""
        if self.cfg.get("use_environment", False):
            obs = self.env.step(node.state)
            return obs['reward']
        
        # Safely extract goal from state
        if isinstance(node.state, dict):
            goal_text = node.state.get("goal", "Unknown goal")
        else:
            goal_text = str(node.state)
        
        score_result = self._score_hypothesis(
            {"text": "\n".join(str(node.trace))},
            {"goal": {"goal_text": goal_text}},
            metrics="lats_reflection"
        )
        return score_result["score"] / 100  # Normalize

    @time_function(logger=None)
    def _build_prompt(self, node, context:dict, mode="reason"):
        """Build prompt from node state"""
        if isinstance(node.state, dict):
            state = node.state["current"]
        else:
            state = str(node.state)
        
        merged = {
            **context,
            "state": state,
            "trace": node.trace,
            "mode": mode,
        }   
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)
        print(f"Prompt for {mode}: {prompt[:100]}...")  # Debugging output
        return prompt
    
    def _extract_goal_text(self, state):
        """
        Safely extract goal text from state
        Handles both string and dict-based state
        """
        if isinstance(state, dict):
            return state.get("goal", state.get("current", "Unknown goal"))
        return str(state)
    
    def create_node(self, state, trace, parent=None):
        """Create a new node with proper structure"""
        node = LATSNode(state, trace, parent)
        self.nodes.append(node)
        return node

    def _is_terminal(self, node: LATSNode) -> bool:
        """Check if node is terminal state"""
        return node.is_terminal or len(node.trace) >= self.max_depth

    def _backpropagate(self, node: LATSNode, reward: float):
        """Update node statistics up the tree"""
        while node:
            node.visits += 1
            node.reward += reward
            node = node.parent

    def _uct_score(self, parent_visits: int, child: LATSNode):
        """Calculate UCT score for node selection"""
        if child.visits == 0:
            return float('inf')
        return (child.reward / child.visits) + \
               self.ucb_weight * math.sqrt(math.log(parent_visits) / child.visits)

    def _best_uct(self, node: LATSNode):
        """Select best child using UCT formula"""
        return max(node.children, key=lambda c: self._uct_score(node.visits, c))

    def _select(self, node: LATSNode):
        """Select node for expansion using UCT"""
        while not self._is_terminal(node) and not node.is_leaf():
            node = self._best_uct(node)
        return node

    def _expand(self, node: LATSNode, context: dict):
        """Generate children nodes using agent-specific expansion"""
        completions = self._generate_completions(node, context)
        
        for comp in completions:
            new_state = self._update_state(node.state, comp)
            new_trace = node.trace + [comp]
            

                        # Add mode to context
            score_context = {
                "goal": {"goal_text": context["goal"]["goal_text"]}
            }

            # Generate dimension scores
            score_result = self._score_hypothesis(
                {"text": comp},
                score_context,
                metrics="lats_node"
            )
            
            child = self.create_node(new_state, new_trace, parent=node)
            child.score = score_result.get("score", 0.0)
            child.dimension_scores = score_result.get("scores", {})
            child.reward = child.score  # Initialize reward
            node.children.append(child)

---END-OF-FILE---


"co_ai\agents\literature.py"
---START-OF-FILE---
# co_ai/agents/literature.py

import re

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL
from co_ai.tools import WebSearchTool
from co_ai.utils.file_utils import write_text_to_file


class LiteratureAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy = cfg.get("strategy", "query_and_summarize")
        self.preferences = cfg.get("preferences", ["goal_consistency", "novelty"])
        self.max_results = cfg.get("max_results", 5)
        self.web_search_tool = WebSearchTool(cfg.get("web_search", {}), self.logger)

        self.logger.log("LiteratureAgentInit", {
            "strategy": self.strategy,
            "preferences": self.preferences,
            "max_results": self.max_results,
        })

    async def run(self, context: dict) -> dict:
        self.logger.log("LiteratureQuery", {"context": context})
        goal = self.extract_goal_text(context.get(GOAL))

        # Step 1: Generate search query using LLM
        search_query = self._generate_search_query(context)
        if not search_query:
            self.logger.log("LiteratureQueryFailed", {"goal": goal})
            return context

        self.logger.log("SearchingWeb", {"query": search_query, "goal": goal})

        # Step 2: Perform web search
        results = await self.web_search_tool.search(
            search_query, max_results=self.max_results
        )

        if not results:
            self.logger.log("NoResultsFromWebSearch", {
                "goal_snippet": goal[:60],
                "search_query": search_query,
            })
            return context

        self.logger.log("SearchResult", {"results": results})

        # Step 3: Parse each result with LLM
        parsed_results = []
        for result in results:
            summary_context = {
                **{
                    "title": result.get("title", "no Title"),
                    "link": result.get("url", ""),
                    "snippet": result.get("snippet", ""),
                    "page": result.get("page", ""),
                },
                **context,
            }

            summary = self._summarize_result(summary_context)

            if summary.strip():
                parsed_results.append(f"""
                    [Title: {result["title"]}]({result["url"]})\n
                    Summary: {summary}
                """)

        self.logger.log("LiteratureSearchCompleted", {
            "total_results": len(parsed_results),
            "goal": goal,
            "search_query": search_query,
        })

        context["literature"] = parsed_results

        return context

    def _generate_search_query(self, context: dict) -> str:
        try:
            prompt = self.prompt_loader.load_prompt(self.cfg, context)
            self.logger.log("LLMPromptGenerated_SearchQuery", {"prompt_snippet": prompt[:200]})

            response = self.call_llm(prompt, context)
            self.logger.log("LLMResponseReceived_SearchQuery", {"response_snippet": response[:200]})

            # Structured format
            match = re.search(r"search query:<([^>]+)>", response, re.IGNORECASE)
            if match:
                return match.group(1).strip()

            # Fallback format
            match = re.search(r"(?:query|search)[:\s]+\"([^\"]+)\"", response, re.IGNORECASE)
            if match:
                query = match.group(1).strip()
                self.logger.log("SearchQuery", {"Search Query": query})
                return query

            # Fallback to goal
            goal = self.extract_goal_text(context.get(GOAL))
            self.logger.log("FallingBackToGoalAsQuery", {"goal": goal})
            return f"{goal} productivity study"

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("LiteratureQueryGenerationFailed", {"error": str(e)})
            return f"{context.get('goal', '')} remote work meta-analysis"

    def _summarize_result(self, context: dict) -> str:
        try:
            prompt = self.prompt_loader.from_file(
                self.cfg.get("parse_prompt", "parse.txt"), self.cfg, context
            )
            self.logger.log("LLMPromptGenerated_Summarize", {
                "title": context.get("title", ""),
                "prompt_snippet": prompt[:200]
            })

            raw_summary = self.call_llm(prompt, context)
            self.logger.log("LLMResponseReceived_Summarize", {
                "title": context.get("title", ""),
                "response_snippet": raw_summary[:200]
            })

            # Try extracting "Summary" section
            summary_match = re.search(
                r"Summary\s*\n(?:.*\n)*?\s*(.+?)(?=\n#|\Z)",
                raw_summary,
                re.DOTALL | re.IGNORECASE,
            )
            if summary_match:
                return summary_match.group(1).strip()

            # Fallback: first paragraph of sufficient length
            lines = raw_summary.splitlines()
            for line in lines:
                if len(line.strip()) > 50:
                    return line.strip()

            return ""

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("FailedToParseLiterature", {"error": str(e)})
            return ""
---END-OF-FILE---


"co_ai\agents\lookahead.py"
---START-OF-FILE---
# co_ai/agents/lookahead.py
import re
from dataclasses import asdict

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL, PIPELINE
from co_ai.models import LookaheadORM


class LookaheadAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict):
        goal = self.memory.goals.get_or_create(context.get(GOAL))

        # Build context for prompt template

        pipeline = context.get(PIPELINE, [])

        agent_registry = context.get("agent_registry", {})
        # Current agents and all available agents from the registry
        pipeline_info = {
            step: agent_registry.get(step, {"description": "No description."})
            for step in pipeline
        }
        print(f"Pipeline info: {pipeline_info}")

        all_agents_info = {name: data for name, data in agent_registry.items()}
        print(f"All agents info: {all_agents_info}")

        prompt_context = {
            "goal": goal.goal_text,
            "goal_type": goal.goal_type,
            "focus_area": goal.focus_area,
            "strategy": goal.strategy,
            "llm_suggested_strategy": goal.llm_suggested_strategy,
            PIPELINE: pipeline,
            "pipeline_info": {
                step: agent_registry.get(step, {"description": "No description"})
                for step in pipeline
            },
            "all_agents": agent_registry, 
            **context
        }

        prompt_template = self.prompt_loader.load_prompt(self.cfg, prompt_context)

        # Call LLM to generate anticipated issues and fallbacks
        response = self.call_llm(prompt_template, prompt_context).strip()

        # Store the reflection for traceability
        model_name = self.cfg.get("model").get("name")
        extracted = self.parse_response(response)
        context.update(extracted)
        pipeline = context.get(PIPELINE, [])
        lookahead_data = LookaheadORM(
            goal=goal.id,
            agent_name=self.name,
            model_name=model_name,
            input_pipeline=context.get(PIPELINE),
            suggested_pipeline=["generation", "verifier", "judge"],
            rationale="Input pipeline lacks verification step.",
            reflection="# Predicted Risks\n- Hallucination risk\n- No validation",
            backup_plans=["Plan A: Add fact-checking", "Plan B: Use retrieval-augmented generation"],
            metadata={"domain": "AI Safety"},
            run_id=context.get("run_id")
        )
        self.memory.lookahead.insert(goal.id, lookahead_data)
        # Log the result
        self.logger.log(
            "LookaheadGenerated",
            {
                "goal": goal.goal_text,
                "lookahead": response[:250],  # short preview
            },
        )

        # Store in context
        context[self.output_key] = lookahead_data
        return context

    def parse_response(self, text: str) -> dict:
        import re

        suggested = re.search(r"# Suggested Pipeline\s*(.*?)\n#", text, re.DOTALL)
        rationale = re.search(r"# Rationale\s*(.*)", text, re.DOTALL)

        pipeline = suggested.group(1).strip().splitlines() if suggested else []
        pipeline = [line.strip("- ").strip() for line in pipeline if line.strip()]

        return {
            "suggested_pipeline": pipeline if pipeline else None,
            "rationale": rationale.group(1).strip() if rationale else None,
        }

    def extract_sections(self, text: str) -> dict:
        # Simple section splitting
        risks_match = re.search(r"# Predicted Risks\s*(.*?)(?:#|$)", text, re.DOTALL)
        backups_match = re.search(r"# Backup Plans\s*(.*)", text, re.DOTALL)

        return {
            "rationale": risks_match.group(1).strip() if risks_match else None,
            "backup_plans": [
                line.strip("- ").strip()
                for line in (
                    backups_match.group(1).strip().split("\n") if backups_match else []
                )
                if line.strip()
            ],
        }
---END-OF-FILE---


"co_ai\agents\meta_review.py"
---START-OF-FILE---
# co_ai/agents/meta_review.py

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import (DATABASE_MATCHES, EVOLVED, HYPOTHESES, PROXIMITY,
                             RANKING, REFLECTION, REVIEW)


class MetaReviewAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        # Load preferences from config or default list
        self.preferences = cfg.get("preferences", ["goal_consistency", "plausibility"])

    async def run(self, context: dict) -> dict:
        """
        Synthesize insights from reviews and evolved hypotheses.

        Takes:Well I'd like to understand this what ok I've got to review the reflection ground
        - Evolved hypotheses
        - Reflections (from ReflectionAgent)
        - Reviews (from ReviewAgent)
        - Rankings (from RankingAgent)
        - Strategic directions (from ProximityAgent)

        Returns enriched context with:
        - meta_review summary
        - extracted feedback for future generations
        """

        # Get inputs from context
        evolved = context.get(EVOLVED, [])
        if len(evolved) == 0:
            evolved = context.get(HYPOTHESES, [])
        review = context.get(REVIEW, [])
        reflection = context.get(REFLECTION, [])
        ranking = context.get(RANKING, [])
        strategic_directions = context.get("strategic_directions", [])
        db_matches = context.get(PROXIMITY, {}).get(DATABASE_MATCHES, [])

        # Extract key themes from DB hypotheses
        db_themes = "\n".join(f"- {h[:100]}" for h in db_matches)

        # Extract text if needed
        hypothesis_texts = [h.text if hasattr(h, "text") else h for h in evolved]
        reflection_texts = [
            r.review if hasattr(r, "reflection") else r for r in reflection
        ]
        reviewed_texts = [r.review if hasattr(r, "text") else r for r in review]

        # Log inputs for traceability
        self.logger.log(
            "MetaReviewInput",
            {
                "hypothesis_count": len(hypothesis_texts),
                "evolved_count": len(evolved),
                "review_count": len(reviewed_texts),
                "ranked_count": len(ranking),
                "reflection_count": len(reflection_texts),
                "strategic_directions_count": len(strategic_directions),
                "strategic_directions": strategic_directions,
            },
        )

        merged = {
            **context,
            **{
                EVOLVED: evolved,
                REVIEW: review,
                RANKING: ranking,
                "db_themes": db_themes,
            },
        }
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)

        raw_response = self.call_llm(prompt, context)

        # Store full response for debugging
        self.logger.log(
            "RawMetaReviewOutput", {"raw_output": raw_response[:500] + "..."}
        )

        # Add to context
        context[self.output_key] = raw_response

        # Extract structured feedback
        feedback = self._extract_feedback_from_meta_review(raw_response)
        context["feedback"] = feedback

        return context

    def _extract_feedback_from_meta_review(self, meta_review_text):
        try:
            sections = {}
            current_section = None

            for line in meta_review_text.split("\n"):
                line = line.strip()
                if line.startswith("# Meta-Analysis Summary"):
                    current_section = "summary"
                    sections[current_section] = []
                elif line.startswith("# Recurring Critique Points"):
                    current_section = "recurrent_critiques"
                    sections[current_section] = []
                elif line.startswith("# Strengths Observed"):
                    current_section = "strengths"
                    sections[current_section] = []
                elif line.startswith("# Recommended Improvements"):
                    current_section = "improvements"
                    sections[current_section] = []
                elif line.startswith("# Strategic Research Directions"):
                    current_section = "strategic_directions"
                    sections[current_section] = []
                elif line.startswith("- "):
                    if current_section not in sections:
                        sections[current_section] = []
                    sections[current_section].append(line[2:].strip())

            return {
                "summary": "\n".join(sections.get("summary", [])),
                "recurring_critiques": sections.get("recurrent_critiques", []),
                "strengths_observed": sections.get("strengths", []),
                "recommended_improvements": sections.get("improvements", []),
                "strategic_directions": sections.get("strategic_directions", []),
            }

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("FeedbackExtractionFailed", {"error": str(e)})
            return {}
---END-OF-FILE---


"co_ai\agents\method_planner.py"
---START-OF-FILE---
# co_ai/agents/method_planner.py
import re

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL, HYPOTHESES, PIPELINE
from co_ai.models.method_plan import MethodPlanORM


class MethodPlannerAgent(BaseAgent):
    """
    The MethodPlannerAgent converts abstract research ideas into executable methodological frameworks.

    Based on NOVELSEEK's Method Development Agent:
    > _"The transformation function is represented as: T: I Ã— T Ã— B Ã— L â†’ M"_

    Where:
    - I = Research idea
    - T = Task description
    - B = Baseline implementation
    - L = Relevant literature or knowledge baseAll right Um OK so you're going to come here come here now what you can do is that
    - M = Resulting method plan

    This agent supports both initial planning and iterative refinement of methodologies.
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.max_refinements = cfg.get("max_refinements", 3)
        self.use_refinement = cfg.get("use_refinement", True)

    async def run(self, context: dict) -> dict:
        """
        Main execution loop for the MethodPlannerAgent.

        Args:
            context (dict): Contains goal, hypotheses, baseline code, and literature summary

        Returns:
            dict: Updated context with generated method plan
        """
        # Extract input from context
        goal = context.get(GOAL, {})
        hypothesis = context.get(HYPOTHESES, [])
        baseline_approach = self._get_baseline(goal.get("focus_area"))
        literature_summary = context.get("knowledge_base_summaries", [])
        pipeline_stage = context.get(PIPELINE, "initial_method_plan")

        # Build prompt context
        prompt_context = {
            "idea": hypothesis or goal.get("goal_text"),
            "task_description": self._extract_task_description(goal),
            "baseline_approach": baseline_approach,
            "literature_summary": self._summarize_literature(literature_summary),
            "preferences": self.cfg.get("preferences", ["novelty", "feasibility"]),
        }

        merged = {**context, **prompt_context}

        # Load and render prompt
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)

        # Call LLM to generate method plan
        raw_plan = self.call_llm(prompt, merged)

        # Parse output into structured format
        try:
            plan_data = self.parse_method_plan_output(raw_plan)
        except Exception as e:
            self.logger.log("MethodPlanParseFailed", {"error": str(e), "raw": raw_plan})
            return context

        # Save to database
        method_plan = self._save_to_db(plan_data, merged)

        # Update context with result
        context[self.output_key] = plan_data
        context["method_plan_id"] = method_plan.id
        context["code_plan"] = plan_data.get("code_plan", "")

        self.logger.log(
            "MethodPlanGenerated", {"plan": plan_data, "pipeline_stage": pipeline_stage}
        )

        return context

    def _extract_task_description(self, goal: dict) -> str:
        """
        Extract domain-specific constraints and goals
        Example: Reaction Yield Prediction on Suzuki-Miyaura dataset using SMILES input
        """
        if goal.get("focus_area") == "chemistry":
            return f"{goal.get('goal_text')} ({goal.get('focus_area')})"

        elif goal.get("focus_area") == "nlp":
            return f"{goal.get('goal_text')} ({goal.get('focus_area')})"

        else:
            return goal.get("goal_text", "")

    def _get_baseline(self, focus_area: str) -> str:
        """
        Retrieve baseline implementation from config or file system
        """
        if focus_area == "chemistry":
            return self.cfg.get("baselines").get("reaction_yield_model", "")
        elif focus_area == "nlp":
            return self.cfg.get("baselines").get("sentiment_transformer", "")
        elif focus_area == "cv":
            return self.cfg.get("baselines").get("pointnet_classifier", "")
        else:
            return ""

    def _summarize_literature(self, literature: list) -> str:
        """
        Format literature summaries for use in prompt
        """
        if not literature:
            return "No relevant prior work found."

        return "\n".join(
            [f"- {r['title']}: {r['refined_summary']}" for r in literature[:5]]
        )

    def parse_method_plan_output(self, output: str) -> dict:
        sections = {
            "research_objective": r"\*\*Research Objective:\*\*(.*?)\n\n",
            "key_components": r"\*\*Key Components:\*\*(.*?)\n\n",
            "experimental_plan": r"\*\*Experimental Plan:\*\*(.*?)\n\n",
            "hypothesis_mapping": r"\*\*Hypothesis Mapping:\*\*(.*?)\n\n",
            "search_strategy": r"\*\*Search Strategy:\*\*(.*?)\n\n",
            "knowledge_gaps": r"\*\*Knowledge Gaps:\*\*(.*?)\n\n",
            "next_steps": r"\*\*Next Steps:\*\*(.*?)$",
        }

        result = {}
        for key, pattern in sections.items():
            match = re.search(pattern, output, re.DOTALL)
            if match:
                content = match.group(1).strip()
                if key in ["key_components"]:
                    result[key] = [
                        line.strip() for line in content.splitlines() if line.strip()
                    ]
                else:
                    result[key] = content
            else:
                result[key] = ""

        return result

    def _save_to_db(self, plan_data: dict, context: dict) -> MethodPlanORM:
        """
        Store method plan in ORM with metadata
        """
        plan = MethodPlanORM(
            idea_text=context.get("idea"),
            task_description=plan_data.get("task_description"),
            baseline_method=plan_data.get("baseline_used"),
            literature_summary=plan_data.get("relevant_papers"),
            code_plan=plan_data.get("code_plan"),
            score_novelty=plan_data.get("score_novelty"),
            score_feasibility=plan_data.get("score_feasibility"),
            score_impact=plan_data.get("score_impact"),
            score_alignment=plan_data.get("score_alignment"),
            goal_id=context.get("goal", {}).get("id"),
            focus_area=context.get("goal", {}).get("focus_area"),
            strategy=context.get("goal", {}).get("strategy"),
            evolution_level=0,  # Initial plan
        )

        self.memory.method_plans.add_method_plan(plan.to_dict())  # Or plan.to_dict() if needed
        return plan

    def _refine_plan(self, plan: dict, feedback: dict) -> dict:
        """
        Apply refinement logic based on critique or scoring data
        """
        refinement_prompt = self.prompt_loader.load_prompt(
            "prompts/method_refine.j2", {"current_plan": plan, "feedback": feedback}
        )

        raw_refined = self.call_llm(refinement_prompt)
        return self._parse_plan_output(raw_refined)

    def _score_plan(self, plan: dict, context: dict) -> dict:
        """
        Use ScorerAgent to evaluate methodology quality
        """
        scorer = self.memory.scorer
        scores = scorer.score(plan, context)
        return scores---END-OF-FILE---


"co_ai\agents\model_selector.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL


class ModelSelectorAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.model_rankings = {}

    async def run(self, context: dict) -> dict:
        goal = self.memory.goals.get_or_create(context.get(GOAL))
        preferences = context.get("preferences", [])

        # Use metadata to select best model
        best_model = self._select_best_model(goal, preferences)

        context["model"] = best_model
        return context

    def _select_best_model(self, goal: str, preferences: list):
        """Select best model based on historical rankings"""
        if not preferences:
            return "qwen3"  # default

        if "novelty" in preferences:
            return "mistral"
        elif "biological_plausibility" in preferences:
            return "phi3"
        elif "simplicity" in preferences:
            return "llama3.2-3b"
        else:
            return "qwen3"  # fallback---END-OF-FILE---


"co_ai\agents\mrq_scoring.py"
---START-OF-FILE---
from co_ai.agents import BaseAgent
from co_ai.evaluator import MRQSelfEvaluator
from co_ai.models import EvaluationORM


class MRQScoringAgent(BaseAgent):
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)
        self.evaluator = MRQSelfEvaluator(memory=memory, logger=logger)
        self.score_source = cfg.get("score_source", "mrq")

    async def run(self, context: dict) -> dict:
        goal = context.get("goal")
        goal_text = goal["goal_text"]
        hypotheses = self.memory.hypotheses.get_by_goal(goal_text)
        count_scored = 0

        for hypothesis in hypotheses:
            if not hypothesis.prompt or not hypothesis.text:
                continue

            existing_score = self.memory.evaluations.get_by_hypothesis_id(
                hypothesis.id, source=self.score_source
            )
            if existing_score:
                continue  # Skip if already scored by MR.Q

            # Run evaluator
            result = self.evaluator.score_single(
                prompt=hypothesis.prompt.prompt_text,
                output=hypothesis.text
            )

            # Handle result: could be float or dict of dimensions
            if isinstance(result, dict):
                score_value = result.get("overall", 0.0)
                dimensions = {k: v for k, v in result.items() if k != "overall"}
            else:
                score_value = result
                dimensions = {}

            rationale = (
                f"MRQSelfEvaluator assigned a score of {score_value:.4f} "
                f"based on hypothesis embedding alignment."
            )

            score_obj = EvaluationORM(
                goal_id=hypothesis.goal_id,
                hypothesis_id=hypothesis.id,
                agent_name=self.name,
                model_name=self.model_name,
                evaluator_name="MRQScoringAgent",
                scores=result,
                pipeline_run_id=context.get("pipeline_run_id"),
                extra_data=self.cfg,
                dimensions=dimensions  # ðŸ”¥ store rich sub-scores
            )

            self.memory.evaluations.insert(score_obj)
            count_scored += 1

        self.logger.log(
            "MRQScoringComplete",
            {
                "goal": goal,
                "scored_count": count_scored,
                "total_hypotheses": len(hypotheses),
            },
        )
        return context
---END-OF-FILE---


"co_ai\agents\mrq_strategy.py"
---START-OF-FILE---
from omegaconf import OmegaConf

from co_ai.agents import BaseAgent
from co_ai.constants import GOAL

DEFAULT_PIPELINES = [
    ["generation", "judge"],
    ["generation", "verifier", "judge"],
    ["generation", "reviewer", "judge"],
    ["cot_generator", "reviewer", "judge"],
    ["retriever", "generation", "judge"],
    ["retriever", "cot_generator", "judge"],
    ["retriever", "generation", "verifier", "judge"]
]

class MRQStrategyAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        # Load candidate strategies
        file_path = cfg.get("strategy_file")
        if file_path:
            strategy_cfg = OmegaConf.load(file_path)
            self.candidate_strategies = strategy_cfg.get("candidate_strategies", [])
        else:
            self.candidate_strategies = cfg.get("candidate_strategies", [])

        # Initialize model
        self.trained_ranker = None
        self.training_data = []

        # Attempt training (will be partial if data is incomplete)
        self.train_from_reflection_deltas()


    async def run(self, context: dict) -> dict:
        goal = context.get("goal", {})
        goal_text = goal.get("goal_text", "")

        scored = []
        for pipeline in self.cfg.get("candidate_strategies", DEFAULT_PIPELINES):
            s = self.trained_ranker(pipeline)
            scored.append((pipeline, s))

        scored.sort(key=lambda x: x[1], reverse=True)
        best = scored[0][0]

        context["mrq_suggested_pipeline"] = best
        self.logger.log(
            "MRQPipelineSuggested",
            {"goal": goal_text, "suggested": best, "scored_candidates": scored},
        )

        return context

    def train_from_reflection_deltas(self):
        deltas = self.memory.reflection_deltas.get_all()
        examples = []

        for d in deltas:
            a = d.pipeline_a
            b = d.pipeline_b
            score_a = d.score_a
            score_b = d.score_b

            if not isinstance(a, list) or not isinstance(b, list):
                continue
            if score_a is None or score_b is None:
                continue
            if abs(score_a - score_b) < 0.05:
                continue

            label = "b" if score_b > score_a else "a"
            examples.append({
                "goal_text": d.get("goal_text"),
                "pipeline_a": a,
                "pipeline_b": b,
                "score_a": score_a,
                "score_b": score_b,
                "label": label
            })

        self.training_data = examples
        self.logger.log("MRQTrainingDataLoaded", {"count": len(examples)})

        # Train dummy ranker
        self.trained_ranker = self.symbolic_ranker()

    def symbolic_ranker(self):
        """
        Simple ranker that scores pipelines based on symbolic features.
        Prefers longer pipelines and known strong agents.
        """
        def score(pipeline):
            return (
                len(pipeline)
                + 1.5 * ("verifier" in pipeline)
                + 1.2 * ("reviewer" in pipeline)
                + 1.0 * ("retriever" in pipeline)
                + 0.8 * ("cot_generator" in pipeline)
            )
        return score---END-OF-FILE---


"co_ai\agents\paper_injest.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.models.document import DocumentORM
from co_ai.utils.pdf_tools import extract_text_from_url  # You should have a PDF extraction util


class PaperIngestAgent(BaseAgent):
    def __init__(self, cfg: dict, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    def ingest(self, title: str, url: str, source: str = "arxiv", external_id: str = None):
        self.logger(f"ðŸ“¥ Ingesting paper: {title}")

        # Check if document already exists
        existing = (
            self.session.query(DocumentORM)
            .filter_by(url=url)
            .first()
        )
        if existing:
            self.logger(f"âš ï¸ Document already ingested: {url}")
            return existing

        # Extract text (from PDF or HTML)
        try:
            content = extract_text_from_url(url)
        except Exception as e:
            self.logger(f"âŒ Failed to extract content from {url}: {e}")
            content = None

        # Save to DB
        doc = DocumentORM(
            title=title,
            url=url,
            source=source,
            external_id=external_id,
            content=content,
        )
        self.session.add(doc)
        self.session.commit()
        self.logger(f"âœ… Paper saved: {title} ({url})")
        return doc

    def batch_ingest_from_list(self, paper_list: list[dict]):
        for paper in paper_list:
            self.ingest(
                title=paper.get("title"),
                url=paper.get("url"),
                source=paper.get("source", "arxiv"),
                external_id=paper.get("external_id")
            )
---END-OF-FILE---


"co_ai\agents\pipeline_comparison.py"
---START-OF-FILE---
from collections import defaultdict
from co_ai.models import ScoreORM, EvaluationORM
from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin
from co_ai.analysis.rule_effect_analyzer import RuleEffectAnalyzer
from co_ai.constants import PIPELINE_RUN_ID
import csv
import os
from datetime import datetime
from sqlalchemy import func
from co_ai.models.comparison_preference import ComparisonPreferenceORM

class PipelineComparisonAgent(ScoringMixin, BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.print_results = cfg.get("print_results", True)
        self.tags = cfg.get("tags", [])
        self.analyzer = RuleEffectAnalyzer(session=self.memory.session, logger=self.logger)


    async def run(self, context: dict) -> dict:
        self.logger.log("PipelineComparisonAgentStart", {PIPELINE_RUN_ID: context.get(PIPELINE_RUN_ID)})

        if not self.tags:
            raise ValueError("No tags provided for comparison. Please specify tags in the configuration.")

        results = self.compare_runs(self.tags, goal_id=context.get("goal_id"))

        if self.print_results:
            print("\n=== Pipeline Comparison Results ===")
            for result in results:
                print(f"Goal ID: {result['goal_id']}, Winner: {result['winner']}, Scores: {result['avg_scores']}")

        self.logger.log("PipelineComparisonAgentEnd", {"output_key": self.output_key})
        return context
    

    def compare_runs(self, tags: list[str], goal_id: int = None):
        from co_ai.models.comparison_preference import ComparisonPreferenceORM
        """
        Compare multiple sets of pipeline runs by tag across the same goals.
        Store MR.Q-style preferences when there's a clear winner.
        """
        print(f"\nðŸ” Comparing Pipelines: {tags}\n")

        # Fetch all runs for each tag
        runs_by_tag = {tag: self.memory.pipeline_runs.find({"tag": tag}) for tag in tags}

        # Index runs by goal_id
        from collections import defaultdict
        runs_by_goal = defaultdict(dict)
        for tag, runs in runs_by_tag.items():
            for run in runs:
                if goal_id and run.goal_id != goal_id:
                    continue
                runs_by_goal[run.goal_id][tag] = run

        results = []
        for goal, tag_run_map in runs_by_goal.items():
            if len(tag_run_map) < 2:
                continue  # Skip if fewer than 2 runs to compare

            score_map = {}
            for tag, run in tag_run_map.items():
                score_map[tag] = self.get_multidimensional_score(run.id)

            avg_scores = {tag: scores.get("overall", 0.0) for tag, scores in score_map.items()}
            best_tag = max(avg_scores, key=avg_scores.get)
            is_tie = list(avg_scores.values()).count(avg_scores[best_tag]) > 1
            winner = "Tie" if is_tie else best_tag

            if winner != "Tie":
                loser = [tag for tag in tags if tag != winner][0]
                preference = ComparisonPreferenceORM(
                    goal_id=goal,
                    preferred_tag=winner,
                    rejected_tag=loser,
                    preferred_run_id=tag_run_map[winner].run_id,
                    rejected_run_id=tag_run_map[loser].run_id,
                    preferred_score=avg_scores[winner],
                    rejected_score=avg_scores[loser],
                    dimension_scores=score_map,
                    reason=f"{winner} outperformed {loser} on score {avg_scores[winner]:.2f} > {avg_scores[loser]:.2f}",
                )
                self.memory.session.add(preference)
                self._save_comparison_preference(goal, winner, loser, tag_run_map, avg_scores, score_map)

            results.append({
                "goal_id": goal,
                "avg_scores": avg_scores,
                "winner": winner,
                "run_ids": {tag: tag_run_map[tag].run_id for tag in tag_run_map},
                "dimensions": score_map
            })

        self.memory.session.commit()  # commit all preferences
        self._print_summary(results, tags)
        from co_ai.models.mrq_memory_entry import MRQMemoryEntryORM  # adjust as needed
        from co_ai.utils.time import now_timestamp  # or use datetime.now() directly

        for r in results:
            if r["winner"] == "Tie":
                continue  # skip ties
            goal_id = r["goal_id"]
            preferred_tag = r["winner"]
            rejected_tag = [t for t in tags if t != preferred_tag][0]

            preferred_run_id = r["run_ids"][preferred_tag]
            rejected_run_id = r["run_ids"][rejected_tag]
            preferred_score = r["avg_scores"][preferred_tag]
            rejected_score = r["avg_scores"][rejected_tag]
            dimension_scores = r["dimensions"]

            self.memory.mrq.insert_or_update_mrq_sample(
                goal_id=goal_id,
                preferred_run_id=preferred_run_id,
                rejected_run_id=rejected_run_id,
                preferred_score=preferred_score,
                rejected_score=rejected_score,
                dimension_scores=dimension_scores,
                source="pipeline_comparison",
            )



        self.export_to_csv(results, tags)
        return results
    
    def _get_avg_score(self, run_id: str) -> float:
        scores = (
            self.memory.session.query(ScoreORM)
            .filter(ScoreORM.pipeline_run_id == run_id)
            .all()
        )
        if not scores:
            return 0.0
        return sum(s.score for s in scores) / len(scores)

    def _print_summary(self, results, tags):
        from tabulate import tabulate

        headers = ["Goal ID"] + [f"Score {tag}" for tag in tags] + ["Winner"]
        table_data = []

        for r in results:
            row = [r["goal_id"]] + [f"{r['avg_scores'].get(tag, 0.0):.2f}" for tag in tags] + [r["winner"]]
            table_data.append(row)

        print(tabulate(table_data, headers=headers, tablefmt="fancy_grid"))

        total = len(results)
        win_counts = {tag: sum(1 for r in results if r["winner"] == tag) for tag in tags}
        ties = sum(1 for r in results if r["winner"] == "Tie")

        print("\nâœ… Summary:")
        for tag in tags:
            print(f"  {tag} wins: {win_counts[tag]}")
        print(f"  Ties: {ties} (Total comparisons: {total})")

    def export_to_csv(self, results: list[dict], tags: list[str], filename: str = None):
        """
        Exports the comparison results to a CSV file.
        """
        filename = filename or f"pipeline_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        filepath = os.path.join("reports", filename)

        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        with open(filepath, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            headers = ["goal_id"] + [f"{tag}_score" for tag in tags] + ["winner"]
            writer.writerow(headers)

            for r in results:
                row = [r["goal_id"]] + [r["avg_scores"].get(tag, 0.0) for tag in tags] + [r["winner"]]
                writer.writerow(row)

        print(f"\nðŸ“„ Exported comparison to {filepath}")

    def get_multidimensional_score(self, pipeline_run_id: int) -> dict:
        """
        Fetch and average all score values for a given pipeline run via its evaluations.
        """
         # Step 1: Get latest evaluation per hypothesis
        subquery = (
            self.memory.session.query(
                EvaluationORM.hypothesis_id,
                func.max(EvaluationORM.id).label("latest_eval_id")
            )
            .filter(EvaluationORM.pipeline_run_id == pipeline_run_id)
            .group_by(EvaluationORM.hypothesis_id)
            .subquery()
        )

        latest_eval_ids = [row.latest_eval_id for row in self.memory.session.query(subquery).all()]
        if not latest_eval_ids:
            return {}


        from co_ai.models.score import ScoreORM  # avoid circular import
        from collections import defaultdict

        dim_totals = defaultdict(list)
        for evaluation in latest_eval_ids:
            scores = (
                self.memory.session.query(ScoreORM)
                .filter(ScoreORM.evaluation_id == evaluation)
                .all()
            )
            for s in scores:
                dim_totals[s.dimension].append(s.score)

        averaged = {
            dim: sum(vals) / len(vals) for dim, vals in dim_totals.items()
        }
        if averaged:
            averaged["overall"] = sum(averaged.values()) / len(averaged)

        return averaged

    def _save_comparison_preference(self, goal_id, preferred_tag, rejected_tag, tag_run_map, avg_scores, score_map):
        from co_ai.models.comparison_preference import ComparisonPreferenceORM

        preferred_run_id = tag_run_map[preferred_tag].run_id
        rejected_run_id = tag_run_map[rejected_tag].run_id

        existing = (
            self.memory.session.query(ComparisonPreferenceORM)
            .filter_by(
                goal_id=goal_id,
                preferred_run_id=preferred_run_id,
                rejected_run_id=rejected_run_id,
                source="pipeline_comparison"
            )
            .first()
        )

        data = {
            "goal_id": goal_id,
            "preferred_tag": preferred_tag,
            "rejected_tag": rejected_tag,
            "preferred_run_id": preferred_run_id,
            "rejected_run_id": rejected_run_id,
            "preferred_score": avg_scores[preferred_tag],
            "rejected_score": avg_scores[rejected_tag],
            "dimension_scores": score_map,
            "reason": f"{preferred_tag} outperformed {rejected_tag} ({avg_scores[preferred_tag]:.2f} > {avg_scores[rejected_tag]:.2f})",
            "source": "pipeline_comparison"
        }

        if existing:
            for key, value in data.items():
                setattr(existing, key, value)
        else:
            preference = ComparisonPreferenceORM(**data)
            self.memory.session.add(preference)
---END-OF-FILE---


"co_ai\agents\pipeline_judge.py"
---START-OF-FILE---
import re

from tabulate import tabulate

from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin
from co_ai.analysis.rule_analytics import RuleAnalytics
from co_ai.analysis.rule_effect_analyzer import RuleEffectAnalyzer
from co_ai.constants import PIPELINE_RUN_ID
import csv
import os
from datetime import datetime


class PipelineJudgeAgent(ScoringMixin, BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.print_results = cfg.get("print_results", True)

    async def run(self, context: dict) -> dict:
        self.logger.log("PipelineJudgeAgentStart", {PIPELINE_RUN_ID: context.get(PIPELINE_RUN_ID)})
        hypotheses = context.get("scored_hypotheses", []) or context.get("hypotheses", [])

        self.logger.log("HypothesesReceived", {
            "count": len(hypotheses),
            "source": "scored_hypotheses" if context.get("scored_hypotheses") else "hypotheses"
        })

        for hypo in hypotheses:
            score_result = self.score_hypothesis(
                hypothesis=hypo,
                context=context,
                metrics="pipeline_judge",
            )
            self.logger.log(
                "HypothesisJudged",
                {
                    "hypothesis_id": hypo.get("id"),
                    "score": score_result.get("score"),
                    "details": score_result.get("details", {})
                }
            )
        self.report_rule_analytics(context)
        self.run_rule_effects_evaluation(context)

        self.logger.log("PipelineJudgeAgentEnd", {"output_key": self.output_key})
        return context

    def report_rule_analytics(self, context: dict):
        analytics = RuleAnalytics(db=self.memory, logger=self.logger)
        results = analytics.analyze_rules_for_run(context.get(PIPELINE_RUN_ID))

        if isinstance(results, list) and self.print_results:
            print("\n=== Rule Analytics Summary ===")
            print(f"{'Rule ID':<10}{'Applications':<15}{'Avg Score':<12}")
            print("-" * 40)
            for result in results:
                rule_id = result.get("rule_id")
                count = result.get("count", 0)
                avg_score = result.get("avg_score", 0.0)
                print(f"{rule_id:<10}{count:<15}{avg_score:<12.2f}")
            print("-" * 40)

    def run_rule_effects_evaluation(self, context: dict, output_dir: str = "./reports"):
        analyzer = RuleEffectAnalyzer(session=self.memory.session, logger=self.logger)
        summary = analyzer.analyze(context.get(PIPELINE_RUN_ID))

        # Sort by average score descending
        top_rules = sorted(
            summary.items(), key=lambda x: x[1]["avg_score"], reverse=True
        )

        # Prepare CSV rows
        rows = []
        for rule_id, data in top_rules:
            rows.append([
                rule_id,
                f"{data['avg_score']:.2f}",
                data["count"],
                data["min"],
                data["max"],
                f"{data['std']:.2f}",
                f"{data['success_rate']:.2%}",
            ])

        # Define headers
        headers = [
            "Rule ID",
            "Avg Score",
            "Count",
            "Min",
            "Max",
            "Std Dev",
            "Success Rate â‰¥50%",
        ]

        # Ensure export directory exists
        os.makedirs(output_dir, exist_ok=True)

        # Compose output filename with timestamp
        run_id = context.get(PIPELINE_RUN_ID, "unknown")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rule_effects_{run_id}_{timestamp}.csv"
        filepath = os.path.join(output_dir, filename)

        # Write to CSV
        with open(filepath, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(headers)
            writer.writerows(rows)

        print(f"\nâœ… Rule effect summary saved to: {filepath}")

        # Optional: also return summary if needed
        return summary

    def run_rule_effects_evaluation_console(self, context: dict):
        from tabulate import tabulate
        analyzer = RuleEffectAnalyzer(session=self.memory.session, logger=self.logger)
        summary = analyzer.analyze(context.get(PIPELINE_RUN_ID))

        # Sort by average score descending
        top_rules = sorted(
            summary.items(), key=lambda x: x[1]["avg_score"], reverse=True
        )

        # Prepare table data
        table_data = []
        for rule_id, data in top_rules[:5]:
            table_data.append(
                [
                    rule_id,
                    f"{data['avg_score']:.2f}",
                    data["count"],
                    f"{data['min']} / {data['max']}",
                    f"{data['std']:.2f}",
                    f"{data['success_rate']:.2%}",
                ]
            )

        # Define headers
        headers = [
            "Rule ID",
            "Avg Score",
            "Count",
            "Min / Max",
            "Std Dev",
            "Success Rate â‰¥50",
        ]

        # Print table
        print("\nðŸ“ˆ Top Performing Rules:")
        print(tabulate(table_data, headers=headers, tablefmt="fancy_grid"))

        analyzer.pipeline_run_scores(context=context)
---END-OF-FILE---


"co_ai\agents\pipeline_preference_trainer.py"
---START-OF-FILE---
# co_ai/agents/pipeline_preference_trainer.py

import joblib
from co_ai.models.comparison_preference import ComparisonPreferenceORM
from co_ai.models.goal import GoalORM
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import numpy as np


class PipelinePreferenceTrainerAgent:
    def __init__(self, session, logger=None):
        self.session = session
        self.logger = logger
        self.model = None
        self.scaler = None

    def train(self):
        prefs = self.session.query(ComparisonPreferenceORM).all()
        if not prefs:
            print("No preference data found.")
            return

        X, y = [], []

        for pref in prefs:
            goal = self.session.query(GoalORM).get(pref.goal_id)
            if not goal:
                continue

            # Extract features from preferred and rejected runs
            feat_pref = self._extract_features(goal, pref.preferred_tag, pref.dimension_scores.get(pref.preferred_tag))
            feat_rej = self._extract_features(goal, pref.rejected_tag, pref.dimension_scores.get(pref.rejected_tag))
            if feat_pref is None or feat_rej is None:
                continue

            # Create difference vector (preferred - rejected)
            X.append(np.array(feat_pref) - np.array(feat_rej))
            y.append(1)  # preferred > rejected

        if not X:
            print("No valid feature pairs.")
            return

        X = np.array(X)
        self.scaler = StandardScaler().fit(X)
        X_scaled = self.scaler.transform(X)

        self.model = LogisticRegression()
        self.model.fit(X_scaled, y)

        joblib.dump((self.model, self.scaler), "models/pipeline_preference_model.pkl")
        print("âœ… Trained and saved preference model.")

    def _extract_features(self, goal, tag, scores):
        if not scores:
            return None

        features = []

        # Add basic features â€” e.g. per-dimension score
        for dim in sorted(scores.keys()):
            if dim == "overall":
                continue
            features.append(scores[dim])

        # Add tag as one-hot (TODO: use embedding or smarter encoding)
        tag_feature = hash(tag) % 1000 / 1000.0
        features.append(tag_feature)

        # Add goal length (example feature)
        features.append(len(goal.description or ""))

        return features
---END-OF-FILE---


"co_ai\agents\pipeline.py"
---START-OF-FILE---
import json
import re
from dataclasses import asdict
from datetime import datetime

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import PIPELINE, PIPELINE_RUN_ID, RUN_ID
from co_ai.models import EvaluationORM, RuleApplicationORM


class PipelineJudgeAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        self.logger.log("PipelineJudgeAgentStart", {"run_id": context.get(RUN_ID)})

        goal = context["goal"]
        pipeline = context[PIPELINE]
        hypotheses = context.get("scored_hypotheses") or context.get("hypotheses") or []

        self.logger.log("HypothesesReceived", {
            "count": len(hypotheses),
            "source": "scored_hypotheses" if context.get("scored_hypotheses") else "hypotheses"
        })

        if not hypotheses:
            self.logger.log("JudgementSkipped", {
                "error": "No hypotheses found",
                "goal_id": goal.get("id"),
                "run_id": context.get(RUN_ID)
            })
            return context

        top_hypo = hypotheses[0]
        reflection = context.get("lookahead", {}).get("reflection", "")

        prompt_context = {
            "goal": goal,
            "pipeline": pipeline,
            "hypothesis": top_hypo,
            "lookahead": reflection,
        }

        prompt = self.prompt_loader.load_prompt(self.cfg, prompt_context)
        self.logger.log("PromptLoaded", {"prompt": prompt[:200]})

        judgement = self.call_llm(prompt, prompt_context).strip()
        self.logger.log("JudgementReceived", {"judgement": judgement[:300]})

        # Parse main score
        score_match = re.search(
            r"\*\*?score[:=]?\*\*?\s*([0-9]+(?:\.[0-9]+)?)", judgement, re.IGNORECASE
        )

        score = float(score_match.group(1)) if score_match else None
        rationale = judgement[score_match.end():].strip() if score_match else judgement

        if score is None:
            self.logger.log("ScoreParseFailed", {
                "agent": self.name,
                "judgement": judgement,
                "goal_id": goal.get("id"),
                "run_id": context.get(RUN_ID),
                "emoji": "ðŸš¨â“ðŸ§ "
            })
        else:
            self.logger.log("ScoreParsed", {"score": score, "rationale": rationale[:100]})

        # Parse extra dimensions (look for: relevance, clarity, originality, correctness, etc.)
        dimension_fields = ["relevance", "clarity", "originality", "correctness", "novelty", "feasibility"]
        dimensions = {}
        for field in dimension_fields:
            match = re.search(rf"{field}\s*[:=]?\s*([0-9]+(?:\.[0-9]+)?)", judgement, re.IGNORECASE)
            if match:
                dimensions[field.lower()] = float(match.group(1))

        self.logger.log("ScoreDimensionsParsed", {"dimensions": dimensions})

        # Link rule application if available
        rule_application_id = context.get("symbolic_rule_application_id")

        score_obj = EvaluationORM(
            goal_id=self.get_goal_id(goal),
            hypothesis_id=self.get_hypothesis_id(top_hypo),
            agent_name=self.name,
            model_name=self.model_name,
            evaluator_name="PipelineJudgeAgent",
            score_type="pipeline_judgment",
            score=score,
            rationale=rationale,
            pipeline_run_id=context.get(RUN_ID),
            symbolic_rule_id=rule_application_id,
            extra_data={"raw_response": judgement},
            dimensions=dimensions  # new: parsed dimensions
        )

        self.memory.evaluations.insert(score_obj)
        self.logger.log("ScoreSaved", {
            "score_id": score_obj.id,
            "pipeline_run_id": context.get(PIPELINE_RUN_ID),
            "rule_application_id": rule_application_id,
        })

        context[self.output_key] = {
            "score": score_obj.to_dict(),
            "judgement": judgement
        }

        self.logger.log("PipelineJudgeAgentEnd", {"output_key": self.output_key})
        return context
---END-OF-FILE---


"co_ai\agents\planner.py"
---START-OF-FILE---
from co_ai.agents import DOTSPlannerAgent, LookaheadAgent


class PipelinePlannerAgent:
    def __init__(self, cfg, memory=None, logger=None):
        self.cfg = cfg
        self.memory = memory
        self.logger = logger

        # Load sub-agents if enabled
        self.dots_enabled = cfg.get("dots_enabled", True)
        self.lookahead_enabled = cfg.get("lookahead_enabled", True)

        if self.dots_enabled:
            self.dots = DOTSPlannerAgent(cfg, memory, logger)
        if self.lookahead_enabled:
            self.lookahead = LookaheadAgent(cfg, memory, logger)


    async def run(self, context):
        if self.dots:
            context = await self.dots.run(context)
        if self.lookahead:
            context = await self.lookahead.run(context)
        return context
---END-OF-FILE---


"co_ai\agents\prompt_compiler.py"
---START-OF-FILE---
import dspy

from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.prompt_evolver_mixin import PromptEvolverMixin
from co_ai.compiler.llm_compiler import LLMCompiler
from co_ai.compiler.passes.strategy_mutation_pass import StrategyMutationPass
from co_ai.constants import GOAL
from co_ai.evaluator.evaluator_loader import get_evaluator
from co_ai.models import HypothesisORM


class PromptCompilerAgent(BaseAgent, PromptEvolverMixin):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.prompt_key = cfg.get("prompt_key", "default")
        self.sample_size = cfg.get("sample_size", 20)
        self.generate_count = cfg.get("generate_count", 10)
        self.version = cfg.get("version", 1)
        self.use_strategy_mutation = cfg.get("use_strategy_mutation", False)

        # Initialize the LLM compiler through DSPy
        llm = dspy.LM("ollama_chat/qwen3", api_base="http://localhost:11434")
        self.init_evolver(llm, logger=logger)
        self.compiler = LLMCompiler(llm=self.llm, logger=self.logger)
        self.evaluator = get_evaluator(cfg, memory, self.call_llm, logger)
        if self.use_strategy_mutation:
            self.strategy_pass = StrategyMutationPass(self.evaluator,
                compiler=self.compiler, logger=self.logger
            )


    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        goal_text = self.extract_goal_text(goal)
        total_count = self.sample_size + self.generate_count

        examples = self.memory.prompt.get_prompt_training_set(goal_text, total_count)
        if not examples:
            self.logger.log("PromptCompilerSkipped", {"reason": "no_examples", "goal": goal})
            return context

        refined_prompts = self.evolve_prompts(
            examples, context=context, sample_size=self.sample_size
        )

        scored = []
        hypotheses = []
        for prompt in refined_prompts:
            # Generate hypothesis from the compiled prompt
            hypothesis_text = self.call_llm(prompt, context).strip()
            prompt_id  = self.get_or_save_prompt(prompt).id
            self.logger.log("CompiledPromptHypothesisGenerated", {
                "prompt": prompt[:100],
                "hypothesis": hypothesis_text[:100]
            })
            hyp = HypothesisORM(
                goal_id=goal.get("id"),
                prompt_id=prompt_id,
                source=self.name,
                text=hypothesis_text,
                strategy=self.strategy,
                features={"source": "compiled_prompt"},
                pipeline_run_id=context.get("pipeline_run_id"),
            )
            self.memory.hypotheses.insert(hyp)
            hypotheses.append(hyp.to_dict())
            score = self.score_prompt(
                prompt=prompt,
                reference_output=examples[0].get("hypothesis_text", ""),
                context=context
            )
            scored.append((prompt, score))
            self.add_to_prompt_history(context, prompt, {"source": "dspy_compiler"})

        self.logger.log(
            "PromptCompilerCompleted",
            {"goal": goal, "generated_count": len(refined_prompts)},
        )


        scored_sorted = sorted(scored, key=lambda x: get_winner_score(x[1]), reverse=True)

        context["refined_prompts"] = scored_sorted
        context["hypotheses"] = hypotheses

        # Log top results
        if self.logger:
            for i, (text, score) in enumerate(scored_sorted[:5]):
                self.logger.log(
                    "CompiledPromptScore",
                    {"rank": i + 1, "score": score, "prompt": text[:200]},
                )
        sorted_prompt = scored_sorted[0][0] if scored_sorted else None
        best_score_dict = scored_sorted[0][1] if scored_sorted else {}

        # Assume you already have: sorted_prompt, best_score_dict
        context["compiled_prompt"] = {
            "text": sorted_prompt,  # best-performing prompt
            "score_summary": best_score_dict,
            "compiler_agent": self.name,
        }
        self.logger.log("CompiledPromptSelected", context["compiled_prompt"])

        return context

    def score_prompt(self, prompt: str, reference_output, context:dict) -> float:
        if not self.evaluator:
            return 0.0
        try:
            score = self.evaluator.score_single(prompt, reference_output, context)
            if self.logger:
                self.logger.log("ScoringPrompt", {"score": score, "prompt": prompt[:100], "reference_output": reference_output[:100]})
            return score 
        except Exception as e:
            if self.logger:
                self.logger.log("PromptScoreError", {"prompt": prompt[:100], "error": str(e)})
            return 0.0

def get_winner_score(score_dict):
    if isinstance(score_dict, float):
        return score_dict
    if score_dict["winner"] == "A":
        return score_dict.get("score_a", 0)
    elif score_dict["winner"] == "B":
        return score_dict.get("score_b", 0)
    return 0  # fallback---END-OF-FILE---


"co_ai\agents\prompt_tuning.py"
---START-OF-FILE---
import re
from abc import ABC, abstractmethod

import dspy
from dspy import (BootstrapFewShot, Example, InputField, OutputField, Predict,
                  Signature)

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL


# DSPy signature for prompt refinement: defines input/output fields for tuning
class PromptTuningSignature(Signature):
    goal = InputField(desc="Scientific research goal or question")
    input_prompt = InputField(desc="Original prompt used to generate hypotheses")
    hypotheses = InputField(desc="Best hypothesis generated")
    review = InputField(desc="Expert review of the hypothesis")
    score = InputField(desc="Numeric score evaluating the hypothesis quality")
    refined_prompt = OutputField(desc="Improved version of the original prompt")


# Simple evaluation result class to return from evaluator
class EvaluationResult:
    def __init__(self, score: float, reason: str):
        self.score = score
        self.reason = reason


# Base evaluator interface (not used directly, but useful for future extensions)
class BaseEvaluator(ABC):
    @abstractmethod
    def evaluate(
        self, original: str, proposal: str, metadata: dict = None
    ) -> EvaluationResult:
        pass


# DSPy-based evaluator that can run a Chain-of-Thought program
class DSPyEvaluator(BaseEvaluator):
    def __init__(self):
        self.program = dspy.ChainOfThought(PromptTuningSignature)

    def evaluate(
        self, original: str, proposal: str, metadata: dict = None
    ) -> EvaluationResult:
        result = self.program(
            goal=metadata["goal"],
            input_prompt=original,
            hypotheses=metadata["hypotheses"],
            review=metadata.get("review", ""),
            score=metadata.get("score", 750),
        )
        try:
            score = float(result.score)
        except (ValueError, TypeError):
            score = 0.0
        return EvaluationResult(score=score, reason=result.explanation)


# Main agent class responsible for training and tuning prompts using DSPy
class PromptTuningAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.agent_name = cfg.get("name", "prompt_tuning")
        self.prompt_key = cfg.get("prompt_key", "default")
        self.sample_size = cfg.get("sample_size", 20)
        self.generate_count = cfg.get("generate_count", 10)
        self.current_version = cfg.get("version", 1)

        # Configure DSPy with local LLM (Ollama)
        lm = dspy.LM(
            "ollama_chat/qwen3",
            api_base="http://localhost:11434",
            api_key="",
        )
        dspy.configure(lm=lm)

    async def run(self, context: dict) -> dict:
        goal = self.extract_goal_text(context.get(GOAL))
        generation_count = self.sample_size + self.generate_count
        self.logger.log(
            "PromptTuningExamples",
            {"samples size": self.sample_size, "generation count": generation_count},
        )

        # Get training + validation data
        examples = self.memory.prompt.get_prompt_training_set(goal, generation_count)
        train_data = examples[: self.sample_size]
        val_data = examples[self.sample_size :]

        if not examples:
            self.logger.log(
                "PromptTuningSkipped", {"reason": "no_training_data", "goal": goal}
            )
            return context

        # Build training set for DSPy
        training_set = [
            Example(
                goal=item["goal"],
                input_prompt=item["prompt_text"],
                hypotheses=item["hypothesis_text"],
                review=item.get("review", ""),
                score=item.get("elo_rating", 1000),
            ).with_inputs("goal", "input_prompt", "hypotheses", "review", "score")
            for item in train_data
        ]

        # Wrap our scoring metric so we can inject context during tuning
        def wrapped_metric(example, pred, trace=None):
            return self._prompt_quality_metric(example, pred, context=context)

        # Train prompt-tuning program
        tuner = BootstrapFewShot(metric=wrapped_metric)
        student = Predict(PromptTuningSignature)
        tuned_program = tuner.compile(student=student, trainset=training_set)

        # Use tuned program to generate and store new refined prompt
        await self.generate_and_store_refined_prompts(
            tuned_program, goal, context, val_data
        )
        self.logger.log(
            "PromptTuningCompleted",
            {
                "goal": goal,
                "example_count": len(training_set),
                "generated_count": len(val_data),
            },
        )

        return context

    async def generate_and_store_refined_prompts(
        self, tuned_program, goal: str, context: dict, val_set
    ):
        """
        Generate refined prompts using the tuned DSPy program and store them in the database.

        Args:
            tuned_program: A compiled DSPy program capable of generating refined prompts.
            goal: The scientific goal for this run.
            context: Shared pipeline state.
            val_set: Validation examples to run through the tuned program.
        """

        stored_count = 0
        for i, example in enumerate(val_set):
            try:
                # Run DSPy program on new example
                result = tuned_program(
                    goal=example["goal"],
                    input_prompt=example["prompt_text"],
                    hypotheses=example["hypothesis_text"],
                    review=example.get("review", ""),
                    score=example.get("elo_rating", 1000),
                )

                refined_prompt = result.refined_prompt.strip()

                # Store refined prompt to the DB
                self.memory.prompt.save(
                    goal={"goal_text": example["goal"]},
                    agent_name=self.name,
                    prompt_key=self.prompt_key,
                    prompt_text=refined_prompt,
                    response=None,
                    pipeline_run_id=context.get("pipeline_run_id"),
                    strategy="refined_via_dspy",
                    version=self.current_version + 1,
                )

                stored_count += 1

                # Update context with prompt history
                self.add_to_prompt_history(
                    context, refined_prompt, {"original": example["prompt_text"]}
                )

                self.logger.log(
                    "TunedPromptStored",
                    {"goal": goal, "refined_snippet": refined_prompt[:100]},
                )

            except Exception as e:
                print(f"âŒ Exception: {type(e).__name__}: {e}")
                self.logger.log(
                    "TunedPromptGenerationFailed",
                    {"error": str(e), "example_snippet": str(example)[:100]},
                )

        self.logger.log(
            "BatchTunedPromptsComplete", {"goal": goal, "count": stored_count}
        )

    def _prompt_quality_metric(self, example, pred, context: dict) -> float:
        """Run both prompts and compare results"""
        try:
            prompt_a = example.input_prompt
            prompt_b = pred.refined_prompt
            self.logger.log(
                "PromptQualityCompareStart",
                {
                    "prompt_a_snippet": prompt_a[:100],
                    "prompt_b_snippet": prompt_b[:100],
                },
            )

            hypotheses_a = self.call_llm(prompt_a, context)
            self.logger.log(
                "PromptAResponseGenerated", {"hypotheses_a_snippet": hypotheses_a[:200]}
            )

            hypotheses_b = self.call_llm(prompt_b, context)
            self.logger.log(
                "PromptBResponseGenerated", {"hypotheses_b_snippet": hypotheses_b[:200]}
            )

            # Run comparison
            merged = {
                **context,
                **{
                    "prompt_a": prompt_a,
                    "prompt_b": prompt_b,
                    "hypotheses_a": hypotheses_a,
                    "hypotheses_b": hypotheses_b,
                },
            }
            comparison_prompt = self.prompt_loader.load_prompt(self.cfg, merged)
            self.logger.log(
                "ComparisonPromptConstructed",
                {"comparison_prompt_snippet": comparison_prompt[:200]},
            )

            response = self.call_llm(comparison_prompt, context)
            self.logger.log(
                "ComparisonResponseReceived", {"response_snippet": response[:200]}
            )

            match = re.search(r"better prompt:<([AB])>", response, re.IGNORECASE)
            if match:
                choice = match.group(1).upper()
                score = 1.0 if choice == "B" else 0.5
                self.logger.log(
                    "PromptComparisonResult", {"winner": choice, "score": score}
                )
                return score
            else:
                self.logger.log("PromptComparisonNoMatch", {"response": response})
                return 0.0
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log(
                "PromptQualityMetricError",
                {
                    "error": str(e),
                    "example_input_prompt_snippet": example.input_prompt[:100],
                    "refined_prompt_snippet": getattr(pred, "refined_prompt", "")[:100],
                },
            )
            return 0.0
---END-OF-FILE---


"co_ai\agents\prompt_validation.py"
---START-OF-FILE---
from collections import defaultdict

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import PIPELINE_RUN_ID


class PromptValidationAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        self.logger.log("PromptValidationAgentStart", {PIPELINE_RUN_ID: context.get(PIPELINE_RUN_ID)})

        hypotheses = context.get("scored_hypotheses", [])
        compiled_prompt = context.get("compiled_prompt", {})
        compiled_prompt_hash = hash(compiled_prompt.get("text", ""))

        prompt_to_scores = defaultdict(list)

        for hypo in hypotheses:
            prompt_key = hypo.get("source_prompt")
            score = hypo.get("score")
            if prompt_key is not None and score is not None:
                prompt_to_scores[prompt_key].append(score)

        if not prompt_to_scores:
            self.logger.log("PromptValidationNoData", {"message": "No prompt-score mappings found."})
            return context

        prompt_avg_scores = {
            prompt: sum(scores) / len(scores)
            for prompt, scores in prompt_to_scores.items()
        }

        best_prompt, best_score = max(prompt_avg_scores.items(), key=lambda x: x[1])

        passed_validation = compiled_prompt_hash == best_prompt
        self.logger.log("PromptValidationResult", {
            "compiled_prompt_hash": compiled_prompt_hash,
            "best_prompt_hash": best_prompt,
            "best_score": best_score,
            "passed": passed_validation,
        })

        print("\n=== Prompt Validation ===")
        print(f"Selected prompt hash: {compiled_prompt_hash}")
        print(f"Best-scoring prompt hash: {best_prompt} (avg score: {best_score:.2f})")
        if passed_validation:
            print("\u2705 Prompt compiler selected the best-performing prompt!")
        else:
            print("\u26a0\ufe0f Mismatch: Consider revising compiler strategy or scoring alignment.")

        return context
---END-OF-FILE---


"co_ai\agents\proximity.py"
---START-OF-FILE---
# co_ai/agents/proximity.py
import itertools

import numpy as np

from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import \
    ScoringMixin  # Adjust path if needed
from co_ai.constants import (DATABASE_MATCHES, GOAL, GOAL_TEXT,
                             PIPELINE_RUN_ID, TEXT)
from co_ai.models import EvaluationORM
from co_ai.scoring.proximity import ProximityHeuristicEvaluator
from co_ai.utils import compute_similarity_matrix


class ProximityAgent(ScoringMixin, BaseAgent):
    """
    The Proximity Agent calculates similarity between hypotheses and builds a proximity graph.
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.similarity_threshold = cfg.get("similarity_threshold", 0.75)
        self.max_graft_candidates = cfg.get("max_graft_candidates", 3)
        self.top_k_database_matches = cfg.get("top_k_database_matches", 5)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        goal_text = goal.get(GOAL_TEXT)
        current_hypotheses = self.get_hypotheses(context)

        db_texts = self.memory.hypotheses.get_similar(
            goal_text, limit=self.top_k_database_matches
        )
        self.logger.log(
            "DatabaseHypothesesMatched",
            {
                GOAL: goal,
                "matches": [{"text": h[:100]} for h in db_texts],
            },
        )

        hypotheses_texts = [h.get(TEXT) for h in current_hypotheses]
        all_hypotheses = list(set(hypotheses_texts + db_texts))

        if not all_hypotheses:
            self.logger.log("NoHypothesesForProximity", {"reason": "empty_input"})
            return context

        similarities = compute_similarity_matrix(all_hypotheses, self.memory, self.logger)
        self.logger.log(
            "ProximityGraphComputed",
            {
                "total_pairs": len(similarities),
                "threshold": self.similarity_threshold,
                "top_matches": [
                    {"pair": (h1[:60], h2[:60]), "score": sim}
                    for h1, h2, sim in similarities[:3]
                ],
            },
        )

        graft_candidates = [
            (h1, h2) for h1, h2, sim in similarities if sim >= self.similarity_threshold
        ]
        clusters = self._cluster_hypotheses(graft_candidates)

        context[self.output_key] = {
            "clusters": clusters,
            "graft_candidates": graft_candidates,
            DATABASE_MATCHES: db_texts,
            "proximity_graph": similarities,
        }

        top_similar = similarities[: self.max_graft_candidates]
        to_merge = {
            GOAL: goal,
            "most_similar": "\n".join(
                [
                    f"{i + 1}. {h1} â†” {h2} (sim: {score:.2f})"
                    for i, (h1, h2, score) in enumerate(top_similar)
                ]
            ),
        }

        merged = {**context, **to_merge}
        summary_prompt = self.prompt_loader.load_prompt(self.cfg, merged)

        summary_output = self.call_llm(summary_prompt, merged)
        context["proximity_summary"] = summary_output

        score_result = self.score_hypothesis(
            hypothesis={"text": summary_output, "proximity_analysis": summary_output},
            context=context,
            metrics="proximity",  # Must match your config key: `proximity_score_config`
            evaluator=ProximityHeuristicEvaluator(),
        )
        score = score_result["score"]

        self.logger.log(
            "ProximityAnalysisScored",
            {
                "score": score,
                "analysis": summary_output[:300],
            },
        )

        # Compute additional dimensions
        cluster_count = len(clusters)
        top_k_sims = [sim for _, _, sim in similarities[: self.max_graft_candidates]]
        avg_top_k_sim = sum(top_k_sims) / len(top_k_sims) if top_k_sims else 0.0
        graft_count = len(graft_candidates)

        # Format as new score schema
        structured_scores = {
            "stage": "proximity",
            "dimensions": {
                "proximity_score": {
                    "score": score,
                    "rationale": summary_output,
                    "weight": 1.0,
                },
                "cluster_count": {
                    "score": cluster_count,
                    "rationale": f"Total unique clusters of hypotheses: {cluster_count}",
                    "weight": 0.5,
                },
                "avg_similarity_top_k": {
                    "score": avg_top_k_sim,
                    "rationale": f"Average similarity among top-{self.max_graft_candidates} pairs.",
                    "weight": 0.8,
                },
                "graft_pair_count": {
                    "score": graft_count,
                    "rationale": f"Pairs exceeding similarity threshold ({self.similarity_threshold}).",
                    "weight": 0.7,
                },
            },
        }
        structured_scores["final_score"] = (
            round(
                sum(
                    dim["score"] * dim["weight"]
                    for dim in structured_scores["dimensions"].values()
                )
                / sum(
                    dim["weight"] for dim in structured_scores["dimensions"].values()
                ),
                2,
            ),
        )

        # Save per-hypothesis score
        for hypothesis in current_hypotheses:
            score_obj = EvaluationORM(
                agent_name=self.name,
                model_name=self.model_name,
                goal_id=goal.get("goal_id"),
                hypothesis_id=hypothesis.get("id"),
                evaluator_name=self.name,
                extra_data={"summary": summary_output},
                scores=structured_scores, 
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
            )
            self.memory.evaluations.insert(score_obj)

        return context

    def _cosine(self, a, b):
        a = np.array(list(a), dtype=float)
        b = np.array(list(b), dtype=float)
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

    def _cluster_hypotheses(self, graft_candidates: list[tuple]) -> list[list[str]]:
        clusters = []

        for h1, h2 in graft_candidates:
            found = False
            for cluster in clusters:
                if h1 in cluster or h2 in cluster:
                    if h1 not in cluster:
                        cluster.append(h1)
                    if h2 not in cluster:
                        cluster.append(h2)
                    found = True
                    break
            if not found:
                clusters.append([h1, h2])

        merged_clusters = []
        for cluster in clusters:
            merged = False
            for mc in merged_clusters:
                if set(cluster) & set(mc):
                    mc.extend(cluster)
                    merged = True
                    break
            if not merged:
                merged_clusters.append(list(set(cluster)))

        return merged_clusters
---END-OF-FILE---


"co_ai\agents\ranking.py"
---START-OF-FILE---
# co_ai/agents/ranking.py
import itertools
import random
import re
from typing import Optional

from co_ai.agents.base_agent import BaseAgent


class RankingAgent(BaseAgent):
    """
    The Ranking agent simulates scientific debate between hypotheses using a tournament-style approach.

    From the paper:
    > 'The Ranking agent employs an Elo-based tournament to assess and prioritize generated hypotheses'
    """
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.elo_scores = {}
        self.strategy = cfg.get("strategy", "debate")
        self.max_comparisons = cfg.get("max_comparisons", 6)
        self.initial_elo_score = cfg.get("initial_elo_score", 750)
        self.win_history = []
        self.preferences = cfg.get("preferences", ["novelty", "feasibility"])
        self.elo_scores = {}         # map: hypothesis ID â†’ score
        self.hypothesis_lookup = {}  # map: hypothesis ID â†’ full dict

    async def run(self, context: dict) -> dict:
        """
        Rank hypotheses using pairwise comparisons and Elo updates.

        Args:
            context: Dictionary with keys:
                - hypotheses: list of hypothesis strings
                - goal: research objective
                - preferences: override criteria
        """
        hypotheses = self.get_hypotheses(context)

        if len(hypotheses) < 2:
            self.logger.log("NotEnoughHypothesesForRanking", {
                "count": len(hypotheses),
                "reason": "less than 2 hypotheses"
            })
            context[self.output_key] = [(h, 1000) for h in hypotheses]
            return context

        self._initialize_elo(hypotheses)

        pairs = list(itertools.combinations(hypotheses, 2))
        comparisons = random.sample(pairs, k=min(self.max_comparisons, len(pairs)))

        for hyp1, hyp2 in comparisons:
            prompt = self._build_ranking_prompt(hyp1, hyp2, context)
            response = self.call_llm(prompt, context)
            winner = self._parse_response(response)

            if winner:
                self._update_elo(hyp1, hyp2, winner)
            else:
                self.logger.log(
                    "ComparisonParseFailed",
                    {
                        "prompt_snippet": prompt[:200],
                        "response_snippet": response[:300],
                        "agent": self.__class__.__name__,
                    },
                )

        ranked_ids = sorted(self.elo_scores.items(), key=lambda x: x[1], reverse=True)
        context[self.output_key] = [
            (self.hypothesis_lookup[h_id], score) for h_id, score in ranked_ids
        ]

        self.logger.log(
            "TournamentCompleted",
            {
                "total_hypotheses": len(ranked_ids),
                "win_loss_patterns": self._extract_win_loss_feedback(),
                "preferences": self.preferences,
            },
        )

        return context

    def _initialize_elo(self, hypotheses):
        for h in hypotheses:
            hyp_id = h.get("id") or h.get("text")  # fallback to text if no id
            self.elo_scores[hyp_id] = self.initial_elo_score
            self.hypothesis_lookup[hyp_id] = h

    def _build_ranking_prompt(self, hyp1, hyp2, context):
        return self.prompt_loader.load_prompt(
            self.cfg,
            {
                **context,
                "hypothesis_a": hyp1.get("text"),
                "hypothesis_b": hyp2.get("text"),
            },
        )

    def _conduct_multi_turn_debate(self, context:dict, hyp1:str, hyp2:str, turns:int=3):
        """Simulate multi-turn scientific debate between hypotheses"""
        for i in range(turns):
            prompt = self._build_ranking_prompt(hyp1, hyp2, context=context)
            response = self.call_llm(prompt, context)
            winner = self._parse_response(response)
            if winner:
                self._update_elo(hyp1, hyp2, winner)
            else:
                break


    def _generate_pairwise_comparisons(self, hypotheses):
        """Generate combinations of hypothesis pairs for ranking"""
        return itertools.combinations(hypotheses, 2)

    def _generate_proximity_based_pairs(self, hypotheses):
        """Prioritize comparisons between similar hypotheses"""
        similarities = [
            (h1, h2, self._compute_similarity(h1, h2))
            for h1, h2 in itertools.combinations(hypotheses, 2)
        ]
        return sorted(similarities, key=lambda x: x[2], reverse=True)

    def _extract_win_loss_feedback(self):
        win_counts = {}
        for id1, id2, winner in self.win_history:
            winner_id = id1 if winner == "A" else id2
            win_counts[winner_id] = win_counts.get(winner_id, 0) + 1

        return {
            "top_performers": [
                {
                    "hypothesis": self.hypothesis_lookup[h],
                    "wins": w
                }
                for h, w in sorted(win_counts.items(), key=lambda x: x[1], reverse=True)
            ],
            "total_matches": len(self.win_history),
            "preferences_used": self.preferences
        }

    def _rank_pairwise(self, reviewed, context):
        pairs = list(itertools.combinations(reviewed, 2))
        if not pairs:
            return

        # Limit number of comparisons per round
        comparisons = random.sample(pairs, k=min(self.cfg.get("max_comparisons", 6), len(pairs)))

        for item1, item2 in comparisons:
            hyp1 = item1["hypotheses"]
            hyp2 = item2["hypotheses"]

            merged = {**self.cfg, **{"hypothesis_a": hyp1, "hypothesis_b": hyp2}}


            prompt = self.prompt_loader.load_prompt(merged, context=context)

            self.logger.log("RankingCompare", {"hyp1": hyp1[:60],  "hyp2":hyp2[:60]})

            try:
                response = self.call_llm(prompt, context)
                winner = self._parse_response(response)

                if winner:
                    self._update_elo(hyp1, hyp2, winner)
                else:
                    self.logger.log("ComparisonParseFailed", {
                        "prompt_snippet": prompt[:200],
                        "response_snippet": response[:300]
                    })
            except Exception as e:
                self.logger.log(
                    "ComparisonError",
                    {"error": str(e), "hypotheses": [hyp1[:100], hyp2[:100]]},
                )

    def _update_elo(self, hyp1, hyp2, winner):
        id1 = hyp1.get("id") or hyp1.get("text")
        id2 = hyp2.get("id") or hyp2.get("text")

        K = self.cfg.get("elo_k", 32)
        R1 = 10 ** (self.elo_scores[id1] / 400)
        R2 = 10 ** (self.elo_scores[id2] / 400)
        E1 = R1 / (R1 + R2)
        E2 = R2 / (R1 + R2)

        S1 = 1 if winner == "A" else 0
        S2 = 1 - S1

        self.elo_scores[id1] = max(100, min(2800, self.elo_scores[id1] + K * (S1 - E1)))
        self.elo_scores[id2] = max(100, min(2800, self.elo_scores[id2] + K * (S2 - E2)))

        self.memory.hypotheses.update_elo_rating(id1, self.elo_scores[id1])
        self.memory.hypotheses.update_elo_rating(id2, self.elo_scores[id2])

        self.win_history.append((id1, id2, winner))

        self.logger.log(
            "RankingUpdated",
            {
                "hypothesis_a": id1,
                "hypothesis_b": id2,
                "winner": winner,
                "elo_a": self.elo_scores[id1],
                "elo_b": self.elo_scores[id2],
            },
        )

    def _parse_response(self, response: str) -> Optional[str]:
        """
        Try multiple methods to extract winner from LLM output

        Returns:
            'A' or 'B' based on comparison
        """
        # Try matching structured formats first
        structured_match = re.search(r"better[\s_]?hypothesis[^\w]*([AB12])", response, re.IGNORECASE)
        if structured_match:
            winner_key = structured_match.group(1).upper()
            return "A" if winner_key in ("A", "1") else "B"

        # Try matching natural language statements
        lang_match = re.search(r"(?:prefer|choose|recommend|select)(\s+idea|\s+hypothesis)?[:\s]+([AB12])", response, re.IGNORECASE)
        if lang_match:
            winner_key = lang_match.group(2).upper()
            return "A" if winner_key in ("A", "1") else "B"

        # Try matching conclusion phrases
        conclusion_match = re.search(r"conclude[d]?\s+with\s+better[\s_]idea:\s*(\d)", response, re.IGNORECASE)
        if conclusion_match:
            winner_key = conclusion_match.group(1)
            return "A" if winner_key == "1" else "B"

        # Default fallback logic
        self.logger.log("ParseError", {
                    "error": "Could not extract winner from response",
                    "response": response
                })
        return response---END-OF-FILE---


"co_ai\agents\recovery_parser.py"
---START-OF-FILE---
import json

from co_ai.agents import BaseAgent


class RecoveryParserAgent(BaseAgent):
    def __init__(self, cfg=None, memory=None, logger=None):
        super().__init__(cfg or {}, memory, logger)

    def parse(self, raw_text: str, expected_fields: list[str], regex_hint: str = None) -> dict:
        prompt = self.prompt_loader.load_prompt(self.cfg, {
            "raw_text": raw_text,
            "expected_fields": expected_fields,
            "regex_hint": regex_hint or "None"
        })
        output = self.call_llm(prompt, {})
        try:
            return json.loads(output)
        except Exception as e:
            self.logger and self.logger.log("LLMParseFail", {
                "error": str(e),
                "raw_output": output,
            })
            return {}
---END-OF-FILE---


"co_ai\agents\refiner.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL, HYPOTHESES, PIPELINE, PIPELINE_RUN_ID
from co_ai.models import HypothesisORM
from co_ai.parsers import extract_hypotheses


class RefinerAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        goal = self.extract_goal_text(context.get(GOAL))
        target_agent = self.cfg.get("target_agent", "generation")
        history = context.get("prompt_history", {}).get(target_agent, None)

        if not history:
            self.logger.log("RefinerNoHistoryFound", {
                "target_agent": target_agent,
                "context_keys": list(context.keys())
            })
            return context

        self.logger.log("RefinerStart", {
            "target_agent": target_agent,
            "goal": goal
        })

        original_prompt = history["prompt"]
        original_response = history["response"]
        preferences = history.get("preferences", [])
        original_hypotheses = context.get(HYPOTHESES, [])

        merged = {
            **context,
            "input_prompt": original_prompt,
            "example_output": original_response,
            "preferences": preferences,
        }

        try:
            prompt_improved_prompt = self.prompt_loader.load_prompt(
                self.cfg, context=merged
            )
            self.logger.log("RefinerImprovementPromptLoaded", {
                "snippet": prompt_improved_prompt[:200]
            })

            refined_prompt = self.call_llm(prompt_improved_prompt, context)
            self.logger.log("RefinerPromptGenerated", {
                "prompt_snippet": refined_prompt[:200]
            })

            refined_response = self.call_llm(refined_prompt, context)
            self.logger.log("RefinerResponseGenerated", {
                "response_snippet": refined_response[:200]
            })

            refined_hypotheses = extract_hypotheses(refined_response)
            self.logger.log(
                "RefinerHypothesesExtracted", {"count": len(refined_hypotheses)}
            )

            for h in refined_hypotheses:
                hyp = HypothesisORM(
                    goal=goal,
                    text=h,
                    prompt=refined_prompt,
                    pipeline_run_id=context.get(PIPELINE_RUN_ID),
                    pipeline_signature=context.get(PIPELINE)
                )
                self.memory.hypotheses.insert(hyp)

            info = {
                "original_response": original_response,
                "original_hypotheses": original_hypotheses,
                "refined_prompt": refined_prompt,
                "refined_hypotheses": refined_hypotheses
            }
            refined_merged = {**merged, **info}

            evaluation_template = self.cfg.get("evaluation_template", "evaluate.txt")
            evaluation_prompt = self.prompt_loader.from_file(
                evaluation_template, self.cfg, refined_merged
            )
            self.logger.log("RefinerEvaluationPromptGenerated", {
                "snippet": evaluation_prompt[:200]
            })

            evaluation_response = self.call_llm(evaluation_prompt, context)
            self.logger.log("RefinerEvaluationResponse", {
                "snippet": evaluation_response[:200]
            })

            if " 2" in evaluation_response:
                context[HYPOTHESES] = refined_hypotheses
                self.logger.log("RefinedUpdated", info)
            else:
                self.logger.log("RefinedSkipped", info)

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("RefinerError", {
                "error": str(e),
                "context_keys": list(context.keys())
            })

        return context
---END-OF-FILE---


"co_ai\agents\reflection_delta.py"
---START-OF-FILE---
from dataclasses import asdict
from datetime import datetime

from co_ai.agents.base_agent import BaseAgent
from co_ai.analysis.reflection_delta import compute_pipeline_delta
from co_ai.constants import GOAL
from co_ai.models.reflection_delta import ReflectionDeltaORM


class ReflectionDeltaAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        goal = self.memory.goals.get_or_create(context.get(GOAL))
        if not goal:
            self.logger.log("ReflectionDeltaSkipped", {"reason": "no goal in context"})
            return context
        runs = self.memory.pipeline_runs.get_by_goal_id(goal.id)

        if len(runs) < 2:
            self.logger.log("ReflectionDeltaSkipped", {
                "goal": goal,
                "reason": "only one or zero runs"
            })
            return context

        logged_deltas = 0
        for i, run_a in enumerate(runs):
            for run_b in runs[i+1:]:
                scores_a = self.memory.evaluations.get_by_run_id(run_a.run_id)
                scores_b = self.memory.evaluations.get_by_run_id(run_b.run_id)

                if not scores_a or not scores_b:
                    continue  # skip unscored runs

                delta = compute_pipeline_delta(run_a, run_b, scores_a, scores_b)

                self.memory.reflection_deltas.insert(ReflectionDeltaORM(**delta))
                self.logger.log("ReflectionDeltaLogged", {
                    "goal_id": goal.id,
                    "run_id_a": run_a.run_id,
                    "run_id_b": run_b.run_id,
                    "score_delta": delta.get("score_delta"),
                    "causal": delta.get("causal_improvement")
                })
                logged_deltas += 1

        context["reflection_deltas_logged"] = logged_deltas
        return context
---END-OF-FILE---


"co_ai\agents\reflection.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin


class ReflectionAgent(ScoringMixin, BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = self.get_hypotheses(context)

        reflections = []
        for hyp in hypotheses:
            score = self.score_hypothesis(hyp, context, metrics="reflection")
            self.logger.log(
                "ReflectionScoreComputed",
                score,
            )
            reflections.append(score)

        context[self.output_key] = reflections
        return context---END-OF-FILE---


"co_ai\agents\review.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin


class ReviewAgent(ScoringMixin, BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = self.get_hypotheses(context)
        reviews = []

        for hyp in hypotheses:
            # Score and update review
            score = self.score_hypothesis(hyp, context, metrics="review")
            self.logger.log(
                "ReviewScoreComputed",
                score,
            )
            reviews.append(score)

        context[self.output_key] = reviews
        return context---END-OF-FILE---


"co_ai\agents\rule_generator.py"
---START-OF-FILE---
import statistics
from collections import defaultdict

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import PIPELINE_RUN_ID
from co_ai.models import (EvaluationORM, PipelineRunORM, RuleApplicationORM,
                          SymbolicRuleORM)


class RuleGeneratorAgent(BaseAgent):
    def __init__(self, *args, min_score_threshold=7.5, min_repeat_count=2, **kwargs):
        super().__init__(*args, **kwargs)
        self.min_score_threshold = min_score_threshold
        self.min_repeat_count = min_repeat_count

    async def run(self, context: dict) -> dict:
        self.logger.log("RuleGeneratorStart", {"run_id": context.get(PIPELINE_RUN_ID)})
        new_rules = []

        # Step 1: Get high-scoring runs without rule applications
        high_scores = self._get_high_performance_runs()
        grouped = self._group_by_context_signature(high_scores)

        for sig, entries in grouped.items():
            if len(entries) < self.min_repeat_count:
                continue

            # Check if a rule already exists for this context
            if self.memory.symbolic_rules.exists_by_signature(sig):
                continue

            # Step 2a: Heuristic-based rule suggestion
            rule = self._create_rule_from_signature(sig)
            if rule:
                self.memory.symbolic_rules.insert(rule)
                self.logger.log("HeuristicRuleGenerated", rule.to_dict())
                new_rules.append(rule.to_dict())
            else:
                # Step 2b: LLM fallback
                prompt = self._build_llm_prompt(entries)
                response = self.call_llm(prompt, context)
                self.logger.log("LLMGeneratedRule", {"response": response})
                # Optionally parse/validate this into a SymbolicRuleORM

        context["generated_rules"] = new_rules
        self.logger.log("RuleGeneratorEnd", {"generated_count": len(new_rules)})
        return context

    def _get_high_performance_runs(self):
        scores = self.memory.session.query(EvaluationORM).filter(EvaluationORM.score >= self.min_score_threshold).all()
        runs = []
        for score in scores:
            rule_app = (
                self.memory.session.query(RuleApplicationORM)
                .filter_by(hypothesis_id=score.hypothesis_id)
                .first()
            )
            if rule_app:
                continue  # Skip if rule already applied
            run = self.memory.session.get(PipelineRunORM, score.pipeline_run_id)
            if run:
                runs.append((score, run))
        return runs

    def _group_by_context_signature(self, scored_runs):
        grouped = defaultdict(list)
        for score, run in scored_runs:
            sig = self._make_signature(run.config)
            grouped[sig].append((score, run))
        return grouped

    def _make_signature(self, config: dict) -> str:
        # Could hash or stringify parts of the config, e.g. model + agent + goal
        model = config.get("model", {}).get("name")
        agent = config.get("agent")
        goal_type = config.get("goal", {}).get("goal_type")
        return f"{model}::{agent}::{goal_type}"

    def _create_rule_from_signature(self, sig: str) -> SymbolicRuleORM:
        try:
            model, agent, goal_type = sig.split("::")
            return SymbolicRuleORM(
                source="rule_generator",
                target="agent",
                filter={"goal_type": goal_type},
                attributes={"model.name": model},
                agent_name=agent,
                context_hash=SymbolicRuleORM.compute_context_hash(
                    {"goal_type": goal_type}, {"model.name": model}
                )
            )
        except Exception as e:
            self.logger.log("SignatureParseError", {"sig": sig, "error": str(e)})
            return None

    def _build_llm_prompt(self, entries: list) -> str:
        examples = "\n\n".join(
            f"Goal: {e[1].config.get('goal', {}).get('goal_text')}\n"
            f"Agent: {e[1].config.get('agent')}\n"
            f"Model: {e[1].config.get('model', {}).get('name')}\n"
            f"Score: {e[0].score}" for e in entries[:3]
        )
        return f"""You are a symbolic AI pipeline optimizer.
Given the following successful pipeline configurations with high scores, suggest a symbolic rule that could be applied to future similar tasks.

Examples:
{examples}

Return a YAML snippet that defines a rule with `target`, `filter`, and `attributes`.
"""
---END-OF-FILE---


"co_ai\agents\rule_refiner.py"
---START-OF-FILE---
import statistics

from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import PIPELINE_RUN_ID
from co_ai.models.rule_application import RuleApplicationORM
from co_ai.models.symbolic_rule import SymbolicRuleORM


class RuleRefinerAgent(BaseAgent):
    def __init__(self, *args, min_applications=3, min_score_threshold=6.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.min_applications = min_applications
        self.min_score_threshold = min_score_threshold

    async def run(self, context: dict) -> dict:
        self.logger.log("RuleRefinerStart", {"run_id": context.get(PIPELINE_RUN_ID)})

        rule_apps = self.memory.session.query(RuleApplicationORM).all()
        grouped = self._group_by_rule(rule_apps)

        for rule_id, applications in grouped.items():
            if len(applications) < self.min_applications:
                continue

            scores = [app.result_score for app in applications if app.result_score is not None]
            if not scores:
                continue

            avg_score = statistics.mean(scores)
            if avg_score >= self.min_score_threshold:
                continue  # Only refine low-performing rules

            rule = self.memory.session.query(SymbolicRuleORM).get(rule_id)
            self.logger.log("LowPerformingRuleFound", {
                "rule_id": rule_id,
                "applications": len(scores),
                "avg_score": avg_score
            })

            refinement_prompt = self._build_prompt(rule, scores, applications)
            response = self.call_llm(refinement_prompt, context)
            self.logger.log("RefinementSuggestion", {
                "rule_id": rule_id,
                "suggestion": response.strip()
            })

        self.logger.log("RuleRefinerEnd", {"run_id": context.get(PIPELINE_RUN_ID)})
        return context

    def _group_by_rule(self, rule_apps):
        grouped = {}
        for app in rule_apps:
            grouped.setdefault(app.rule_id, []).append(app)
        return grouped

    def _build_prompt(self, rule: SymbolicRuleORM, scores: list, applications: list) -> str:
        attributes_str = str(rule.attributes) if rule.attributes else "{}"
        filter_str = str(rule.filter) if rule.filter else "{}"
        return f"""You are a symbolic rule optimizer.

The following rule has been applied {len(applications)} times with an average score of {statistics.mean(scores):.2f}.

Filter: {filter_str}
Attributes: {attributes_str}

Here are some example scores: {scores[:5]}

Please suggest improvements to this rule (e.g., modify attributes, adjust filter constraints, or recommend deprecation if not useful). Return only the proposed change."""
---END-OF-FILE---


"co_ai\agents\rule_tuner.py"
---START-OF-FILE---
from collections import defaultdict

from sqlalchemy.orm import joinedload
from tabulate import tabulate

from co_ai.agents.base_agent import BaseAgent
from co_ai.analysis.rule_effect_analyzer import RuleEffectAnalyzer
from co_ai.constants import GOAL, PIPELINE_RUN_ID
from co_ai.memory.symbolic_rule_store import SymbolicRuleStore
from co_ai.models import (EvaluationORM, PipelineRunORM, RuleApplicationORM,
                          SymbolicRuleORM)
from co_ai.models.score import ScoreORM
from co_ai.rules import RuleTuner
from co_ai.utils.high_score_selector import get_high_scoring_runs


class RuleTunerAgent(BaseAgent):
    """
    Analyzes score dimensions from previous pipeline run and adjusts symbolic rule priorities or parameters.
    Also generates new symbolic rules for repeated high-performing configurations.
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.score_target = cfg.get("score_target", "correctness")  # could be 'overall', 'clarity', etc.
        self.rule_store = SymbolicRuleStore(session=self.memory.session, logger=self.logger)
        self.rule_tuner = RuleTuner(memory=self.memory, logger=self.logger)
        self.min_score_threshold = cfg.get("min_score_threshold", 7.5)
        self.min_repeat_count = cfg.get("min_repeat_count", 2)

    async def run(self, context: dict) -> dict:
        run_id = context.get(PIPELINE_RUN_ID)
        goal = context.get(GOAL)

        self.logger.log("RuleTunerAgentStart", {"run_id": run_id, "goal_id": goal.get("id")})

        # Analyze which rules were effective
        analyzer = RuleEffectAnalyzer(session=self.memory.session, logger=self.logger)
        effects = analyzer.analyze(run_id)

        # Score target: e.g. maximize 'correctness' or 'reward'
        best_rules = [rid for rid, data in effects.items() if self.score_target in data.get("dimensions", {})]

        self.logger.log("BestRulesIdentified", {
            "target": self.score_target,
            "count": len(best_rules),
            "examples": best_rules[:5]
        })

        # Tune rule parameters or priorities based on dimension performance
        for rule_id in best_rules:
            result = self.rule_tuner.increase_priority(rule_id)
            self.logger.log("RulePriorityIncreased", {"rule_id": rule_id, "new_priority": result})

        context["rule_tuning"] = {
            "target": self.score_target,
            "top_rules": best_rules
        }

        # Auto-generate rules from high-performing runs without rules
        new_rules = self._generate_rules_from_high_scores()
        context["generated_rules"] = new_rules

        self.logger.log("RuleTunerAgentEnd", {"goal_id": goal.get("id"), "run_id": run_id})
        return context

    def _generate_rules_from_high_scores(self):
        grouped = get_high_scoring_runs(
            session=self.memory.session,
            dimension=self.score_target,
            threshold=self.min_score_threshold,
            min_repeat_count=self.min_repeat_count
        )

        new_rules = []
        for sig, entries in grouped.items():
            if self.memory.symbolic_rules.exists_by_signature(sig):
                continue

            rule = self._create_rule_from_signature(sig)
            if rule:
                self.memory.symbolic_rules.insert(rule)
                self.logger.log("HeuristicRuleGenerated", rule.to_dict())
                new_rules.append(rule.to_dict())

        if new_rules:
            table = [
                [
                    rule.get("agent_name"),
                    rule.get("attributes", {}).get("model.name"),
                    rule.get("filter", {}).get("goal_type"),
                    rule.get("context_hash", "")[:8],  # short hash
                ]
                for rule in new_rules
            ]

            print("\nðŸ“œ New Symbolic Rules Generated:\n")
            print(tabulate(
                table,
                headers=["Agent", "Model", "Goal Type", "Hash"],
                tablefmt="fancy_grid"
            ))
        else:
            print("\nâš ï¸  No new symbolic rules were generated.\n")

        return new_rules

    def _make_signature(self, config: dict) -> str:
        model = config.get("model", {}).get("name")
        agent = config.get("agent")
        goal_type = config.get("goal", {}).get("goal_type")
        return f"{model}::{agent}::{goal_type}"

    def _create_rule_from_signature(self, sig: str) -> SymbolicRuleORM:
        try:
            model, agent, goal_type = sig.split("::")
            return SymbolicRuleORM(
                source="rule_generator",
                target="agent",
                filter={"goal_type": goal_type},
                attributes={"model.name": model},
                agent_name=agent,
                context_hash=SymbolicRuleORM.compute_context_hash(
                    {"goal_type": goal_type}, {"model.name": model}
                )
            )
        except Exception as e:
            self.logger.log("SignatureParseError", {"sig": sig, "error": str(e)})
            return None
---END-OF-FILE---


"co_ai\agents\score_analysis.py"
---START-OF-FILE---
# co_ai/agents/score_analysis_agent.py
import matplotlib.pyplot as plt
import pandas as pd

from co_ai.agents.base_agent import BaseAgent
from co_ai.analysis.score_analyzer import ScoreAnalyzer
from co_ai.constants import PIPELINE_RUN_ID
from co_ai.models import EvaluationORM


class ScoreAnalysisAgent(BaseAgent):
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)
        self.logger.log("AgentInit", {"agent": "ScoreAnalysisAgent"})

    async def run(self, context: dict) -> dict:
        # pipeline_run_id = 1061
        pipeline_run_id = context.get("pipeline_run_id")
        self.logger.log("ScoreAnalysisStarted", {"pipeline_run_id": pipeline_run_id})

        # Fetch all EvaluationORM entries
        raw_scores = self.memory.evaluations.get_by_pipeline_run_id(pipeline_run_id)
        if not raw_scores:
            self.logger.log("ScoreAnalysisEmpty", {"pipeline_run_id": pipeline_run_id})
            return context

        # Flatten the dimension scores from the nested dict structure
        data = []
        for row in raw_scores:
            hypothesis_id = row.get("hypothesis_id")
            stage = row.get("stage")
            scores_dict = row.get("scores", {})
            dims = scores_dict.get("dimensions")
            for dim_name, dim_data in dims.items():
                data.append(
                    {
                        "hypothesis_id": hypothesis_id,
                        "metrics": stage,
                        "dimension": dim_name,
                        "score": dim_data.get("score"),
                        "rationale": dim_data.get("rationale"),
                        "weight": dim_data.get("weight"),
                    }
                )

        df = pd.DataFrame(data)

        # Analyze
        analyzer = ScoreAnalyzer(df)
        desc = analyzer.describe_scores()
        print("\nðŸ“Š Score Summary:\n", desc)

        pca_components, variance_ratio = analyzer.perform_pca()
        print("\nðŸ” PCA Variance Explained:\n", variance_ratio)

        # Plot
        analyzer.plot_pca_clusters(n_clusters=3)

        self.logger.log("ScoreAnalysisCompleted", {
            "pipeline_run_id": pipeline_run_id,
            "pca_variance_ratio": variance_ratio.tolist(),
        })

        return context
---END-OF-FILE---


"co_ai\agents\search_orchestrator.py"
---START-OF-FILE---
from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL
from co_ai.tools import WebSearchTool
from co_ai.tools.arxiv_tool import search_arxiv
from co_ai.tools.cos_sim_tool import get_top_k_similar
from co_ai.tools.huggingface_tool import search_huggingface_datasets
from co_ai.tools.wikipedia_tool import WikipediaTool
from co_ai.agents.knowledge.automind_knowledge_collector import AutoMindKnowledgeCollector

from co_ai.tools.huggingface_tool import recommend_similar_papers

class SearchOrchestratorAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.web_search_tool = WebSearchTool(cfg.get("web_search", {}), self.logger)
        self.wikipedia_tool = WikipediaTool(self.memory, self.logger)
        self.max_results = cfg.get("max_results", 5)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        queries = context.get("search_queries", [])
        goal_id = goal.get("id")
        results = []

        recommend_similar_papers()
        for search_query in queries:
            source = self.route_query(goal, search_query)
            try:
                if source == "arxiv":
                    hits = await search_arxiv([search_query])
                elif source == "huggingface":
                    hits = await search_huggingface_datasets([search_query])
                elif source == "wikipedia":
                    hits = self.wikipedia_tool.find_similar(search_query)
                elif source == "web":
                    hits = await self.web_search_tool.search(
                        search_query, max_results=self.max_results
                    )
                elif source == "automind":
                    collector = AutoMindKnowledgeCollector(self)
                    task_description = context.get("task_description", "AI agent task")
                    knowledge = await collector.retrieve_knowledge(task_description)
                    # Now you can pass this knowledge to planner or tree search agent
                    context["knowledge"] = knowledge
                else:
                    continue

                enriched_hits = [
                    {
                        "query": search_query,
                        "source": source,
                        "result_type": hit.get("type", "unknown"),
                        "title": hit.get("title", hit.get("name", "")),
                        "summary": hit.get("snippet", hit.get("description", "")),
                        "url": hit.get("url", ""),
                        "goal_id": goal_id,
                        "parent_goal": goal.get("goal_text"),
                        "strategy": goal.get("strategy"),
                        "focus_area": goal.get("focus_area"),
                        "extra_data": {
                            "source_specific": hit
                        }
                    }
                    for hit in hits
                ]

                # Store results in DB
                stored_results = self.memory.search_results.bulk_add_results(enriched_hits)
                results.extend(stored_results)

            except Exception as e:
                self.logger.log(
                    "SearchToolFailed",
                    {"query": search_query, "tool": source, "error": str(e)}
                )

        # Save result IDs or ORM objects back to context
        context["search_result_ids"] = [r.id for r in results]
        context["search_results"] = [r.to_dict() for r in results]
        return context

    def route_query(self, goal, query: str) -> str:
        """
        Decide which source to use based on query content.
        """
        query_lower = query.lower()

        # Try fast metadata path first
        source = self.fast_metadata_routing(goal, query_lower)
        if source:
            return source

        # Fallback to semantic similarity
        return self.semantic_fallback_routing(query)

    def fast_metadata_routing(self, goal, query_lower):
        focus_area = goal.get("focus_area", "").lower()
        goal_type = goal.get("goal_type", "").lower()

        if goal_type == "automind":
            return "automind"
        if goal_type == "data_search" or "dataset" in query_lower:
            return "huggingface"
        if goal_type == "model_review" or "model" in query_lower:
            return "arxiv"
        if goal_type == "background" or any(k in query_lower for k in ["overview", "definition"]):
            return "wikipedia"
        if focus_area in ["nlp", "cv", "graph learning"] and "baseline" in query_lower:
            return "arxiv"

        return None

    def semantic_fallback_routing(self, query: str) -> str:
        intent_map = {
            "arxiv": ["find research paper", "latest ML study", "scientific method"],
            "huggingface": ["find dataset", "huggingface model", "nlp corpus"],
            "wikipedia": ["define concept", "what is", "overview of topic"],
            "web": ["general info", "random search", "link to resource"]
        }

        candidates = [(intent, phrase) for intent, phrases in intent_map.items() for phrase in phrases]
        phrases = [p for _, p in candidates]

        top = get_top_k_similar(query, phrases, self.memory, top_k=1)
        best_phrase = top[0][0]

        for intent, phrase in candidates:
            if phrase == best_phrase:
                return intent

        return "web"---END-OF-FILE---


"co_ai\agents\search_result_processing.py"
---START-OF-FILE---
# co_ai/agents/search_result_processing.py
from co_ai.agents.base_agent import BaseAgent
from co_ai.utils.prompt_loader import PromptLoader


class SearchResultProcessingAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy = cfg.get("strategy", "default")
        self.prompt_loader = PromptLoader()
        self.output_key = cfg.get("output_key", "knowledge_base")

    async def run(self, context: dict) -> dict:
        """
        Takes raw search results from SurveyAgent or SearchOrchestratorAgent,
        processes them into structured knowledge.
        """
        goal = context.get("goal")
        raw_results = context.get("search_results", [])

        if not raw_results:
            self.logger.log("NoResultsToProcess", {})
            return context

        processed_results = []

        for result in raw_results:
            # Build prompt context
            prompt_context = {
                "goal_text": goal.get("goal_text"),
                "focus_area": goal.get("focus_area"),
                "goal_type": goal.get("goal_type"),
                "strategy": goal.get("strategy"),
                "preferences": goal.get("preferences", []),
                "title": result.get("title", ""),
                "summary": result.get("summary", ""),
                "source": result.get("source", "")
            }

            # Load prompt template
            prompt = self.prompt_loader.load_prompt(
                self.cfg.get("prompt_file", "prompts/refine_result.j2"),
                prompt_context
            )

            # Call LLM to extract insights
            response = await self.call_llm(prompt)

            try:
                structured = self._parse_refined_result(response)
                processed_results.append(structured)
            except Exception as e:
                self.logger.log("RefinementFailed", {"error": str(e), "raw_response": response})

        # Update context with refined knowledge
        context[self.output_key] = processed_results
        return context

    def _parse_refined_result(self, raw_output: str) -> dict:
        """
        Parse LLM output into structured format (assumes JSON-like structure).
        """
        import json
        return json.loads(raw_output.strip())---END-OF-FILE---


"co_ai\agents\self_aware_planner.py"
---START-OF-FILE---
# --- agents/self_aware_planner_agent.py ---

import copy

from co_ai.agents.automind import AutoMindAgent
from co_ai.agents.symbolic_tuner import SymbolicTunerAgent
from co_ai.memory.symbolic_rule_store import SymbolicRuleStore


class SelfAwarePlannerAgent:
    def __init__(self, base_config):
        self.base_config = base_config
        self.rule_store = SymbolicRuleStore()

    def run(self, goal):
        # Step 1: Apply best symbolic rule to config
        config = copy.deepcopy(self.base_config)
        presets = self.rule_store.to_yaml_presets(top_n=1)
        if presets:
            config.update(presets[0])

        # Step 2: Execute AutoMindAgent with current config
        agent = AutoMindAgent(config)
        tree = agent.run(goal)

        # Step 3: Update symbolic knowledge from experience
        tuner = SymbolicTunerAgent()
        rules = tuner.run()
        self.rule_store.update_from_tuner(rules)

        return tree
---END-OF-FILE---


"co_ai\agents\self_rewarding.py"
---START-OF-FILE---
# File: co_ai/agents/self_rewarding_agent.py

from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from co_ai.agents.base_agent import BaseAgent
from co_ai.evaluator.base import BaseEvaluator
from co_ai.models import EvaluationORM, ScoreORM
from co_ai.prompts import PromptLoader
from co_ai.memory import SymbolicRuleStore
from co_ai.utils import get_high_scoring_runs
from datetime import datetime


@dataclass
class SelfRewardingConfig:
    inner_agent: str  # e.g., "ChainOfThoughtAgent"
    scorer_agent: str = "MRQScoringAgent"
    min_score_threshold: float = 5.0
    use_symbolic_filter: bool = True
    log_to_db: bool = True
    update_rules: bool = False
    rule_store: Optional[SymbolicRuleStore] = None


class SelfRewardingAgent(BaseAgent):
    """
    A self-evaluating agent that wraps another agent,
    scores its own outputs using structured evaluators,
    and optionally updates symbolic rules or training data.
    """

    def __init__(self, cfg: SelfRewardingConfig, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.cfg = cfg
        self.prompt_loader = PromptLoader(cfg.get("prompt_dir", "prompts"))
        self.scorer = self._init_scorer()
        self.inner_agent = self._init_inner_agent()

    def _init_scorer(self) -> BaseEvaluator:
        scorer_type = self.cfg.scorer_agent
        if scorer_type == "MRQScoringAgent":
            from co_ai.evaluator import MRQSelfEvaluator
            return MRQSelfEvaluator(cfg=self.cfg, memory=self.memory, logger=self.logger)
        elif scorer_type == "LLMJudgeEvaluator":
            from co_ai.evaluator import LLMJudgeEvaluator
            return LLMJudgeEvaluator(cfg=self.cfg, memory=self.memory, logger=self.logger)
        else:
            raise ValueError(f"Unsupported scorer type: {scorer_type}")

    def _init_inner_agent(self):
        agent_class = self.cfg.inner_agent
        try:
            module_name, class_name = agent_class.rsplit('.', 1)
            module = __import__(module_name, fromlist=[class_name])
            cls = getattr(module, class_name)
            return cls(cfg=self.cfg, memory=self.memory, logger=self.logger)
        except Exception as e:
            raise ImportError(f"Could not load agent {agent_class}: {e}")

    async def run(self, context: dict) -> dict:
        """Run inner agent, evaluate result, and store score."""
        goal = context.get("goal")
        self.logger.log("SelfRewardingStart", {"goal": goal})

        # Step 1: Run inner agent to generate hypothesis
        hypothesis = await self.inner_agent.run(context)

        # Step 2: Evaluate hypothesis
        scores = self.scorer.evaluate(hypothesis, context=context)

        # Step 3: Store evaluation in DB
        if self.cfg.log_to_db:
            eval_id = self._log_evaluation(hypothesis, scores)

        # Step 4: Rule tuning (optional)
        if self.cfg.update_rules and self.cfg.rule_store:
            self.cfg.rule_store.update_rules_from_scores(scores)

        # Step 5: Decide which hypothesis to return
        best_hypothesis = self._select_best_hypothesis([hypothesis], scores)

        context["best_hypothesis"] = best_hypothesis
        context["scores"] = scores
        self.logger.log("SelfRewardingDone", {"score": scores.get("total", 0)})
        return context

    def _log_evaluation(self, hypothesis, scores):
        """Log hypothesis and scores to database"""
        evaluation = EvaluationORM(
            hypothesis=str(hypothesis),
            goal=hypothesis.get("goal"),
            model_used=self.cfg.get("model", "unknown"),
            timestamp=datetime.now(),
        )
        self.memory.session.add(evaluation)
        self.memory.session.commit()

        for dim, score in scores.items():
            score_orm = ScoreORM(
                evaluation_id=evaluation.id,
                dimension=dim,
                value=score,
                source="self_rewarding"
            )
            self.memory.session.add(score_orm)
        self.memory.session.commit()
        return evaluation.id

    def _select_best_hypothesis(self, hypotheses: List[dict], scores: dict[str, float]) -> Dict:
        """Select hypothesis with highest composite score"""
        # For simplicity, just return the one scored above
        return hypotheses[0] ---END-OF-FILE---


"co_ai\agents\sharpening.py"
---START-OF-FILE---
# co_ai/agents/sharpening.py

from datetime import datetime

from co_ai.agents import BaseAgent
from co_ai.constants import GOAL, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator import MRQSelfEvaluator
from co_ai.models import HypothesisORM
from co_ai.models.sharpening_result import SharpeningResultORM


class SharpeningAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.target = cfg.get("target", "generation")
        self.device = cfg.get("device", "cpu")
        self.evaluator = MRQSelfEvaluator(memory, logger, device=self.device)
        self.templates = cfg.get("templates", ["critic"])

    async def run(self, context: dict):
        goal = self.memory.goals.get_or_create(context.get(GOAL))

        self.evaluator.train_from_database(goal=goal, cfg=self.cfg)

        prompts = context.get("prompt_history", {}).get(self.target, [])
        results = []
        for data in prompts:
            if self.cfg.get("mode", "template") == "judge":
                result = self.run_judge_only(data, context)
            elif self.cfg.get("mode", "template") == "compare_mrq":
                result = self.compare_mrq(data, context)
            else:
                result = self.run_selected(data, context)
            results.append(result)
            if self.cfg.get("log_results", False):
                self.log_sharpening_results(
                    goal, data.get("prompt"), data.get("response"), result
                )
        context[self.output_key] = results
        return context

    def run_selected(self, data: dict, context: dict) -> list[dict]:
        goal = context.get(GOAL)
        results = []
        prompt = data.get("prompt")
        examples = self.memory.hypotheses.get_similar(prompt, 3)
        merged = {**context, **{"prompt": prompt, "examples": examples}}

        if prompt:
            for name in self.templates:
                prompt_template = self.prompt_loader.from_file(name, self.cfg, merged)
                sharpened_hypothesis = self.call_llm(prompt_template, merged)
                hypothesis = data.get("response")  # hypotheses result for prompt
                preferred_output, scores = self.evaluator.judge(
                    goal, prompt, hypothesis, sharpened_hypothesis
                )
                value_a = scores["value_a"]
                value_b = scores["value_b"]
                winner = "a" if value_a >= value_b else "b"
                score = max(value_a, value_b)
                score_diff = abs(value_a - value_b)
                improved = winner == "b"
                comparison = "sharpened_better" if improved else "original_better"
                result = {
                    "template": name,
                    "winner": winner,
                    "improved": improved,
                    "comparison": comparison,
                    "score": round(score, 4),
                    "score_diff": round(score_diff, 4),
                    "output": preferred_output,
                    "raw_scores": scores,
                    "sharpened_hypothesis": sharpened_hypothesis,
                    "prompt_template": prompt_template,
                    PIPELINE: context.get(PIPELINE),
                }
                self.save_improved(goal, prompt_template, result, context)
                results.append(result)
        return sorted(results, key=lambda x: x["score"], reverse=True)

    def compare_mrq(self, data: dict, context: dict) -> list[dict]:
        goal = self.extract_goal_text(context.get(GOAL))
        prompt = data.get("prompt")
        hypothesis = data.get("response")

        # For judge-only, use a simple reflection-based transformation (or leave unchanged)
        sharpened_hypothesis = hypothesis  # no change, just self-judging

        _, scores = self.evaluator.judge(prompt, hypothesis, sharpened_hypothesis, context)
        value_a = scores["value_a"]
        value_b = scores["value_b"]
        winner = "a" if value_a >= value_b else "b"
        score = max(value_a, value_b)
        score_diff = abs(value_a - value_b)
        improved = winner == "b"
        comparison = "sharpened_better" if improved else "original_better"

        result = {
            "template": "judge_only",
            "winner": winner,
            "improved": improved,
            "comparison": comparison,
            "score": round(score, 4),
            "score_diff": round(score_diff, 4),
            "output": hypothesis,
            "raw_scores": scores,
            "sharpened_hypothesis": sharpened_hypothesis,
            "prompt_template": None,
            PIPELINE: context.get(PIPELINE),
        }

        if improved:
            self.save_improved(goal, prompt, result, context)

        return [result]

    async def run_judge_only(self, data: dict, context: dict):
        prompt = data.get("prompt")
        examples = self.memory.hypotheses.get_similar(prompt, 3)
        merged = {**context, **{"prompt": prompt, "examples": examples}}
        prompt_template = self.prompt_loader.from_file(
            "self_reward.txt", self.cfg, merged
        )
        response = self.call_llm(prompt_template, context)
        return response

    def save_improved(self, goal, prompt: str, entry: dict, context: dict):
        if entry["improved"] and self.cfg.get("save_improved", True):
            # Save refined prompt (optional â€“ only if different enough)
            new_prompt_id = self.memory.prompt.save(
                goal=goal,
                agent_name=f"{self.name}_{entry['template']}",
                prompt_key="sharpening",
                prompt_text=prompt,
                response=entry["sharpened_hypothesis"],
                strategy=self.cfg.get("strategy", "default"),
                pipeline_run_id=context.get("pipeline_run_id"),
                meta_data={
                    "original_prompt": prompt,
                    "template": entry["template"],
                    "score_improvement": entry["score_diff"],
                },
            )

            self.logger.log(
                "SharpenedGoalSaved",
                {
                    "prompt_text": prompt[:100],
                },
            )
            hyp = HypothesisORM(
                goal=goal,
                text=entry["sharpened_hypothesis"],
                prompt=prompt,
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
                pipeline_signature=entry.get(PIPELINE),
            )
            # Save new hypothesis for that prompt
            self.memory.hypotheses.insert(hyp)

            self.logger.log(
                "SharpenedHypothesisSaved",
                {
                    "prompt_id": new_prompt_id,
                    "text_snippet": entry["sharpened_hypothesis"][:100],
                    "score": entry["score"],
                },
            )

    from datetime import datetime

    def log_sharpening_results(
        self, goal: str, prompt: str, original_output: str, results: list[dict]
    ):
        for entry in results:
            # Create ORM object
            sharpening_result_orm = SharpeningResultORM(
                goal=goal,
                prompt=prompt,
                template=entry["template"],
                original_output=original_output,
                sharpened_output=entry["sharpened_hypothesis"],
                preferred_output=entry["output"],
                winner=entry["winner"],
                improved=entry["improved"],
                comparison=entry["comparison"],
                score_a=entry["raw_scores"]["value_a"],
                score_b=entry["raw_scores"]["value_b"],
                score_diff=entry["score_diff"],
                best_score=entry["score"],
                prompt_template=entry.get("prompt_template"),
                created_at=datetime.utcnow(),
            )

            # Save to DB via memory
            self.memory.mrq.insert_sharpening_result(sharpening_result_orm)

            # Log the event
            self.logger.log(
                "SharpeningResultSaved",
                sharpening_result_orm.to_dict()
            )---END-OF-FILE---


"co_ai\agents\survey.py"
---START-OF-FILE---
# co_ai/agents/survey.py
from co_ai.agents.base_agent import BaseAgent
from co_ai.constants import GOAL
import re

class SurveyAgent(BaseAgent):
    """
    The Survey Agent generates adaptive search queries for literature exploration.
    
    From the paper:
    > 'The Survey Agent deconstructs the research task into multiple keyword combinations'
    > 'It supports two distinct modes: literature review mode and deep research mode'
    > 'Each idea is mapped to testable components before being executed'
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.max_queries = cfg.get("max_queries", 5)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL, {})
        if not goal:
            self.logger.log("NoGoalProvided", {"reason": "survey_agent_skipped"})
            return context

        # Generate new queries based on goal + baseline + preferences
        prompt_context = {
            "goal_text": goal.get("goal_text"),
            "focus_area": goal.get("focus_area"),
            "baseline_method": context.get("baseline_method", ""),
            "preferences": context.get("preferences", ["novelty", "feasibility"]),
            "previous_ideas": context.get("ideas", [])
        }
        merged = {**self.cfg, **prompt_context}

        prompt = self.prompt_loader.load_prompt(self.cfg, merged)


        raw_output = self.call_llm(prompt, context)
        formatted_output = self.remove_think_blocks(raw_output)
        queries = self._parse_query_response(goal, formatted_output)

        # Store in context for SearchOrchestratorAgent
        context["search_queries"] = queries
        context["search_strategy"] = self.strategy

        self.logger.log("SurveyQueriesGenerated", {
            "queries": queries,
            "strategy_used": self.strategy,
            "pipeline_stage": context.get("pipeline_stage")
        })

        return context

    def _parse_query_response(self, goal, response: str) -> list:
        """Parse LLM output into clean list of search queries"""
        lines = [line.strip() for line in response.splitlines() if line.strip()]
        if not lines:
            # Fallback strategy
            return [
                f"{goal.get('focus_area')} machine learning",
                f"{goal.get('goal_text')}"
            ]
        return lines[:self.max_queries]

    def expand_queries_to_goals(self, queries: list, base_goal: dict) -> list:
        """
        Convert queries into sub-goals for future pipeline stages
        
        Args:
            queries (list): Generated search strings
            base_goal (dict): Original goal
            
        Returns:
            list: List of structured sub-goals
        """
        return [
            {
                "goal_text": q,
                "parent_goal": base_goal.get("goal_text"),
                "focus_area": base_goal.get("focus_area"),
                "strategy": base_goal.get("strategy"),
                "source": "survey_agent"
            }
            for q in queries
        ]

    def remove_think_blocks(self, text: str) -> str:
        return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
---END-OF-FILE---


"co_ai\agents\symbolic_optimizer.py"
---START-OF-FILE---
from collections import defaultdict

from co_ai.agents import BaseAgent
from co_ai.constants import GOAL, PIPELINE
from co_ai.memory.symbolic_rule_store import SymbolicRuleORM


class SymbolicOptimizerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.score_target = cfg.get("score_target", "correctness")
        self.min_scores = cfg.get("min_scores", 2)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL, {})
        goal_type = goal.get("goal_type", "unknown")

        # Step 1: Retrieve score history for this goal type
        score_history = self.memory.evaluations.get_by_goal_type(goal_type)

        # Step 2: Analyze pipelines
        best_pipeline = self.find_best_pipeline(score_history)

        if best_pipeline:
            rule_dict = {
                "target": "pipeline",
                "filter": {"goal_type": goal_type},
                "attributes": {"pipeline": best_pipeline},
                "source": "symbolic_optimizer",
            }

            context["symbolic_suggestion"] = rule_dict

            if self.cfg.get("auto_write_rules", False):
                existing = self.memory.symbolic_rules.find_matching_rule(
                    target="pipeline",
                    filter={"goal_type": goal_type},
                    attributes={"pipeline": best_pipeline},
                )
                if not existing:
                    new_rule = SymbolicRuleORM.from_dict(rule_dict)
                    self.memory.symbolic_rules.insert(new_rule)
                    self.logger.log("SymbolicRuleAutoCreated", rule_dict)

            self.logger.log("SymbolicPipelineSuggestion", {
                "goal_type": goal_type,
                "suggested_pipeline": best_pipeline,
                "score_type": self.score_target
            })

        return context

    def find_best_pipeline(self, score_history):
        scores_by_pipeline = defaultdict(list)

        for score in score_history:
            run_id = score.get("run_id")
            if not run_id:
                continue
            pipeline_run = self.memory.pipeline_runs.get_by_run_id(run_id)
            if not pipeline_run or not pipeline_run.pipeline:
                continue

            str_pipeline = str(pipeline_run.pipeline)
            score_val = score.get("score")
            if score_val is not None:
                scores_by_pipeline[str_pipeline].append(score_val)

        # Only keep pipelines with enough data
        pipeline_scores = {
            pipe: sum(vals) / len(vals)
            for pipe, vals in scores_by_pipeline.items()
            if len(vals) >= self.min_scores
        }

        self.logger.log(
            "PipelineScoreSummary",
            {
                "score_type": self.score_target,
                "pipeline_scores": {
                    pipe: round(avg, 4) for pipe, avg in pipeline_scores.items()
                }
            },
        )

        if not pipeline_scores:
            return None

        best = max(pipeline_scores.items(), key=lambda x: x[1])
        return list(best[0])  # convert stringified list back to list
---END-OF-FILE---


"co_ai\agents\symbolic_tuner.py"
---START-OF-FILE---
# --- agents/symbolic_tuner_agent.py ---

import json
from collections import defaultdict

from co_ai.models import NodeORM


class SymbolicTunerAgent:
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    def run(self):
        # Group metrics by symbolic config
        results = defaultdict(list)

        nodes = self.memory.session.query(NodeORM).all()
        for node in nodes:
            if not node.valid or node.metric is None:
                continue

            config_str = json.dumps(node.config, sort_keys=True)
            results[config_str].append(node.metric)

        symbolic_rules = []
        for config_str, metrics in results.items():
            avg_metric = sum(metrics) / len(metrics)
            rule = {
                "config": json.loads(config_str),
                "mean_score": round(avg_metric, 4),
                "count": len(metrics)
            }
            symbolic_rules.append(rule)

        symbolic_rules.sort(key=lambda r: r["mean_score"], reverse=True)
        return symbolic_rules[:10]  # Return top 10 symbolic patterns


# Example usage:
# tuner = SymbolicTunerAgent()
# rules = tuner.run()
# for rule in rules:
#     print(rule)
---END-OF-FILE---


"co_ai\agents\task_generator_agent.py"
---START-OF-FILE---
# File: co_ai/agents/task_generator_agent.py

from typing import Dict, Any, List, Optional
from co_ai.agents.base_agent import BaseAgent
from co_ai.models.mrq_preference_pair import MRQPreferencePairORM
from co_ai.models.mrq_memory_entry import MRQMemoryEntryORM
import json
import random
import re
from datetime import datetime, timezone


class TaskGeneratorAgent(BaseAgent):
    """
    Generates synthetic problems for self-play loops.
    Uses LADDER-style recursive decomposition and prompt engineering.
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.difficulty_levels = ["easy", "medium", "hard"]

    async def run(self, context: dict) -> dict:
        """Main loop: generate problem â†’ solve â†’ judge â†’ log improvement"""
        goal = context.get("goal", {})
        task_type = goal.get("type", "math")  # e.g., math, logic, coding
        depth = context.get("decomposition_depth", 2)

        # Step 1: Generate problem
        problem = await self.generate_problem(task_type, depth)
        self.logger.log("ProblemGenerated", {"problem": problem[:100]})

        # Step 2: Solve it (via inner agent)
        solution = await self.solve_problem(problem, context)
        self.logger.log("SolutionGenerated", {"solution": solution[:100]})

        # Step 3: Judge quality
        scores = await self.judge_solution(problem, solution)
        self.logger.log("SolutionScored", {"scores": scores})

        # Step 4: Log to MRQ memory
        self.log_to_mrq(problem, solution, scores)

        # Update context with results
        context["generated_problem"] = problem
        context["generated_solution"] = solution
        context["scores"] = scores
        return context

    async def generate_problem(self, task_type: str, depth: int) -> str:
        """Generate problem using prompt templates and recursive decomposition"""
        if depth <= 0:
            return self._generate_base_problem(task_type)

        # Recursive decomposition
        subproblems = []
        for _ in range(random.randint(2, 3)):
            subproblem = await self.generate_problem(task_type, depth - 1)
            subproblems.append(subproblem)

        # Combine subproblems
        prompt = f"""
        Combine these subproblems into a composite {task_type} problem:
        {" ".join([f"Subproblem {i+1}: {p}" for i, p in enumerate(subproblems)])}
        Output only the final problem statement.
        """
        composite_problem = await self.llm(prompt)
        return composite_problem.strip()

    def _generate_base_problem(self, task_type: str) -> str:
        """Generate a base-level problem using templates"""
        templates = {
            "math": [
                "Solve for x: {eq}",
                "What is the value of âˆ«{expr} dx from {a} to {b}?",
                "Find the roots of {poly}"
            ],
            "logic": [
                "Given premises: {premises}, what conclusion follows?",
                "If A implies B and B implies C, does A imply C?"
            ],
            "coding": [
                "Write a Python function to {task}",
                "Debug this code: {code_snippet}"
            ]
        }

        template = random.choice(templates.get(task_type, templates["math"]))
        if "{eq}" in template:
            return template.format(
                eq=self._generate_math_expression(),
                expr=self._generate_math_expression(),
                a=random.randint(1, 10),
                b=random.randint(11, 20),
                poly=self._generate_polynomial(),
                task="reverse a linked list",
                code_snippet="def buggy_func(x): return x + '2'"
            )
        return template

    def _generate_math_expression(self) -> str:
        """Helper: generate random math expressions"""
        return random.choice([
            "sin(x)^2 + cos(x)^2",
            "e^(ix)",
            "lim_{xâ†’0} (sin x)/x"
        ])

    def _generate_polynomial(self) -> str:
        """Helper: generate random polynomials"""
        return random.choice([
            "x^3 - 6x^2 + 11x - 6",
            "2x^2 + 3x + 1",
            "x^4 - 1"
        ])

    async def solve_problem(self, problem: str, context: dict) -> str:
        """Use inner agent to solve the problem"""
        solver_name = self.cfg.get("solver_agent", "ChainOfThoughtAgent")
        try:
            module_name, class_name = solver_name.rsplit('.', 1)
            module = __import__(module_name, fromlist=[class_name])
            solver_cls = getattr(module, class_name)
            solver = solver_cls(cfg=self.cfg, memory=self.memory, logger=self.logger)
            solution = await solver.run({"goal": problem})
            return solution.get("response", "")
        except Exception as e:
            self.logger.error("ProblemSolvingFailed", {"error": str(e)})
            return ""

    async def judge_solution(self, problem: str, solution: str) -> Dict[str, float]:
        """Use MR.Q and LLM judge to score solution"""
        # Use MR.Q for fast embedding alignment
        mrq_score = self._mrq_judge(problem, solution)

        # Use LLM judge for detailed rubric scoring
        llm_scores = await self._llm_judge(problem, solution)

        # Combine scores
        return {
            "mrq_similarity": mrq_score,
            **llm_scores
        }

    def _mrq_judge(self, problem: str, solution: str) -> float:
        """Use MR.Q to compute embedding similarity"""
        try:
            from co_ai.evaluator.mrq_self_evaluator import MRQSelfEvaluator
            evaluator = MRQSelfEvaluator(memory=self.memory, logger=self.logger)
            result = evaluator.score_single(problem, solution, context={})
            return result.get("overall", 0.0)
        except Exception as e:
            self.logger.error("MRQScoringFailed", {"error": str(e)})
            return 0.0

    async def _llm_judge(self, problem: str, solution: str) -> Dict[str, float]:
        """Use structured rubric-based LLM judge"""
        try:
            from co_ai.evaluator.pipeline_judge import PipelineJudgeAgent
            judge = PipelineJudgeAgent(cfg=self.cfg, memory=self.memory, logger=self.logger)
            _, scores = await judge.compare_outputs(problem, solution, solution)
            return scores
        except Exception as e:
            self.logger.error("LLMJudgingFailed", {"error": str(e)})
            return {}

    def log_to_mrq(self, problem: str, solution: str, scores: dict):
        """Log generated problem and solution to MRQ memory"""
        try:
            entry = MRQMemoryEntryORM(
                goal=problem,
                strategy="self_play",
                prompt=problem,
                response=solution,
                reward=scores.get("mrq_similarity", 0.0),
                metadata_=json.dumps({
                    "scores": scores,
                    "source": "task_generator"
                }),
                created_at=datetime.now(timezone.utc)
            )
            self.session.add(entry)
            self.session.commit()
        except Exception as e:
            self.session.rollback()
            self.logger.error("MRQLoggingFailed", {"error": str(e)})---END-OF-FILE---


"co_ai\agents\unified_mrq.py"
---START-OF-FILE---
import os
import pickle
from collections import defaultdict

import numpy as np

from co_ai.agents.base_agent import BaseAgent
from co_ai.evaluator.mrq_trainer import MRQTrainer
from co_ai.models.unified_mrq import UnifiedMRQModelORM
from co_ai.utils.similarity_utils import compute_similarity_matrix


class UnifiedMRQAgent(BaseAgent):
    """
    Unified Multidimensional MR.Q Agent
    - Collects scores across all pipelines and dimensions.
    - Builds contrastive training pairs.
    - Trains a multidimensional preference model.
    - Saves models and logs metadata to DB.
    """

    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)
        self.target_dimensions = cfg.get(
            "target_dimensions", ["correctness", "originality", "clarity", "relevance"]
        )
        self.similarity_threshold = cfg.get("similarity_threshold", 0.85)
        self.top_k_similar = cfg.get("top_k_similar", 20)
        self.min_score_difference = cfg.get("min_score_difference", 10)
        self.output_dir = cfg.get("model_output_dir", "mrq_models")
        self.trainer = MRQTrainer(memory, logger)

    async def run(self, context: dict) -> dict:
        self.logger.log("UnifiedMRQStarted", {})

        # Step 1: Load hypotheses and scores
        hypotheses = self.get_hypotheses(context)
        if not hypotheses:
            self.logger.log("NoHypothesesFound", {})
            return context

        hypothesis_ids = [h["id"] for h in hypotheses]
        evaluations = self.memory.evaluations.get_by_hypothesis_ids(hypothesis_ids)
        evaluation_ids = [e.id for e in evaluations]
        scores = self.memory.scores.get_by_evaluation_ids(evaluation_ids)
        
        # Step 2: Embed and index hypotheses
        embedded = self._index_embeddings(hypotheses)

        print(f"Embedded: {[(k, v[1][:5]) for k, v in embedded.items()]}")

        # Step 3: Collect dimension-wise scores
        score_map = self._group_scores(scores)

        print("Score map keys:", list(score_map.keys()))
        print("Example score entry:", next(iter(score_map.items()), None))

        # Step 4: Generate contrast pairs
        contrast_pairs = self._generate_contrast_pairs(embedded, score_map, context)

        # Step 5: Train model per dimension
        trained_models = self.trainer.train_multidimensional_model(contrast_pairs)
        self.logger.log(
            "UnifiedMRQTrained",
            {
                "pair_count": len(contrast_pairs),
                "dimensions": list(trained_models.keys()),
            },
        )

        # Step 6: Save and log to DB
        os.makedirs(self.output_dir, exist_ok=True)
        for dim, model in trained_models.items():
            path = os.path.join(self.output_dir, f"{dim}_mrq.pkl")
            with open(path, "wb") as f:
                pickle.dump(model, f)

            pair_count = len([p for p in contrast_pairs if p["dimension"] == dim])
            self.memory.session.add(
                UnifiedMRQModelORM(
                    dimension=dim,
                    model_path=path,
                    pair_count=pair_count,
                    trainer_version="v1.0",
                    context={
                        "similarity_threshold": self.similarity_threshold,
                        "min_score_diff": self.min_score_difference,
                    },
                )
            )

        self.memory.session.commit()
        self.logger.log(
            "UnifiedMRQModelsSaved", {"dimensions": list(trained_models.keys())}
        )
        context["unified_mrq_model_paths"] = {
            dim: os.path.join(self.output_dir, f"{dim}_mrq.pkl")
            for dim in trained_models
        }

        return context

    def _index_embeddings(self, hypotheses):
        index = {}
        for hyp in hypotheses:
            text = hyp.get("text")
            if not text:
                continue

            vector = self.memory.embedding.get_or_create(text)
            if vector is not None:
                index[hyp["id"]] = (hyp, np.array(vector))

        return index

    def _group_scores(self, scores):
        grouped = defaultdict(dict)
        for s in scores:
            hypothesis_id = getattr(s.evaluation, "hypothesis_id", None)
            if hypothesis_id and s.dimension:
                grouped[hypothesis_id][s.dimension] = s.score
        return grouped

    def _generate_contrast_pairs(self, embedded: dict, score_map: dict, context: dict) -> list[dict]:
        """
        Given a map of hypothesis_id -> (hypothesis_dict, embedding), and a score_map,
        return all valid contrast pairs where two hypotheses have scores for the same dimensions.
        """
        contrast_pairs = []
        dim_seen = set()

        all_ids = list(embedded.keys())
        self.logger.log(
            "ContrastPairGenerationStart",
            {
                "total_hypotheses": len(all_ids),
                "score_map_keys": list(score_map.keys())[:10],
            },
        )

        for i in range(len(all_ids)):
            for j in range(i + 1, len(all_ids)):
                id_a, id_b = all_ids[i], all_ids[j]

                if id_a not in score_map or id_b not in score_map:
                    continue

                scores_a = score_map[id_a]
                scores_b = score_map[id_b]

                shared_dims = set(scores_a.keys()) & set(scores_b.keys())

                for dim in shared_dims:
                    score_a = scores_a[dim]
                    score_b = scores_b[dim]

                    # Skip if scores are equal
                    if score_a == score_b:
                        continue

                    dim_seen.add(dim)

                    # Get embedding vectors
                    emb_a = embedded[id_a][1]
                    emb_b = embedded[id_b][1]

                    if emb_a is None or emb_b is None:
                        self.logger.log(
                            "MissingEmbeddingInContrast",
                            {"id_a": id_a, "id_b": id_b, "dim": dim},
                        )
                        continue

                    preferred = "a" if score_a > score_b else "b"
                    pair = {
                        "dimension": dim,
                        "prompt": context.get("goal").get("goal_text"),  # Optional: use goal or reasoning task if desired
                        "output_a": embedded[id_a][0]["text"],
                        "output_b": embedded[id_b][0]["text"],
                        "preferred": preferred,
                    }
                    contrast_pairs.append(pair)

        self.logger.log(
            "ContrastPairGenerationComplete",
            {
                "pairs_generated": len(contrast_pairs),
                "dimensions_covered": list(dim_seen),
            },
        )

        return contrast_pairs
---END-OF-FILE---


"co_ai\analysis\__init__.py"
---START-OF-FILE---
from .rubric_classifier import RubricClassifierMixin
from .rubric_clusterer import RubricClusterer
from .score_analyzer import ScoreAnalyzer

---END-OF-FILE---


"co_ai\analysis\reflection_delta.py"
---START-OF-FILE---
# co_ai/analyzer/reflection_delta.py
from datetime import datetime, timezone
from statistics import mean


def compare_pipeline_runs(memory, goal_id):
    runs = memory.pipeline_runs.get_by_goal_id(goal_id)
    if len(runs) < 2:
        return []

    deltas = []
    for i, run_a in enumerate(runs):
        for run_b in runs[i+1:]:
            scores_a = memory.evaluations.get_by_run_id(run_a.run_id)
            scores_b = memory.evaluations.get_by_run_id(run_b.run_id)

            if not scores_a or not scores_b:
                continue  # skip if unscored

            delta = compute_pipeline_delta(run_a, run_b, scores_a, scores_b)
            deltas.append(delta)

    return deltas


def average_score(scores):
    numeric_scores = [s["score"] for s in scores if s.get("score") is not None]
    return round(mean(numeric_scores), 4) if numeric_scores else None

def list_diff(list1, list2):
    return {
        "only_in_a": [x for x in list1 if x not in list2],
        "only_in_b": [x for x in list2 if x not in list1]
    }

def compute_pipeline_delta(run_a, run_b, scores_a, scores_b):
    score_a = average_score(scores_a)
    score_b = average_score(scores_b)

    return {
        "goal_id": run_a.goal_id,
        "run_id_a": run_a.run_id,
        "run_id_b": run_b.run_id,
        "score_a": score_a,
        "score_b": score_b,
        "score_delta": round(score_b - score_a, 4) if score_a is not None and score_b is not None else None,
        "pipeline_a": run_a.pipeline,
        "pipeline_b": run_b.pipeline,
        "pipeline_diff": list_diff(run_a.pipeline, run_b.pipeline),
        "strategy_diff": run_b.strategy != run_a.strategy,
        "model_diff": run_b.model_name != run_a.model_name,
        "rationale_diff": (
            run_a.lookahead_context.get("rationale") if run_a.lookahead_context else None,
            run_b.lookahead_context.get("rationale") if run_b.lookahead_context else None,
        ),
        "created_at": datetime.now(timezone.utc).isoformat(),
    }
---END-OF-FILE---


"co_ai\analysis\rubric_classifier.py"
---START-OF-FILE---
from datetime import datetime, timezone

from co_ai.constants import GOAL
from co_ai.models import PatternStatORM


class RubricClassifierMixin:
    def _load_enabled_rubrics(self, cfg):
        enabled_rubrics = []
        rubrics_cfg = cfg.get("rubrics", [])
        for entry in rubrics_cfg:
            if entry.get("enabled", False):
                enabled_rubrics.append(
                    {
                        "dimension": entry["dimension"],
                        "rubric": entry["rubric"],
                        "options": entry["options"],
                    }
                )
        return enabled_rubrics

    def classify_with_rubrics(self, hypothesis, context, prompt_loader, cfg, logger):
        results = {}
        pattern_file = cfg.get("pattern_prompt_file", "cot_pattern.txt")
        rubrics = self._load_enabled_rubrics(cfg)

        for rubric in rubrics:
            rubric["hypotheses"] = hypothesis.get("text")
            merged = {**context, **rubric}
            prompt_text = prompt_loader.from_file(pattern_file, cfg, merged)
            custom_llm = cfg.get("analysis_model", None)
            result = self.call_llm(prompt_text, merged, custom_llm)
            results[rubric["dimension"]] = result
            logger.log(
                "RubricClassified",
                {
                    "dimension": rubric["dimension"],
                    "rubric": rubric["rubric"],
                    "classification": result,
                },
            )

        return results

    def classify_and_store_patterns(
        self,
        hypothesis,
        context,
        prompt_loader,
        cfg,
        memory,
        logger,
        agent_name,
        score=None,  # Optional numeric score or win count
    ):
        """Classifies rubrics and stores pattern stats for the given hypothesis."""
        pattern = self.classify_with_rubrics(
            hypothesis=hypothesis,
            context=context,
            prompt_loader=prompt_loader,
            cfg=cfg,
            logger=logger,
        )

        goal = context.get(GOAL)
        summarized = self._summarize_pattern(pattern)

        pattern_stats = self.generate_pattern_stats(
            goal, hypothesis, summarized, cfg, agent_name, score
        )
        memory.pattern_stats.insert(pattern_stats)
        logger.log(
            "RubricPatternsStored",
            {"summary": summarized, "goal": goal, "hypothesis": hypothesis},
        )

        context["pattern_stats"] = summarized
        return summarized

    def generate_pattern_stats(
        self,
        goal,
        hypothesis,
        pattern_dict,
        cfg,
        agent_name,
        confidence_score=None,
    ):
        """
        Create PatternStatORM entries for a classified CoT using DB lookup for IDs.
        """
        try:
            # Get or create goal
            goal_id = self.get_goal_id(goal)

            # Get hypothesis by text
            hypothesis_id = self.get_hypothesis_id(hypothesis)
            model_name = cfg.get("model", {}).get("name", "unknown")

            stats = []
            for dimension, label in pattern_dict.items():
                stat = PatternStatORM(
                    goal_id=goal_id,
                    hypothesis_id=hypothesis_id,
                    model_name=model_name,
                    agent_name=agent_name,
                    dimension=dimension,
                    label=label,
                    confidence_score=confidence_score,
                    created_at=datetime.now(timezone.utc).isoformat(),
                )
                stats.append(stat)

            return stats
        except Exception as e:
            print(f"âŒ Failed to generate pattern stats: {e}")
            raise
---END-OF-FILE---


"co_ai\analysis\rubric_clusterer.py"
---START-OF-FILE---
import numpy as np
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity


class RubricClusterer:
    def __init__(self, memory):
        """
        Parameters:
        - memory: allos us to call the embedding object.
        """
        self.memory = memory

    def embed_rubrics(self, rubrics):
        """Embed each rubric using the provided embedding function."""
        embedded = []
        for r in rubrics:
            text = r["rubric"]
            vec = self.memory.embedding.get_or_create(text)
            embedded.append({
                "text": text,
                "dimension": r.get("dimension", "Unknown"),
                "vector": vec
            })
        return embedded

    def cluster_rubrics(self, embedded_rubrics, num_clusters=6):
        vectors = np.array([r["vector"] for r in embedded_rubrics])
        clustering = AgglomerativeClustering(n_clusters=num_clusters)
        labels = clustering.fit_predict(vectors)
        for i, label in enumerate(labels):
            embedded_rubrics[i]["cluster"] = int(label)
        return embedded_rubrics

    def summarize_clusters(self, clustered_rubrics):
        """Pick the most central rubric in each cluster as representative."""
        df = pd.DataFrame(clustered_rubrics)
        summaries = []

        for cluster_id in sorted(df["cluster"].unique()):
            items = df[df["cluster"] == cluster_id]
            vectors = np.stack(items["vector"])
            centroid = np.mean(vectors, axis=0)
            sims = cosine_similarity([centroid], vectors)[0]
            best_idx = np.argmax(sims)
            rep = items.iloc[best_idx]

            summaries.append({
                "cluster": int(cluster_id),
                "representative_rubric": rep["text"],
                "dimension": rep["dimension"],
                "count": len(items)
            })

        return summaries
---END-OF-FILE---


"co_ai\analysis\rule_analytics.py"
---START-OF-FILE---
from collections import defaultdict
from typing import Dict, List, Optional

from tabulate import tabulate

from co_ai.models.rule_application import RuleApplicationORM


class RuleAnalytics:
    def __init__(self, db, logger=None):
        self.db = db
        self.logger = logger

    def get_score_summary(self, rule_id: int) -> dict:
        scores = (
            self.db.session.query(RuleApplicationORM.post_score)
            .filter(RuleApplicationORM.rule_id == rule_id)
            .filter(RuleApplicationORM.post_score != None)
            .all()
        )
        values = [s[0] for s in scores]
        if not values:
            return {"average": None, "count": 0}
        return {
            "average": sum(values) / len(values),
            "count": len(values),
            "min": min(values),
            "max": max(values),
        }

    def get_feedback_summary(self, rule_id: int) -> Dict[str, int]:
        results = (
            self.db.session.query(RuleApplicationORM.change_type)
            .filter(RuleApplicationORM.rule_id == rule_id)
            .all()
        )
        summary = defaultdict(int)
        for (label,) in results:
            if label:
                summary[label] += 1
        return dict(summary)

    def compute_rule_rank(
        self,
        score_avg: Optional[float],
        usage_count: int,
        feedback: Dict[str, int]
    ) -> float:
        """Compute a basic rule quality score. Can be replaced with DPO/MRQ later."""
        if score_avg is None:
            return -float("inf")
        bonus = feedback.get("good", 0)
        penalty = feedback.get("bad", 0) * 0.5
        return score_avg + bonus - penalty

    def analyze_all_rules(self) -> List[dict]:
        rules = self.db.symbolic_rules.get_all_rules()
        output = []
        table_rows = []
        for rule in rules:
            score_summary = self.get_score_summary(rule.id)
            feedback_summary = self.get_feedback_summary(rule.id)
            rank = self.compute_rule_rank(
                score_summary.get("average"), score_summary.get("count"), feedback_summary
            )
            result = {
                "rule_id": rule.id,
                "rule_text": rule.rule_text,
                "target": rule.target,
                "attributes": rule.attributes,
                "score_summary": score_summary,
                "feedback_summary": feedback_summary,
                "rank_score": rank,
            }
            output.append(result)

            avg_score = score_summary.get("average") or 0.0
            score_count = score_summary.get("count") or 0
            pos_feedback = feedback_summary.get("positive", 0)
            neg_feedback = feedback_summary.get("negative", 0)

            # Prepare a table row summary for printout
            table_rows.append([
                rule.id,
                rule.target or "â€”",
                rule.rule_text[:30] + "â€¦" if rule.rule_text and len(rule.rule_text) > 30 else rule.rule_text,
                f"{avg_score:.2f}",
                score_count,
                pos_feedback,
                neg_feedback,
                f"{rank:.2f}",
                ])
            output.append(result)

        # Print final table
        print("\nðŸ“‹ Rule Analysis Summary:")
        headers = [
            "Rule ID",
            "Target",
            "Rule Text",
            "Avg Score",
            "Score Count",
            "ðŸ‘ Feedback",
            "ðŸ‘Ž Feedback",
            "Rank Score",
        ]
        print(tabulate(table_rows, headers=headers, tablefmt="fancy_grid"))
        return output

    def analyze_rules_for_run(self, pipeline_run_id: str) -> List[dict]:
        """Analyze rules used in a specific pipeline run."""
        rule_apps = (
            self.db.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.pipeline_run_id == pipeline_run_id)
            .all()
        )

        rules_by_id = defaultdict(list)
        for app in rule_apps:
            rules_by_id[app.rule_id].append(app)

        output = []
        table_rows = []
        for rule_id, applications in rules_by_id.items():
            scores = [app.post_score for app in applications if app.post_score is not None]
            changes = [app.change_type for app in applications if app.change_type]

            feedback_summary = defaultdict(int)
            for label in changes:
                feedback_summary[label] += 1

            avg_score = sum(scores) / len(scores) if scores else None
            rank = self.compute_rule_rank(avg_score, len(scores), feedback_summary)

            rule = self.db.symbolic_rules.get_by_id(rule_id)
            result = {
                "rule_id": rule_id,
                "rule_text": rule.rule_text,
                "target": rule.target,
                "attributes": rule.attributes,
                "score_summary": {
                    "average": avg_score,
                    "count": len(scores),
                    "min": min(scores) if scores else None,
                    "max": max(scores) if scores else None,
                },
                "feedback_summary": dict(feedback_summary),
                "rank_score": rank,
            }
            output.append(result)

            table_rows.append([
                rule_id,
                rule.target or "â€”",
                rule.rule_text[:30] + "â€¦" if rule.rule_text and len(rule.rule_text) > 30 else rule.rule_text,
                f"{avg_score:.2f}" if avg_score is not None else "â€”",
                len(scores),
                feedback_summary.get("positive", 0),
                feedback_summary.get("negative", 0),
                f"{rank:.2f}" if avg_score is not None else "â€”",
            ])

        print(f"\nðŸ“Š Rule Analysis for Pipeline Run: {pipeline_run_id}")
        headers = [
            "Rule ID", "Target", "Rule Text", "Avg Score", "Score Count",
            "ðŸ‘ Feedback", "ðŸ‘Ž Feedback", "Rank Score",
        ]
        print(tabulate(table_rows, headers=headers, tablefmt="fancy_grid"))

        return output
---END-OF-FILE---


"co_ai\analysis\rule_effect_analyzer.py"
---START-OF-FILE---
import json
import math
from collections import defaultdict
from typing import Optional

from sqlalchemy.orm import Session
from tabulate import tabulate

from co_ai.models import (EvaluationORM, EvaluationRuleLinkORM, PipelineRunORM,
                          RuleApplicationORM)


class RuleEffectAnalyzer:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger

    def _compute_stats(self, values):
        if not values:
            return {}
        avg = sum(values) / len(values)
        std = math.sqrt(sum((x - avg) ** 2 for x in values) / len(values))
        return {
            "avg": avg,
            "min": min(values),
            "max": max(values),
            "std": std,
            "count": len(values),
            "success_rate_â‰¥50": len([v for v in values if v >= 50]) / len(values),
        }
        
    def get_scores_for_evaluation(self, evaluation_id):
        from co_ai.models.score import \
            ScoreORM  # local import to avoid circularity
        return (
            self.session.query(ScoreORM)
            .filter_by(evaluation_id=evaluation_id)
            .order_by(ScoreORM.dimension.asc())
            .all()
        )
    
    def analyze(self, pipeline_run_id: int) -> dict:
        """
        Analyze rule effectiveness by collecting all scores linked to rule applications.

        Returns:
            dict: rule_id â†’ summary of performance metrics, broken down by param config.
        """
        rule_scores = defaultdict(list)
        param_scores = defaultdict(lambda: defaultdict(list))  # rule_id â†’ param_json â†’ scores

        # Join ScoreRuleLinkORM with RuleApplicationORM to filter on pipeline_run_id
        links = (
            self.session.query(EvaluationRuleLinkORM)
            .join(RuleApplicationORM, RuleApplicationORM.id == EvaluationRuleLinkORM.rule_application_id)
            .filter(RuleApplicationORM.pipeline_run_id == pipeline_run_id)
            .all()
        )

        for link in links:
            eval_id = link.evaluation_id
            rule_app = self.session.get(RuleApplicationORM, link.rule_application_id)
            if not eval_id or not rule_app:
                continue

            rule_id = rule_app.rule_id
            try:
                param_key = json.dumps(rule_app.stage_details or {}, sort_keys=True)
            except Exception:
                param_key = "{}"

            scores = self.get_scores_for_evaluation(eval_id)
            for score in scores:
                dim = score.dimension
                val = score.value
                rule_scores[rule_id][dim].append(val)
                param_scores[rule_id][param_key][dim].append(val)

        # Output
        for rule_id, dim_dict in rule_scores.items():
            print(f"\nðŸ“˜ Rule {rule_id} Dimensional Summary:")
            table = []
            for dim, vals in dim_dict.items():
                stats = self._compute_stats(vals)
                table.append([
                    dim,
                    f"{stats['avg']:.2f}",
                    f"{stats['min']:.1f} / {stats['max']:.1f}",
                    f"{stats['std']:.2f}",
                    stats["count"],
                    f"{stats['success_rate_â‰¥50']:.0%}",
                ])
            print(tabulate(
                table,
                headers=["Dimension", "Avg", "Min/Max", "Std", "Count", "Success â‰¥50"],
                tablefmt="fancy_grid"
            ))

            for param_key, dim_subscores in param_scores[rule_id].items():
                print(f"\n    ðŸ”§ Param Config: {param_key}")
                table = []
                for dim, vals in dim_subscores.items():
                    stats = self._compute_stats(vals)
                    table.append([
                        dim,
                        f"{stats['avg']:.2f}",
                        f"{stats['min']:.1f} / {stats['max']:.1f}",
                        f"{stats['std']:.2f}",
                        stats["count"],
                        f"{stats['success_rate_â‰¥50']:.0%}",
                    ])
                print(tabulate(
                    table,
                    headers=["Dimension", "Avg", "Min/Max", "Std", "Count", "Success â‰¥50"],
                    tablefmt="rounded_outline"
                ))

        return rule_scores  # or return summarized dict if needed

    def pipeline_run_scores(self, pipeline_run_id: Optional[int] = None, context: dict = None) -> None:
        """
        Generate a summary log showing all scores for a specific pipeline run.

        Args:
            pipeline_run_id (Optional[int]): ID of the pipeline run to inspect.
            context (dict): Optional context containing 'pipeline_run_id' as fallback.
        """
        if pipeline_run_id is None:
            if context and "pipeline_run_id" in context:
                pipeline_run_id = context["pipeline_run_id"]
            else:
                raise ValueError("No pipeline_run_id provided or found in context.")

        pipeline_run = self.session.get(PipelineRunORM, pipeline_run_id)
        if not pipeline_run:
            raise ValueError(f"No pipeline run found with ID {pipeline_run_id}")

        scores = (
            self.session.query(EvaluationORM)
            .filter(EvaluationORM.pipeline_run_id == pipeline_run_id)
            .all()
        )

        if not scores:
            if self.logger:
                self.logger.log(
                    "PipelineRunScoreSummary",
                    {
                        "pipeline_run_id": pipeline_run_id,
                        "total_scores": 0,
                        "message": "No scores found",
                    },
                )
            return

        table_rows = []
        for score in scores:
            rule_app_link = (
                self.session.query(EvaluationRuleLinkORM)
                .filter(EvaluationRuleLinkORM.evaluation_id == score.id)
                .first()
            )
            rule_app = (
                self.session.get(RuleApplicationORM, rule_app_link.rule_application_id)
                if rule_app_link
                else None
            )

            row = [
                score.id,
                score.agent_name or "N/A",
                score.model_name or "N/A",
                score.evaluator_name or "N/A",
                score.scores,
                rule_app.rule_id if rule_app else "â€”",
                score.hypothesis_id or "â€”",
            ]
            table_rows.append(row)

        headers = [
            "Score ID",
            "Agent",
            "Model",
            "Evaluator",
            "Type",
            "Value",
            "Rule ID",
            "Hypothesis ID",
        ]

        # Print the table
        print(f"\nðŸ“Š Scores for Pipeline Run {pipeline_run_id}:")
        print(tabulate(table_rows, headers=headers, tablefmt="fancy_grid"))

        if self.logger:
            self.logger.log("PipelineRunScoreSummary", {
                "pipeline_run_id": pipeline_run_id,
                "total_scores": len(scores)
            })
---END-OF-FILE---


"co_ai\analysis\score_analyzer.py"
---START-OF-FILE---
# analysis/score_analyzer.py
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression


class ScoreAnalyzer:
    def __init__(self, score_data: pd.DataFrame):
        """
        Expected format:All right
        - 'hypothesis_id': str
        - 'dimension': str
        - 'score': float
        - Optional: 'outcome' (e.g., final ranking, human eval)
        """
        self.df = score_data
        self.pivot = self.df.pivot(index='hypothesis_id', columns='dimension', values='score')

    def describe_scores(self):
        return self.pivot.describe()

    def fit_linear_regression(self, outcome_col: str):
        merged = self.pivot.copy()
        merged[outcome_col] = self.df.drop_duplicates(subset='hypothesis_id').set_index('hypothesis_id')[outcome_col]
        merged = merged.dropna()
        X = merged.drop(columns=[outcome_col])
        y = merged[outcome_col]
        model = LinearRegression().fit(X, y)
        return model, dict(zip(X.columns, model.coef_))

    def perform_pca(self, n_components=2):
        pca = PCA(n_components=n_components)
        components = pca.fit_transform(self.pivot.fillna(0))
        return components, pca.explained_variance_ratio_

    def cluster_outputs(self, n_clusters=3):
        km = KMeans(n_clusters=n_clusters, n_init=10)
        labels = km.fit_predict(self.pivot.fillna(0))
        return labels

    def plot_pca_clusters(self, n_clusters=3):
        components, _ = self.perform_pca()
        labels = self.cluster_outputs(n_clusters=n_clusters)
        plt.scatter(components[:, 0], components[:, 1], c=labels, cmap='tab10')
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title('PCA of Score Vectors (Colored by Cluster)')
        plt.show()---END-OF-FILE---


"co_ai\analysis\score_evaluator.py"
---START-OF-FILE---
import re
from pathlib import Path

import yaml
from jinja2 import Template
from sqlalchemy.orm import Session
from tabulate import tabulate

from co_ai.models.evaluation import EvaluationORM
from co_ai.models.score import ScoreORM
from co_ai.models.score_dimension import ScoreDimensionORM


class ScoreEvaluator:
    def __init__(self, dimensions, prompt_loader, cfg, logger, memory):
        self.dimensions = dimensions
        self.prompt_loader = prompt_loader
        self.cfg = cfg
        self.logger = logger
        self.memory = memory
        self.output_format = cfg.get("output_format", "simple")  # default fallback

    @classmethod
    def from_db(
        cls, session: Session, stage: str, prompt_loader=None, agent_config=None
    ):
        rows = session.query(ScoreDimensionORM).filter_by(stage=stage).all()
        dimensions = [
            {
                "name": row.name,
                "prompt_template": row.prompt_template,
                "weight": row.weight,
                "parser": cls.get_parser(row.extra_data or {}),
                "file": row.extra_data.get("file") if row.extra_data else None,
            }
            for row in rows
        ]
        return cls(dimensions, prompt_loader=prompt_loader, agent_config=agent_config)

    @classmethod
    def from_file(cls, filepath: str, prompt_loader, cfg, logger, memory):
        with open(Path(filepath), "r") as f:
            data = yaml.safe_load(f)

        # Default to 'simple' if not provided
        output_format = data.get("output_format", "simple")

        dimensions = [
            {
                "name": d["name"],
                "file": d.get("file"),
                "prompt_template": d.get(
                    "prompt_template", d.get("file")
                ),  # fallback to file
                "weight": d.get("weight", 1.0),
                "parser": cls.get_parser(d.get("extra_data", {})),
            }
            for d in data["dimensions"]
        ]

        # Ensure the output_format is accessible in instance
        cfg = cfg.copy()
        cfg["output_format"] = output_format

        return cls(
            dimensions=dimensions,
            prompt_loader=prompt_loader,
            cfg=cfg,
            logger=logger,
            memory=memory,
        )

    @staticmethod
    def get_parser(extra_data):
        parser_type = extra_data.get("parser", "numeric")
        if parser_type == "numeric":
            return lambda r: ScoreEvaluator.extract_score_from_last_line(r)
        if parser_type == "numeric_cor":
            return lambda r: ScoreEvaluator.parse_numeric_cor(r)

        return lambda r: 0.0

    @staticmethod
    def extract_score_from_last_line(response: str) -> float:
        """
        Looks for a line ending with 'score: <number>' (case-insensitive).
        """
        lines = response.strip().splitlines()
        for line in reversed(lines):
            match = re.search(r"score:\s*(\d+(\.\d+)?)", line.strip(), re.IGNORECASE)
            if match:
                return float(match.group(1))
        return 0.0

    @staticmethod
    def parse_numeric_cor(response: str) -> float:
        """
        Extracts a numeric score from within <answer> tags, supporting [[X]], [X], or bare numbers.
        Example: <answer>[[3]]</answer> â†’ 3.0
        """
        match = re.search(
            r"<answer>\s*\[*\s*(\d+(?:\.\d+)?)\s*\]*\s*</answer>",
            response,
            re.IGNORECASE,
        )
        if not match:
            raise ValueError(
                f"Could not extract numeric score from CoR-style answer: {response}"
            )
        return float(match.group(1))

    def evaluate(self, hypothesis: dict, context: dict = {}, llm_fn=None):
        if self.output_format == "cor":
            return self._evaluate_cor(
                hypothesis=hypothesis, context=context, llm_fn=llm_fn
            )
        else:
            return self._evaluate_simple(
                hypothesis=hypothesis, context=context, llm_fn=llm_fn
            )

    def _evaluate_cor(self, hypothesis: dict, context: dict = {}, llm_fn=None):
        """
        Evaluate using Chain-of-Rubrics (CoR) format with rubric, eval, and <answer>[[score]]</answer>.
        """
        if llm_fn is None:
            raise ValueError(
                "You must pass a call_llm function (e.g., agent.call_llm) to ScoreEvaluator.evaluate"
            )

        results = {}
        for dim in self.dimensions:
            # Load prompt using prompt_loader and dimension-specific CoR template
            if self.prompt_loader and dim.get("file"):
                prompt = self.prompt_loader.from_file(
                    file_name=dim["file"],
                    config=self.cfg,
                    context={"hypothesis": hypothesis, **context},
                )
            elif dim.get("prompt_template"):
                prompt = Template(dim["prompt_template"]).render(
                    hypothesis=hypothesis, **context
                )
            else:
                raise ValueError(f"No prompt found for dimension {dim['name']}")

            response = llm_fn(prompt, context=context)
            try:
                score = dim["parser"](response)
            except Exception as e:
                self.logger.log(
                    "ScoreParseError",
                    {"dimension": dim["name"], "response": response, "error": str(e)},
                )
                score = 0.0

            self.logger.log(
                "CorDimensionEvaluated",
                {"dimension": dim["name"], "score": score, "response": response},
            )

            results[dim["name"]] = {
                "score": score,
                "rationale": response,
                "weight": dim["weight"],
            }

        self.save_score_to_memory(results, hypothesis, context)
        return results

    def _evaluate_simple(self, hypothesis: dict, context: dict = {}, llm_fn=None):
        if llm_fn is None:
            raise ValueError(
                "You must pass a call_llm function (e.g., agent.call_llm) to ScoreEvaluator.evaluate"
            )

        results = {}
        for dim in self.dimensions:
            if self.prompt_loader and dim.get("file"):
                prompt = self.prompt_loader.from_file(
                    file_name=dim["file"],
                    config=self.cfg,
                    context={
                        **context,
                        "goal": context.get("goal"),
                        "hypothesis": hypothesis,
                    },
                )
            else:
                prompt = Template(dim["prompt_template"]).render(
                    hypothesis=hypothesis, **context
                )

            response = llm_fn(prompt, context=context)
            score = dim["parser"](response)
            self.logger.log(
                "DimensionEvaluated",
                {"dimension": dim["name"], "score": score, "response": response},
            )
            results[dim["name"]] = {
                "score": score,
                "rationale": response,
                "weight": dim["weight"],
            }
        self.save_score_to_memory(results, hypothesis, context)
        return results

    def save_score_to_memory(self, results, hypothesis, context):
        """Save all dimension scores and associated ScoreORM entries."""
        goal = context.get("goal")
        pipeline_run_id = context.get("pipeline_run_id")
        hypothesis_id = hypothesis.get("id")

        weighted_score = sum(
            s["score"] * s.get("weight", 1.0) for s in results.values()
        ) / max(sum(s.get("weight", 1.0) for s in results.values()), 1.0)

        scores_json = {
            "stage": self.cfg.get("stage", "review"),
            "dimensions": results,
            "final_score": round(weighted_score, 2),
        }

        # Step 1: Insert EvaluationORM
        eval_orm = EvaluationORM(
            goal_id=goal.get("id"),
            pipeline_run_id=pipeline_run_id,
            hypothesis_id=hypothesis_id,
            agent_name=self.cfg.get("name"),
            model_name=self.cfg.get("model", {}).get("name"),
            evaluator_name=self.cfg.get("evaluator", "ScoreEvaluator"),
            strategy=self.cfg.get("strategy"),
            reasoning_strategy=self.cfg.get("reasoning_strategy"),
            scores=scores_json,
            extra_data={"source": "ScoreEvaluator"},
        )
        self.memory.session.add(eval_orm)
        self.memory.session.flush()  # Get eval_orm.id before committing

        # Step 2: Insert ScoreORM entries
        for dimension_name, result in results.items():
            score = ScoreORM(
                evaluation_id=eval_orm.id,
                dimension=dimension_name,
                score=result["score"],
                weight=result["weight"],
                rationale=result["rationale"],
            )
            self.memory.session.add(score)

        self.memory.session.commit()

        self.logger.log(
            "ScoreSavedToMemory",
            {
                "goal_id": goal.get("id"),
                "hypothesis_id": hypothesis_id,
                "scores": scores_json,
            },
        )

        self.display_results(results, weighted_score)

    def display_results(self, results, weighted_score):
        table_data = [
            [
                dim_name,
                f"{dim_data['score']:.2f}",
                dim_data["weight"],
                dim_data["rationale"][:60],
            ]
            for dim_name, dim_data in results.items()
        ]
        table_data.append(["FINAL", f"{weighted_score:.2f}", "-", "Weighted average"])

        print("\nðŸ“Š Dimension Scores Summary")
        print(tabulate(
            table_data,
            headers=["Dimension", "Score", "Weight", "Rationale (preview)"],
            tablefmt="fancy_grid"
        ))
---END-OF-FILE---


"co_ai\analysis\symbolic_impact_analyzer.py"
---START-OF-FILE---
from co_ai.utils.graph_tools import compare_graphs


class SymbolicImpactAnalyzer:
    """
    Analyzes structural overlap and divergence between two graph representations (e.g., symbolic vs. LATS)
    and attributes score delta to divergent paths.
    """

    def __init__(self, score_lookup_fn):
        self.score_lookup_fn = (
            score_lookup_fn  # Function to get scores for a given node or trace
        )

    def analyze(self, graph1, graph2):
        matches, only_1, only_2 = compare_graphs(graph1, graph2)
        results = []

        for node in matches:
            score_1 = self.score_lookup_fn(node, source="graph1")
            score_2 = self.score_lookup_fn(node, source="graph2")
            results.append({
                "node": node,
                "type": "converged",
                "delta": score_2 - score_1
            })

        for node in only_1 + only_2:
            score = self.score_lookup_fn(node, source="graph1")
            results.append({
                "node": node,
                "type": "diverged",
                "score": score
            })

        return results---END-OF-FILE---


"co_ai\compiler\passes\strategy_mutation_pass.py"
---START-OF-FILE---
# co_ai/compiler/passes/strategy_mutation_pass.py

class StrategyMutationPass:
    def __init__(self, cfg:dict, compiler=None, evaluator=None, logger=None):
        self.cfg = cfg    
        self.compiler = compiler
        self.evaluator = evaluator
        self.logger = logger

    def apply(self, base_prompt: str, metadata: dict) -> list[dict]:
        """Generate and score prompt mutations using the evaluator."""
        mutations = []

        # Example symbolic mutations (could be expanded)
        candidates = [
            base_prompt.replace("Let's think step by step.", "Let's work through this carefully."),
            base_prompt + "\nProvide a rationale before giving your final answer.",
            base_prompt.replace("explain", "analyze"),
        ]

        for variant in candidates:
            try:
                score = self.evaluator.evaluate(variant, metadata=metadata) if self.evaluator else 0
                mutations.append({"prompt": variant, "score": score})
            except Exception as e:
                if self.logger:
                    self.logger.log("StrategyMutationEvalError", {
                        "error": str(e),
                        "prompt_snippet": variant[:100],
                    })

        # Optionally sort by score descending
        return sorted(mutations, key=lambda x: x["score"], reverse=True)
---END-OF-FILE---


"co_ai\compiler\llm_compiler.py"
---START-OF-FILE---
from jinja2 import BaseLoader, Environment

from co_ai.models.prompt_program import PromptProgramORM


class LLMCompiler:
    def __init__(self, llm, evaluator=None, logger=None):
        """
        llm: callable that takes prompt_text and returns a response
        evaluator: optional scoring function (e.g., MR.Q or LLM judge)
        logger: optional logging tool
        """
        self.llm = llm
        self.evaluator = evaluator
        self.logger = logger
        self.jinja_env = Environment(loader=BaseLoader())

    def render_prompt(self, program: PromptProgramORM) -> str:
        try:
            template = self.jinja_env.from_string(program.template)
            rendered = template.render(**program.inputs)
            program.prompt_text = rendered
            return rendered
        except Exception as e:
            if self.logger:
                self.logger.log("PromptRenderError", {"error": str(e), "template": program.template})
            raise

    def execute(self, program: PromptProgramORM, context: dict = {}) -> PromptProgramORM:
        try:
            # Step 1: Render prompt
            prompt = self.render_prompt(program)

            # Step 2: Call LLM
            response = self.llm(prompt)
            program.hypothesis = response
            program.execution_trace = response  # raw output; extend if needed

            # Step 3: Score hypothesis (optional)
            if self.evaluator:
                score_result = self.evaluator.evaluate(program, context=context)
                program.score = score_result.score
                program.rationale = score_result.reason

            if self.logger:
                self.logger.log("PromptProgramExecuted", {
                    "program_id": program.id,
                    "score": program.score,
                    "rationale_snippet": program.rationale[:100] if program.rationale else None,
                })

            return program

        except Exception as e:
            if self.logger:
                self.logger.log("PromptExecutionError", {"error": str(e)})
            raise
---END-OF-FILE---


"co_ai\compiler\prompt_evaluator.py"
---START-OF-FILE---
from abc import ABC, abstractmethod


class EvaluationResult:
    def __init__(self, score: float, reason: str):
        self.score = score
        self.reason = reason

class BasePromptEvaluator(ABC):
    @abstractmethod
    def evaluate(self, program, context: dict = None) -> EvaluationResult:
        pass


class MRQPromptEvaluator(BasePromptEvaluator):
    def __init__(self, llm, prompt_loader, logger=None):
        self.llm = llm
        self.prompt_loader = prompt_loader
        self.logger = logger

    def evaluate(self, program, context: dict = None) -> EvaluationResult:
        context = context or {}
        try:
            evaluation_context = {
                **context,
                "goal": program.goal,
                "prompt": program.prompt_text,
                "hypothesis": program.hypothesis,
            }
            prompt = self.prompt_loader.load_prompt("prompt_evaluation", evaluation_context)
            response = self.llm(prompt)

            # Very basic scoring extraction
            import re
            match = re.search(r"score:(\d+(\.\d+)?)", response)
            score = float(match.group(1)) if match else 0.0

            return EvaluationResult(score=score, reason=response)

        except Exception as e:
            if self.logger:
                self.logger.log("PromptEvaluationFailed", {"error": str(e)})
            return EvaluationResult(score=0.0, reason=str(e))
---END-OF-FILE---


"co_ai\compiler\prompt_evolver.py"
---START-OF-FILE---
import dspy
from dspy import BootstrapFewShot, Example, Predict

from co_ai.compiler.llm_compiler import LLMCompiler
from co_ai.compiler.passes.strategy_mutation_pass import StrategyMutationPass
from co_ai.compiler.prompt_evaluator import EvaluationResult
from co_ai.compiler.prompt_mutator import PromptMutator
from co_ai.compiler.prompt_tuning_signature import PromptTuningSignature
from co_ai.evaluator.evaluator_loader import get_evaluator
from co_ai.models.prompt_program import PromptProgramORM


class PromptEvolver:
    def __init__(self, llm, logger=None, use_strategy_mutation=False, evaluator_cfg=None, memory=None):
        self.llm = llm
        self.logger = logger
        self.use_strategy_mutation = use_strategy_mutation
        dspy.configure(lm=self.llm)

        self.compiler = LLMCompiler(llm=self.llm, logger=self.logger)
        if self.use_strategy_mutation:
            self.strategy_pass = StrategyMutationPass(compiler=self.compiler, logger=self.logger)

        self.evaluator = None
        if evaluator_cfg:
            self.evaluator = get_evaluator(evaluator_cfg, memory=memory, llm=llm, logger=logger)

    def evolve(self, examples: list[dict], context: dict = {}, sample_size: int = 10) -> list[str]:
        """
        Use DSPy to tune prompts based on performance signals.
        Optionally use symbolic strategy mutation.
        Returns a list of refined prompt strings.
        """
        if not examples:
            return []

        training_set = [
            Example(
                goal=ex["goal"],
                input_prompt=ex["prompt_text"],
                hypotheses=ex["hypothesis_text"],
                review=ex.get("review", ""),
                score=ex.get("elo_rating", 1000),
            ).with_inputs("goal", "input_prompt", "hypotheses", "review", "score")
            for ex in examples[:sample_size]
        ]

        def fallback_metric(example, pred, trace=None):
            return 1.0  # fallback metric for training

        tuner = BootstrapFewShot(metric=fallback_metric)
        student = Predict(PromptTuningSignature)
        tuned_program = tuner.compile(student=student, trainset=training_set)

        refined_prompts = []

        # Use DSPy tuned program
        for ex in examples[sample_size:]:
            try:
                result = tuned_program(
                    goal=ex["goal"],
                    input_prompt=ex["prompt_text"],
                    hypotheses=ex["hypothesis_text"],
                    review=ex.get("review", ""),
                    score=ex.get("elo_rating", 1000),
                )
                refined = result.refined_prompt.strip()
                refined_prompts.append(refined)
            except Exception as e:
                if self.logger:
                    self.logger.log("DSPyPromptEvolutionFailed", {"error": str(e)})

        # Optionally add symbolic strategy mutations
        if self.use_strategy_mutation:
            for ex in examples:
                base_prompt = ex["prompt_text"]
                metadata = {
                    "goal": ex["goal"],
                    "hypotheses": ex.get("hypothesis_text", ""),
                    "review": ex.get("review", ""),
                    "score": ex.get("elo_rating", 1000),
                }
                try:
                    mutations = self.strategy_pass.apply(base_prompt, metadata)
                    for mut in mutations:
                        prompt_text = mut["prompt"]
                        score = self.score_prompt(prompt_text, reference_output=metadata["hypotheses"], context=context)
                        if score >= 0:  # optionally apply a score threshold
                            refined_prompts.append(prompt_text)
                except Exception as e:
                    if self.logger:
                        self.logger.log("StrategyMutationFailed", {"error": str(e)})

        return refined_prompts

    def score_prompt(self, prompt: str, reference_output: str = "", context:dict={}) -> float:
        return self.evaluator.score_single(prompt, reference_output, context)
---END-OF-FILE---


"co_ai\compiler\prompt_mutator.py"
---START-OF-FILE---
# co_ai/compiler/prompt_mutator.py

class PromptMutator:
    """
    A utility class to generate mutated versions of a prompt using symbolic or structural transformations.
    """

    def __init__(self, strategies: list[str] = None):
        self.strategies = strategies or [
            "Think step by step.",
            "Take a skeptical perspective.",
            "Consider multiple points of view.",
            "Use a detailed explanation.",
        ]

    def mutate_with_strategies(self, base_prompt: str) -> list[str]:
        """
        Prepend various reasoning strategies to the base prompt.
        """
        return [f"{strategy} {base_prompt}" for strategy in self.strategies]

    def mutate(self, base_prompt: str, metadata: dict = None) -> list[str]:
        """
        Apply one or more symbolic mutations to a prompt.
        """
        return self.mutate_with_strategies(base_prompt)

    def mutate_with_templates(self, base_prompt: str, template_list: list[str]) -> list[str]:
        """
        Apply a custom list of templates where `{prompt}` is replaced with the base prompt.
        """
        return [template.format(prompt=base_prompt) for template in template_list]
---END-OF-FILE---


"co_ai\compiler\prompt_tuning_signature.py"
---START-OF-FILE---
# co_ai/compiler/prompt_tuning_signature.py

import dspy
from dspy import InputField, OutputField, Signature


class PromptTuningSignature(Signature):
    goal = InputField(desc="Scientific research goal or question")
    input_prompt = InputField(desc="Original prompt used to generate hypotheses")
    hypotheses = InputField(desc="Best hypothesis generated")
    review = InputField(desc="Expert review of the hypothesis")
    score = InputField(desc="Numeric score evaluating the hypothesis quality")
    refined_prompt = OutputField(desc="Improved version of the original prompt")
---END-OF-FILE---


"co_ai\components\__init__.py"
---START-OF-FILE---
from co_ai.components.lats_component import LATSComponent
---END-OF-FILE---


"co_ai\components\agent_tree_search.py"
---START-OF-FILE---
import time
from co_ai.agents.ats.solution_node import SolutionNode


class AgenticTreeSearch:
    def __init__(self, agent, max_iterations=500, time_limit=86400):
        self.agent = agent
        self.tree = []
        self.max_iterations = max_iterations
        self.time_limit = time_limit
        self.iteration = 0

    async def run(self, context):
        start_time = time.time()
        while self.iteration < self.max_iterations and (time.time() - start_time) < self.time_limit:
            action, parent_node = self.select_action()
            new_plan = await self.generate_plan(parent_node, action)
            new_code = self.generate_code(new_plan)
            result = self.execute_code(new_code)

            # Verify output
            verification = self.verify_output(result)
            new_node = SolutionNode(
                plan=new_plan,
                code=new_code,
                metric=verification["metric"],
                output=result,
                summary=verification["summary"]
            )
            self.tree.append(new_node)
            self.iteration += 1

        # Return best solution
        return self.get_best_solution()

    def select_action(self):
        # Implement Algorithm 1 logic here
        pass

    async def generate_plan(self, parent_node, action):
        # Use prompts to generate plan
        pass

    def generate_code(self, plan):
        # Decide one-pass vs stepwise
        pass

    def execute_code(self, code):
        # Run in sandbox
        pass

    def verify_output(self, output):
        # Check for bugs, metrics, submission file
        pass

    def get_best_solution(self):
        # Return node with highest Î·
        pass---END-OF-FILE---


"co_ai\components\coding_strategy.py"
---START-OF-FILE---
# --- components/coding_strategy.py ---

class SelfAdaptiveCoder:
    def __init__(self, config):
        self.threshold = config.get("complexity_threshold", 3)

    def score_complexity(self, plan):
        # Placeholder: use LLM or rubric to rate complexity (1â€“5)
        return 4 if "multi-stage" in plan.lower() else 2

    def generate_code(self, plan):
        complexity = self.score_complexity(plan)
        if complexity <= self.threshold:
            return self._generate_one_pass(plan)
        else:
            return self._generate_stepwise(plan)

    def _generate_one_pass(self, plan):
        return f"# One-pass code for plan\n# {plan}"

    def _generate_stepwise(self, plan):
        steps = [f"Step {i+1}: logic" for i in range(3)]  # Placeholder decomposition
        integrated_code = "\n".join([f"# {s}" for s in steps])
        return f"# Stepwise code for complex plan\n{integrated_code}"
---END-OF-FILE---


"co_ai\components\lats_component.py"
---START-OF-FILE---
import math
import random
import uuid
from typing import Any, Callable, Dict, List, Optional


class LATSNode:
    def __init__(self, state, trace, parent=None):
        self.id = str(uuid.uuid4())
        self.state = state
        self.trace = trace
        self.parent = parent
        self.children = []
        self.visits = 0
        self.reward = 0.0
        self.score = None
        self.dimension_scores = {}

    def is_leaf(self):
        return len(self.children) == 0


class LATSComponent:
    def __init__(
        self,
        expand_fn: Callable[[LATSNode, Dict[str, Any]], List[Dict]],
        score_fn: Callable[[LATSNode, Dict[str, Any]], Dict[str, Any]],
        is_terminal_fn: Optional[Callable[[LATSNode], bool]] = None,
        memory = None,
        logger: Optional[Any] = None,
        max_steps: int = 10,
        exploration_weight: float = 1.4,
    ):
        self.expand_fn = expand_fn
        self.score_fn = score_fn
        self.is_terminal_fn = is_terminal_fn or (lambda node: False)
        self.memory = memory
        self.logger = logger or (lambda *args, **kwargs: None)
        self.max_steps = max_steps
        self.exploration_weight = exploration_weight
        self.root = None

    def create_node(self, state, trace, parent=None):
        return LATSNode(state, trace, parent)

    def uct_score(self, parent_visits, child):
        if child.visits == 0:
            return float('inf')
        return child.reward / child.visits + self.exploration_weight * math.sqrt(math.log(parent_visits) / child.visits)

    def select_best_child(self, node):
        return max(node.children, key=lambda c: self.uct_score(node.visits, c))

    def backpropagate(self, node, reward):
        while node:
            node.visits += 1
            node.reward += reward
            node = node.parent

    def simulate(self, node, context):
        if self.is_terminal_fn(node):
            return node.reward

        children = self.expand_fn(node, context)
        for child_info in children:
            child_node = self.create_node(child_info['state'], child_info['trace'], parent=node)
            node.children.append(child_node)

            score_result = self.score_fn(child_node, context)
            child_node.score = score_result.get('score', 0.0)
            child_node.dimension_scores = score_result.get('dimension_scores', {})
            reward = child_node.score or 0.0

            self.backpropagate(child_node, reward)

    def run(self, root_state, context):
        self.root = self.create_node(root_state, trace=[])

        for step in range(self.max_steps):
            node = self.root
            while not node.is_leaf():
                node = self.select_best_child(node)
            self.simulate(node, context)

        best = max(self.root.children, key=lambda c: c.reward / c.visits if c.visits > 0 else -1)
        return best.trace, best.score, best.dimension_scores
---END-OF-FILE---


"co_ai\components\search_policy.py"
---START-OF-FILE---
# --- components/search_policy.py ---

import random

class TreeSearchPolicy:
    def __init__(self, config):
        self.n_init = config.get("n_init", 3)
        self.p_debug = config.get("p_debug", 0.2)
        self.p_greedy = config.get("p_greedy", 0.6)
        self.use_prediction = config.get("use_prediction", True)

    def select(self, tree, predictor=None):
        draft_count = len(tree.nodes)

        if draft_count < self.n_init:
            return None, "draft"

        if random.random() < self.p_debug:
            buggy = tree.get_buggy()
            if buggy:
                return random.choice(buggy), "debug"

        valid = tree.get_valid()
        if not valid:
            return None, "draft"

        if self.use_prediction and predictor:
            # Rank by predicted future value
            ranked = sorted(valid, key=lambda n: predictor.predict(tree.goal, n.plan), reverse=True)
            return ranked[0], "improve"

        if random.random() < self.p_greedy:
            return max(valid, key=lambda n: n.metric), "improve"
        else:
            return random.choice(valid), "improve"
---END-OF-FILE---


"co_ai\components\solution_tree.py"
---START-OF-FILE---
# --- components/solution_tree.py ---

class SolutionNode:
    def __init__(self, plan, code, metric, output, valid):
        self.plan = plan
        self.code = code
        self.metric = metric
        self.output = output
        self.valid = valid

class SolutionTree:
    def __init__(self):
        self.nodes = []

    def initialize(self, goal):
        self.goal = goal
        self.nodes.clear()

    def add_node(self, node):
        self.nodes.append(node)

    def get_best(self):
        valid_nodes = [n for n in self.nodes if n.valid]
        return max(valid_nodes, key=lambda n: n.metric, default=None)

    def get_buggy(self):
        return [n for n in self.nodes if not n.valid]

    def get_valid(self):
        return [n for n in self.nodes if n.valid]


---END-OF-FILE---


"co_ai\data\interfaces.py"
---START-OF-FILE---
from abc import ABC, abstractmethod


class DataSource(ABC):
    @abstractmethod
    def get_training_pairs(self, goal: str, limit: int) -> list[dict]:
        pass

    @abstractmethod
    def get_prompt_examples(self, goal: str, limit: int) -> list[dict]:
        pass
---END-OF-FILE---


"co_ai\dataloaders\__init__.py"
---START-OF-FILE---
from .arm_to_mrq_dpo import ARMDataLoader---END-OF-FILE---


"co_ai\dataloaders\arm_to_mrq_dpo.py"
---START-OF-FILE---
import json
import random
from collections import Counter
from typing import Dict, List, Optional

from datasets import load_dataset

REASONING_FORMATS = {
    "direct": "<Direct>",
    "short_cot": "<Short_CoT>",
    "code": "<Code>",
    "long_cot": "<Long_CoT>"
}

FORMAT_END_TAGS = {
    "direct": "</Direct>",
    "short_cot": "</Short_CoT>",
    "code": "</Code>",
    "long_cot": "</Long_CoT>"
}


class ARMDataLoader:
    def __init__(
        self,
        dataset_name: str = "aqua_rat",
        subset: Optional[str] = None,
        split: str = "train",
        max_samples: int = 500,
        memory=None,
        logger=None,
    ):
        self.dataset_name = dataset_name
        self.subset = subset
        self.split = split
        self.max_samples = max_samples
        self.memory = memory
        self.logger = logger

        # Format tokens
        self.format_tokens = {
            "direct": "<Direct>",
            "short_cot": "<Short_CoT>",
            "code": "<Code>",
            "long_cot": "<Long_CoT>",
        }
        self.format_end_tokens = {
            "direct": "</Direct>",
            "short_cot": "</Short_CoT>",
            "code": "</Code>",
            "long_cot": "</Long_CoT>",
        }

        self._debug_count = 0
        self.dataset = None

    def log(self, event_name: str, payload: dict):
        if self.logger:
            self.logger.log(event_name, payload)
        else:
            print(f"[{event_name}] {json.dumps(payload)}")

    def adapt(self, context: dict):
        """Main method: Load â†’ Convert â†’ Save to Memory"""
        self.log("DatasetLoading", {"name": self.dataset_name, "split": self.split})
        self.load_dataset()
        self.summarize_difficulties()
        self.print_samples_by_difficulty()

        total_samples = len(self.dataset)
        indices = random.sample(
            range(total_samples), min(self.max_samples, total_samples)
        )

        count = 0
        goal_text = context.get("goal").get("goal_text")
        run_id = context.get("run_id")
        for idx in indices:
            sample = self.dataset[idx]
            pairs = self.build_preference_pairs(sample)
            for pair in pairs:
                prompt = pair["prompt"]
                chosen = pair["chosen"]
                rejected = pair["rejected"]
                preferred = pair["preferred_format"]
                fmt_a = self.detect_format(chosen)
                fmt_b = self.detect_format(rejected)
                difficulty = self.detect_difficulty(prompt)
                # Embed everything once
                self._get_or_cache_embedding(prompt)
                self._get_or_cache_embedding(chosen)
                self._get_or_cache_embedding(rejected)

                # Save to database
                try:
                    self.memory.mrq.add_preference_pair(
                        goal=goal_text,
                        prompt=prompt,
                        output_a=chosen,
                        output_b=rejected,
                        preferred=preferred,
                        fmt_a=fmt_a,
                        fmt_b=fmt_b,
                        difficulty=difficulty,
                        run_id=run_id,
                    )
                    count += 1
                except Exception as e:
                    self.log(
                        "PreferencePairSaveError",
                        {
                            "error": str(e),
                            "prompt": prompt[:80],
                            "chosen": chosen[:80],
                            "rejected": rejected[:80],
                        },
                    )

        self.log("PreferencePairsSaved", {"count": count, "goal": "arm_dpo"})
        context["dpo_samples"] = count
        return context

    def _get_or_cache_embedding(self, text: str) -> List[float]:
        """
        Get embedding from cache or compute and store.
        Uses your existing memory.embedding.get_or_create() method.
        """
        emb = self.memory.embedding.get_or_create(text)
        return emb

    def load_dataset(self):
        """Load dataset from Hugging Face."""
        try:
            self.dataset = load_dataset(
                self.dataset_name, self.subset, split=self.split
            )
            self.log("DatasetLoaded", {"count": len(self.dataset)})
        except Exception as e:
            raise RuntimeError(
                f"Failed to load dataset '{self.dataset_name}': {str(e)}"
            )

    def _detect_difficulty(self, question: str) -> str:
        words = question.split()
        if len(words) < 20:
            return "easy"
        elif len(words) < 50:
            return "medium"
        else:
            return "hard"

    def build_preference_pairs(self, sample: Dict) -> List[Dict]:
        """
        Build DPO-style preference pairs by comparing formats.
        Returns list of dicts like:
        {
          'prompt': ...,
          'chosen': ...,
          'rejected': ...,
          'preferred_format': ...,
          'difficulty': ...
        }
        """
        question = sample.get("question", "").strip()
        ground_truth = sample.get("correct", "").strip()
        difficulty = self._detect_difficulty(question)

        # Generate all four reasoning formats
        direct = self.generate_direct(ground_truth)
        short_cot = self.generate_short_cot(question, ground_truth)
        code = self.generate_code(question, ground_truth)
        long_cot = self.generate_long_cot(question, ground_truth)

        format_to_response = {
            "direct": direct,
            "short_cot": short_cot,
            "code": code,
            "long_cot": long_cot,
        }

        # Filter out empty responses
        valid_formats = [
            fmt for fmt, resp in format_to_response.items() if resp.strip()
        ]
        format_to_response = {
            k: v for k, v in format_to_response.items() if k in valid_formats
        }

        # Define which formats are preferred based on difficulty
        if difficulty == "easy":
            preferred_formats = ["direct", "short_cot", "code"]
            non_preferred_formats = ["long_cot"]
        elif difficulty == "hard":
            preferred_formats = ["long_cot", "code"]
            non_preferred_formats = ["direct", "short_cot"]
        else:  # medium or default case
            preferred_formats = ["short_cot", "code"]
            non_preferred_formats = ["direct", "long_cot"]

        # Build all possible pairs
        pairs = []
        for pref in preferred_formats:
            p_resp = format_to_response.get(pref)
            if not p_resp:
                continue
            for non_pref in non_preferred_formats:
                np_resp = format_to_response.get(non_pref)
                if not np_resp:
                    continue
                pairs.append(
                    {
                        "prompt": question,
                        "chosen": p_resp,
                        "rejected": np_resp,
                        "preferred_format": pref,
                        "rejected_format": non_pref,
                        "difficulty": difficulty,
                    }
                )

        return pairs

    def summarize_difficulties(self):
        counts = Counter()
        for sample in self.dataset:
            question = sample.get("question", "")
            detected = self._detect_difficulty(question)
            counts[detected] += 1
        self.log("DifficultySummary", dict(counts))
        return counts

    def print_samples_by_difficulty(self, count_per_level=3):
        buckets = {"easy": [], "medium": [], "hard": []}
        for sample in self.dataset:
            question = sample.get("question", "")
            difficulty = self._detect_difficulty(question)
            if len(buckets[difficulty]) < count_per_level:
                buckets[difficulty].append(question)

        for diff, questions in buckets.items():
            self.log("SampleByDifficulty", {"difficulty": diff, "examples": questions})

    def _detect_difficulty(self, question: str) -> str:
        """Basic heuristic to infer difficulty based on question length."""
        words = question.split()
        if len(words) < 20:
            return "easy"
        elif len(words) < 50:
            return "medium"
        else:
            return "hard"

    def generate_direct(self, answer: str) -> str:
        return f"{self.format_tokens['direct']}The answer is {answer}.{self.format_end_tokens['direct']}"

    def generate_short_cot(self, question: str, answer: str) -> str:
        return (
            f"{self.format_tokens['short_cot']}"
            "Let me think briefly:\n"
            "Step 1: Understand the question.\n"
            "Step 2: Apply basic logic.\n"
            f"Final Answer: {answer}"
            f"{self.format_end_tokens['short_cot']}"
        )

    def generate_code(self, question: str, answer: str) -> str:
        return (
            f"{self.format_tokens['code']}"
            "def solve():\n"
            "    # Placeholder code generated by GPT-4o\n"
            f"    return '{answer}'\n"
            "solve()\n"
            f"# Output: {answer}"
            f"{self.format_end_tokens['code']}"
        )

    def generate_long_cot(self, question: str, answer: str) -> str:
        return (
            f"{self.format_tokens['long_cot']}"
            "Let's analyze this step-by-step:\n\n"
            "1. Read the question carefully.\n"
            "2. Identify key information.\n"
            "3. Consider multiple approaches.\n"
            "4. Evaluate thoroughly.\n"
            "...\n"
            "Reflection: This approach ensures correctness by exploring multiple paths.\n"
            f"Final Answer: {answer}"
            f"{self.format_end_tokens['long_cot']}"
        )

    def detect_difficulty(self, text: str) -> str:
        words = text.split()
        if len(words) < 20:
            return "easy"
        elif len(words) < 50:
            return "medium"
        else:
            return "hard"

    @staticmethod
    def detect_format(text: str) -> str:
        text = text.strip().lower()
        if not text:
            return "unknown"
        if "<direct>" in text:
            return "direct"
        elif "<short_cot>" in text:
            return "short_cot"
        elif "<code>" in text:
            return "code"
        elif "<long_cot>" in text:
            return "long_cot"
        
        # Direct Answer
        if text.startswith("the answer is") or text.startswith("answer:"):
            return "direct"

        # Short CoT
        elif text.startswith("let me think briefly"):
            return "short_cot"

        # Long CoT
        elif text.startswith("let's analyze this step-by-step"):
            return "long_cot"

        # Code
        elif any(kw in text for kw in ["def", "return", "solve()", "print(", "for ", "if "]):
            return "code"

        else:
            print(f"[WARNING] Unknown format:\n{text[:100]}...")
            return "unknown"---END-OF-FILE---


"co_ai\envs\__init__.py"
---START-OF-FILE---
from .research_env import ResearchEnv---END-OF-FILE---


"co_ai\envs\research_env.py"
---START-OF-FILE---
from typing import Any, Dict


class ResearchEnv:
    """
    A simulated environment for testing hypotheses in a research context.
    Returns feedback based on internal knowledge and hypothesis scoring.
    """

    def __init__(self, cfg=None):
        self.cfg = cfg or {}
        self.knowledge_base = self._load_knowledge_base()

    def _load_knowledge_base(self):
        """
        Load static knowledge used for validating hypotheses.
        Can be extended to use vector DB or embedding search.
        """
        return {
            "Arthurâ€™s Magazine": {
                "started": 1846,
                "merged_into": "Godeyâ€™s Ladyâ€™s Book"
            },
            "First for Women": {
                "started": 1989
            }
        }

    def reset(self, goal: str) -> str:
        """
        Start a new research task.
        """
        self.current_goal = goal
        return f"Goal: {goal}"

    def step(self, hypothesis: str) -> Dict[str, Any]:
        """
        Take a hypothesis as action, simulate feedback using scoring.
        Returns:
            dict: {
                "text": <natural language feedback>,
                "reward": <numerical score>,
                "success": <bool>
            }
        """
        # Simulate a hypothesis ORM object
        fake_hyp = {
            "text": hypothesis,
            "id": "hyp_simulated",
            "goal_id": "goal_simulated"
        }

        # Score the hypothesis using your scoring system
        from co_ai.agents.mixins.scoring_mixin import ScoringMixin
        class DummyAgent(ScoringMixin): pass

        dummy_agent = DummyAgent({})
        score_result = dummy_agent.score_hypothesis(
            fake_hyp, {"goal": {"goal_text": self.current_goal}}, metrics="reason"
        )

        # Build simulated feedback
        success = score_result["score"] > 70
        feedback = {
            "text": f"Hypothesis scored {score_result['score']} across dimensions.",
            "score": score_result,
            "dimensions": score_result["scores"],
            "reward": score_result["score"] / 100,
            "success": success
        }

        return feedback---END-OF-FILE---


"co_ai\evaluator\__init__.py"
---START-OF-FILE---
from .arm_reassoning_self_evaluator import ARMReasoningSelfEvaluator
from .evaluator_loader import get_evaluator
from .hypothesis_value_predictor import HypothesisValuePredictor
from .llm_judge_evaluator import LLMJudgeEvaluator
from .mrq_self_evaluator import MRQSelfEvaluator
from .text_encoder import TextEncoder
---END-OF-FILE---


"co_ai\evaluator\agreement_checker.py"
---START-OF-FILE---
# evaluators/agreement_checker.py

class EvaluatorAgreementChecker:
    def __init__(self, logger):
        self.logger = logger

    def check(self, mrq_scores: dict, llm_judgement: str, context: dict = None):
        """
        Compares MR.Q and LLM preferred outputs and logs agreement.
        `llm_judgement` should be 'a' or 'b' (or full judgement string if parsed).
        """
        mrq_preference = "a" if mrq_scores["value_a"] >= mrq_scores["value_b"] else "b"

        agreement = mrq_preference == llm_judgement

        self.logger.log("JudgementAgreement", {
            "mrq_preference": mrq_preference,
            "llm_preference": llm_judgement,
            "agreement": agreement,
            "value_a": round(mrq_scores["value_a"], 4),
            "value_b": round(mrq_scores["value_b"], 4),
            "context": context or {}
        })

        return agreement
---END-OF-FILE---


"co_ai\evaluator\arm_reassoning_self_evaluator.py"
---START-OF-FILE---
import json
from copy import deepcopy

import torch
import torch.nn.functional as F

from co_ai.dataloaders import ARMDataLoader
from co_ai.evaluator.base import BaseEvaluator
from co_ai.evaluator.hypothesis_value_predictor import HypothesisValuePredictor
from co_ai.evaluator.text_encoder import TextEncoder


class ARMReasoningSelfEvaluator(BaseEvaluator):
    def __init__(self, cfg, memory, logger):
        self.cfg = cfg
        self.memory = memory
        self.logger = logger
        self.device = cfg.get("device", "cpu")

        self.format_freq = cfg.get(
            "format_freq", {"direct": 1, "short_cot": 1, "code": 1, "long_cot": 1}
        )
        self.format_rewards = cfg.get(
            "format_rewards", {k: [0.5] for k in self.format_freq}
        )

        self.apply_penalty_bonus = cfg.get("apply_penalty_bonus", True)
        self.epsilon = cfg.get("epsilon", 0.1)
        self.kl_penalty_coeff = cfg.get("kl_penalty_coeff", 0.1)

        self.encoder = TextEncoder().to(self.device)
        self.value_predictor = HypothesisValuePredictor(512, 1024).to(self.device)
        self.ref_value_predictor = deepcopy(self.value_predictor)
        self.ref_value_predictor.eval()

    def judge(self, prompt, output_a, output_b, context: dict):
        prompt_emb = torch.tensor(
            self.memory.embedding.get_or_create(prompt), device=self.device
        ).unsqueeze(0)
        output_a_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_a), device=self.device
        ).unsqueeze(0)
        output_b_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_b), device=self.device
        ).unsqueeze(0)

        zsa_a = self.encoder(prompt_emb, output_a_emb)
        zsa_b = self.encoder(prompt_emb, output_b_emb)

        value_a = self.value_predictor(zsa_a).item()
        value_b = self.value_predictor(zsa_b).item()

        preferred_output = output_a if value_a >= value_b else output_b
        scores = {
            "value_a": value_a,
            "value_b": value_b,
            "fmt_a": ARMDataLoader.detect_format(output_a),
            "fmt_b": ARMDataLoader.detect_format(output_b),
        }

        return preferred_output, scores

    def score_single(self, prompt: str, output: str, context) -> float:
        """Minimal ABC-compliant scoring method."""
        prompt_emb = torch.tensor(
            self.memory.embedding.get_or_create(prompt), device=self.device
        ).unsqueeze(0)
        output_emb = torch.tensor(
            self.memory.embedding.get_or_create(output), device=self.device
        ).unsqueeze(0)
        zsa = self.encoder(prompt_emb, output_emb)
        return self.value_predictor(zsa).item()

    def _update_format_stats(self, fmt: str, reward: float):
        """
        Track format usage and average reward per format.

        This enables format-aware reward shaping and prevents format collapse.
        """
        if fmt not in self.format_freq:
            self.format_freq[fmt] = 0
            self.format_rewards[fmt] = []

        self.format_freq[fmt] += 1
        self.format_rewards[fmt].append(reward)

    def train_from_database(self, goal_text: str, cfg: dict):
        limit = cfg.get("limit", 1000)
        epochs = cfg.get("epochs", 20)
        lr = cfg.get("lr", 1e-4)
        batch_size = cfg.get("batch_size", 16)

        samples = self.memory.mrq.get_training_pairs(goal=goal_text, limit=limit)
        if not samples:
            self.logger.log(
                "TrainingError", {"message": "No samples found", "goal": goal_text}
            )
            return

        inputs, labels = [], []
        for item in samples:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["output_a"])
            output_b_emb = self.memory.embedding.get_or_create(item["output_b"])
            preferred = item["preferred"]

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if preferred == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0], device=self.device))

        dataset = torch.utils.data.TensorDataset(
            torch.stack(inputs), torch.stack(labels)
        )
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=True
        )

        opt = torch.optim.Adam(self.value_predictor.parameters(), lr=lr)
        self.value_predictor.train()

        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                loss = -torch.log(torch.sigmoid(preds)).mean()
                opt.zero_grad()
                loss.backward()
                opt.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log(
                "TrainingEpoch",
                {"epoch": epoch + 1, "avg_loss": avg_loss, "goal": goal_text},
            )

        self.logger.log("TrainingComplete", {"goal": goal_text})

    def score(self, prompt: str, response: str) -> float:
        """Framework-level scoring method with reward shaping."""
        base_score = self.score_single(prompt, response, context={})
        if not self.apply_penalty_bonus:
            return base_score

        token_len = len(response.split())
        fmt = ARMDataLoader.detect_format(response)
        rarity_bonus = 1.0 / (1 + self.format_freq.get(fmt, 1))
        shaped_score = base_score - 0.01 * token_len + rarity_bonus
        self._update_format_stats(fmt, shaped_score)
        return shaped_score

    def _score_response(self, prompt_emb, response_emb):
        """Score a single response using prompt-response encoder + value predictor"""
        zsa = self.encoder(prompt_emb, response_emb)
        return self.value_predictor(zsa), zsa

    def train_from_context(self, context: dict, cfg: dict):
        """
        Trains the value predictor using DPO samples stored in the context.
        Applies format-aware reward shaping and KL penalty.
        """
        dpo_samples = context.get("dpo_samples", [])
        if not dpo_samples:
            self.logger.log(
                "TrainingError", {"message": "No DPO samples found in context."}
            )
            return

        self.logger.log(
            "TrainingStarted", {"sample_count": len(dpo_samples), "config": cfg}
        )

        inputs, labels = [], []

        # Extract preference data
        for item in dpo_samples:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["chosen"])
            output_b_emb = self.memory.embedding.get_or_create(item["rejected"])

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if item["preferred_format"] == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0]))

        dataset = torch.utils.data.TensorDataset(
            torch.stack(inputs), torch.stack(labels)
        )
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=cfg.get("batch_size", 16), shuffle=True
        )

        opt = optim.Adam(self.value_predictor.parameters(), lr=cfg.get("lr", 1e-4))
        self.value_predictor.train()

        epochs = cfg.get("epochs", 20)
        best_loss = float("inf")
        patience_counter = 0
        patience = cfg.get("patience", 3)

        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                policy_log_probs = torch.log_softmax(preds, dim=-1)

                with torch.no_grad():
                    ref_preds = self.ref_value_predictor(x_batch)
                    ref_log_probs = torch.log_softmax(ref_preds, dim=-1)

                advantages = policy_log_probs - ref_log_probs
                advantages = (advantages - advantages.mean()) / (
                    advantages.std() + 1e-6
                )

                ratios = torch.exp(policy_log_probs - ref_log_probs)
                clipped_ratios = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon)
                unclipped_loss = ratios * advantages
                clipped_loss = clipped_ratios * advantages

                policy_loss = -torch.min(unclipped_loss, clipped_loss).mean()
                kl = F.kl_div(ref_log_probs, policy_log_probs, reduction="batchmean")
                loss = policy_loss + self.kl_penalty_coeff * kl

                loss.backward()
                opt.step()
                opt.zero_grad()

                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log(
                "TrainingEpoch",
                {
                    "epoch": epoch + 1,
                    "avg_loss": round(avg_loss, 5),
                    "goal": "arm_dpo",
                    "format_usage": self.format_freq.copy(),
                    "format_rewards": {
                        k: round(sum(v) / len(v), 5) if v else 0
                        for k, v in self.format_rewards.items()
                    },
                },
            )

            if avg_loss < best_loss - 0.0001:
                best_loss = avg_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience:
                self.logger.log(
                    "EarlyStopping",
                    {"stopped_epoch": epoch + 1, "best_loss": round(best_loss, 5)},
                )
                break

        self.logger.log(
            "TrainingComplete",
            {"total_epochs": epoch + 1, "final_loss": round(avg_loss, 5)},
        )

    def export_samples_to_json(self, samples: list, output_path: str):
        """
        Exports raw preference pairs to a structured JSON file.

        Each entry includes:
            - Prompt
            - Output A / B
            - Format A / B
            - Preferred side
            - Token lengths
            - Rarity bonuses
            - Difficulty level
        """
        processed = []

        for item in samples:
            prompt = item.get("prompt", "")
            output_a = item.get("output_a", "")
            output_b = item.get("output_b", "")
            preferred = item.get("preferred", "a")

            # Detect format types
            fmt_a = ARMDataLoader.detect_format(output_a)
            fmt_b = ARMDataLoader.detect_format(output_b)

            # Count tokens
            token_len_a = len(output_a.split())
            token_len_b = len(output_b.split())

            # Add rarity bonus
            G = len(samples)
            F_a = (
                sum(
                    1
                    for s in samples
                    if ARMDataLoader.detect_format(s.get("output_a", "")) == fmt_a
                )
                + 1
            )
            F_b = (
                sum(
                    1
                    for s in samples
                    if ARMDataLoader.detect_format(s.get("output_b", "")) == fmt_b
                )
                + 1
            )

            rarity_bonus_a = G / F_a
            rarity_bonus_b = G / F_b

            # Infer difficulty from question length
            words = prompt.split()
            if len(words) < 20:
                difficulty = "easy"
            elif len(words) < 50:
                difficulty = "medium"
            else:
                difficulty = "hard"

            processed.append(
                {
                    "prompt": prompt,
                    "output_a": output_a,
                    "output_b": output_b,
                    "preferred": preferred,
                    "fmt_a": fmt_a,
                    "fmt_b": fmt_b,
                    "token_len_a": token_len_a,
                    "token_len_b": token_len_b,
                    "rarity_bonus_a": round(rarity_bonus_a, 3),
                    "rarity_bonus_b": round(rarity_bonus_b, 3),
                    "difficulty": difficulty,
                }
            )

        with open(output_path, "w") as fp:
            json.dump(processed, fp, indent=2)

        print(f"[INFO] Exported {len(processed)} samples to {output_path}")
---END-OF-FILE---


"co_ai\evaluator\base.py"
---START-OF-FILE---
# co_ai/evaluator/base.py
from abc import ABC, abstractmethod


class BaseEvaluator(ABC):
    @abstractmethod
    def judge(self, prompt, output_a, output_b, context:dict) -> dict:
        pass

    @abstractmethod
    def score_single(self, prompt, output, context:dict) -> float:
        pass

---END-OF-FILE---


"co_ai\evaluator\callibration.py"
---START-OF-FILE---
# evaluators/calibration.py

import torch


class MRQCalibrator:
    @staticmethod
    def calibrated_preference(value_a: float, value_b: float) -> float:
        """
        Returns a probability (0 to 1) that A is preferred over B.
        """
        return torch.sigmoid(torch.tensor(value_a - value_b)).item()

    @staticmethod
    def predicted_preference(value_a: float, value_b: float) -> str:
        """
        Returns 'a' or 'b' based on which value is higher.
        """
        return "a" if value_a >= value_b else "b"
---END-OF-FILE---


"co_ai\evaluator\evaluator_loader.py"
---START-OF-FILE---
# co_ai/evaluator/evaluator_loader.py

def get_evaluator(cfg, memory=None, call_llm=None, logger=None):
    if cfg["judge"] == "llm":
        from co_ai.evaluator.llm_judge_evaluator import LLMJudgeEvaluator

        llm = cfg.get("judge_model", cfg.get("model"))
        prompt_file = cfg.get("judge_prompt_file", "evaluator.txt")
        logger.log(
            "EvaluatorInit", {"strategy": "LLM", "prompt_file": prompt_file}
        )
        return LLMJudgeEvaluator(cfg, llm, prompt_file, call_llm, logger)
    elif cfg["judge"] == "mrq":
        from co_ai.evaluator.mrq_self_evaluator import MRQSelfEvaluator
        return MRQSelfEvaluator(memory=memory, logger=logger)
    else:
        raise ValueError(f"Unknown evaluator type: {cfg['type']}")
---END-OF-FILE---


"co_ai\evaluator\hypothesis_value_predictor.py"
---START-OF-FILE---
from torch import nn


class HypothesisValuePredictor(nn.Module):
    """Predicts a quality score for a hypothesis given its embedding."""
    def __init__(self, zsa_dim=512, hdim=1024):
        super().__init__()
        self.value_net = nn.Sequential(
            nn.Linear(zsa_dim, hdim),
            nn.ReLU(),
            nn.Linear(hdim, 1)
        )

    def forward(self, zsa_embedding):
        return self.value_net(zsa_embedding)
---END-OF-FILE---


"co_ai\evaluator\llm_judge_evaluator.py"
---START-OF-FILE---
# co_ai/evaluator/llm_judge_evaluator.py

import re

from co_ai.evaluator.base import BaseEvaluator
from co_ai.prompts import PromptLoader


class LLMJudgeEvaluator(BaseEvaluator):
    def __init__(self, cfg, llm_cfg, prompt_file, llm, logger):
        self.cfg = cfg
        self.llm_cfg = llm_cfg
        self.prompt_file = prompt_file
        self.llm = llm  # callable: prompt, context, llm_cfg -> response
        self.logger = logger

    def judge(self, prompt, output_a, output_b, context: dict):
        """
        Compare two outputs using an LLM-based judge.

        Args:
            prompt (str): The name or identifier for the evaluation prompt.
            output_a (str): First hypothesis/output to compare.
            output_b (str): Second hypothesis/output to compare.
            context (dict): Execution context with additional variables.

        Returns:
            tuple: (preferred_output, score_details)
        """

        # Step 1: Merge context with hypotheses and optional notes
        eval_context = {
            **context,
            "hypothesis_a": output_a,
            "hypothesis_b": output_b,
            "comparison_notes": self.cfg.get("comparison_notes", ""),
        }

        # Step 2: Load the evaluation prompt
        prompt_loader = PromptLoader(None, self.logger)
        prompt_text = prompt_loader.from_file(self.prompt_file, self.cfg, eval_context)

        # Step 3: Run the LLM to get a judgement
        raw_response = self.llm(prompt_text, eval_context, llm_cfg=self.llm_cfg)
        cleaned_response = remove_think_blocks(raw_response)
        parsed = parse_response(cleaned_response)

        # Step 4: Determine preferred output and package scores
        preferred_output = output_a if parsed["winner"] == "A" else output_b
        scores = {
            "winner": parsed["winner"],
            "reason": parsed["reason"],
            "score_a": parsed["score_a"],
            "score_b": parsed["score_b"],
        }

        # Step 5: Logging
        self.logger.log(
            "LLMJudgeResult",
            {
                "prompt": prompt,
                "output_a": output_a[:100],
                "output_b": output_b[:100],
                "winner": parsed["winner"],
                "score_a": parsed["score_a"],
                "score_b": parsed["score_b"],
                "reason": parsed["reason"],
                "raw_response": cleaned_response[:300],
            },
        )

        return preferred_output, scores

    def score_single(self, prompt, output, context: dict):
        """
        Compare two outputs using an LLM-based judge.

        Args:
            prompt (str): The name or identifier for the evaluation prompt.
            output_a (str): First hypothesis/output to compare.
            output_b (str): Second hypothesis/output to compare.
            context (dict): Execution context with additional variables.

        Returns:
            tuple: (preferred_output, score_details)
        """

        # Step 1: Merge context with hypotheses and optional notes
        eval_context = {
            **context,
            "hypothesis": output,
            "comparison_notes": self.cfg.get("comparison_notes", ""),
        }

        # Step 2: Load the evaluation prompt
        prompt_loader = PromptLoader(None, self.logger)
        prompt_text = prompt_loader.from_file(self.prompt_file, self.cfg, eval_context)

        # Step 3: Run the LLM to get a judgement
        raw_response = self.llm(prompt_text, eval_context, llm_cfg=self.llm_cfg)
        cleaned_response = remove_think_blocks(raw_response)
        parsed = parse_response(cleaned_response)

        # Step 4: Determine preferred output and package scores
        # Step 5: Logging
        self.logger.log(
            "LLMJudgeSinglwResult",
            {
                "prompt": prompt,
                "output": output[:100],
                "score_a": parsed["score_a"],
                "score_b": parsed["score_b"],
                "reason": parsed["reason"],
                "raw_response": cleaned_response[:300],
            },
        )

        return parsed


def parse_response(response: str):
    # Normalize spacing
    lines = response.strip().splitlines()
    text = "\n".join(
        [line.strip() for line in lines if line.strip()]
    )  # remove extra spaces

    # Flexible matchers
    winner_match = re.search(
        r"better hypothesis[:ï¼š]\s*<?([AB])>?", text, re.IGNORECASE
    )
    reason_match = re.search(
        r"reason[:ï¼š]\s*(.+?)(?=\n(?:score_a|score_b)[:ï¼š])",
        text,
        re.IGNORECASE | re.DOTALL,
    )
    score_a_match = re.search(r"score_a[:ï¼š]\s*<?(\d{1,3})>?", text, re.IGNORECASE)
    score_b_match = re.search(r"score_b[:ï¼š]\s*<?(\d{1,3})>?", text, re.IGNORECASE)

    return {
        "winner": (winner_match.group(1).upper() if winner_match else "A"),
        "reason": (
            reason_match.group(1).strip() if reason_match else "No reason provided."
        ),
        "score_a": int(score_a_match.group(1)) if score_a_match else 0,
        "score_b": int(score_b_match.group(1)) if score_b_match else 0,
    }


def remove_think_blocks(text: str) -> str:
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
---END-OF-FILE---


"co_ai\evaluator\mrq_self_evaluator.py"
---START-OF-FILE---
import torch

from co_ai.evaluator.base import BaseEvaluator
from co_ai.evaluator.hypothesis_value_predictor import HypothesisValuePredictor
from co_ai.evaluator.mrq_trainer import MRQTrainer
from co_ai.evaluator.text_encoder import TextEncoder
from co_ai.models.sharpening_prediction import SharpeningPredictionORM


class MRQSelfEvaluator(BaseEvaluator):
    def __init__(self, memory, logger, device="cpu"):
        self.device = device
        self.memory = memory  # memory provides get_embedding
        self.logger = logger
        self.encoder = TextEncoder().to(self.device)
        self.value_predictor = HypothesisValuePredictor(512, 1024).to(self.device)
        self.trainer = MRQTrainer(
            memory=self.memory,
            logger=self.logger,
            value_predictor=self.value_predictor,
            encoder=self.encoder,
            device=self.device,
        )

    def judge(self, goal, prompt, output_a, output_b):
        prompt_emb = torch.tensor(
            self.memory.embedding.get_or_create(prompt), device=self.device
        ).unsqueeze(0)
        output_a_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_a), device=self.device
        ).unsqueeze(0)
        output_b_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_b), device=self.device
        ).unsqueeze(0)

        zsa_a = self.encoder(prompt_emb, output_a_emb)
        zsa_b = self.encoder(prompt_emb, output_b_emb)

        value_a = self.value_predictor(zsa_a).item()
        value_b = self.value_predictor(zsa_b).item()

        preferred_output = output_a if value_a >= value_b else output_b
        scores = {"value_a": value_a, "value_b": value_b}

        if self.memory.mrq.log_evaluations():
            prediction = SharpeningPredictionORM(
                id=None,
                goal_id=-1,
                prompt_text=prompt,
                output_a=output_a,
                output_b=output_b,
                preferred="a" if value_a >= value_b else "b",
                predicted="a" if value_a >= value_b else "b",
                value_a=value_a,
                value_b=value_b,
            )

            self.memory.sharpening.insert_sharpening_prediction(
                prediction.to_dict(), goal
            )

        return preferred_output, scores

    def score_single(self, prompt: str, output: str, context: dict) -> float:
        prompt_emb = torch.tensor(
            self.memory.embedding.get_or_create(prompt), device=self.device
        ).unsqueeze(0)
        output_emb = torch.tensor(
            self.memory.embedding.get_or_create(output), device=self.device
        ).unsqueeze(0)
        zsa = self.encoder(prompt_emb, output_emb)
        value = self.value_predictor(zsa).item()
        return value

    def train_from_database(self, goal: str, cfg: dict):
        samples = self.memory.mrq.get_training_pairs(
            goal=goal, limit=cfg.get("limit", 1000)
        )
        if not samples:
            self.logger.log(
                "MRQTrainingError",
                {
                    "error": "No training samples found for the given goal.",
                    "goal": goal,
                },
            )
            return

        dataloader = self.trainer.prepare_training_data(samples)
        self.trainer.train(dataloader, cfg)

    def train_from_context(self, context: dict, cfg: dict):
        samples = context.get("mrq_training_pairs", [])
        if not samples:
            self.logger.log(
                "MRQContextTrainingError",
                {"error": "No training samples found in context."},
            )
            return

        dataloader = self.trainer.prepare_training_data(samples)
        self.trainer.train(dataloader, cfg)
---END-OF-FILE---


"co_ai\evaluator\mrq_trainer.py"
---START-OF-FILE---
# co_ai/evaluator/mrq_trainer.py
import copy
from collections import defaultdict

import torch
from torch.utils.data import DataLoader, TensorDataset

from co_ai.evaluator.hypothesis_value_predictor import HypothesisValuePredictor
from co_ai.evaluator.text_encoder import TextEncoder


class MRQTrainer:
    def __init__(self, memory, logger, encoder=None, value_predictor=None, device="cpu"):
        self.memory = memory
        self.logger = logger
        self.device = device
        self.value_predictor = HypothesisValuePredictor(512, 1024).to(self.device)
        if encoder is not None:
            self.encoder = encoder.to(device)
        else:
            self.encoder = TextEncoder().to(device)
        if value_predictor is not None:            
            self.value_predictor = value_predictor.to(device)
        else:
            self.value_predictor = HypothesisValuePredictor(512, 1024).to(device)

    def prepare_training_data(self, samples):
        inputs, labels = [], []
        for item in samples:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["output_a"])
            output_b_emb = self.memory.embedding.get_or_create(item["output_b"])
            preferred = item["preferred"]

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if preferred == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0], device=self.device))

        dataset = TensorDataset(torch.stack(inputs), torch.stack(labels))
        return DataLoader(dataset, batch_size=16, shuffle=True)

    def train(self, dataloader, cfg):
        epochs = cfg.get("epochs", 20)
        lr = cfg.get("lr", 1e-4)
        patience = cfg.get("patience", 3)
        min_delta = cfg.get("min_delta", 0.0001)

        opt = torch.optim.Adam(self.value_predictor.parameters(), lr=lr)
        self.value_predictor.train()

        best_loss = float("inf")
        epochs_no_improve = 0

        first = next(iter(dataloader))
        print("Sample batch:", first)

        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                loss = -torch.log(torch.sigmoid(preds)).mean()
                opt.zero_grad()
                loss.backward()
                opt.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log("MRQTrainerEpoch", {
                "epoch": epoch + 1,
                "avg_loss": round(avg_loss, 5)
            })

            if best_loss - avg_loss > min_delta:
                best_loss = avg_loss
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve >= patience:
                    self.logger.log("MRQTrainerEarlyStopping", {
                        "stopped_epoch": epoch + 1,
                        "best_loss": round(best_loss, 5)
                    })
                    break

        self.logger.log(
            "MRQTrainerTrainingComplete",
            {"epochs_trained": epoch + 1, "final_loss": round(avg_loss, 5)},
        )

    def train_multidimensional_model(self, contrast_pairs, cfg=None):
        """
        Trains a separate model for each scoring dimension using the provided contrast pairs.
        Each pair is a dict with: output_a, output_b, prompt, preferred, dimension.
        """
        from collections import defaultdict

        # Group contrast pairs by dimension
        by_dimension = defaultdict(list)
        for pair in contrast_pairs:
            dim = pair["dimension"]
            by_dimension[dim].append(pair)

        trained_models = {}

        for dim, samples in by_dimension.items():
            if not samples:
                self.logger.log("DimensionSkippedNoSamples", {"dimension": dim})
                continue

            self.logger.log(
                "TrainingDimensionStart",
                {"dimension": dim, "num_samples": len(samples)},
            )

            dataloader = self.prepare_training_data(samples)
            self.train(dataloader, cfg or {})

            # Save model for this dimension if needed
            trained_models[dim] = self.value_predictor.state_dict()

            self.logger.log(
                "TrainingDimensionComplete", {"dimension": dim, "samples": len(samples)}
            )

        return trained_models
---END-OF-FILE---


"co_ai\evaluator\sae.py"
---START-OF-FILE---
import torch
from torch import nn


# Sparse Autoencoder for embedding analysis and dimensionality reduction
class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed, latent

    def encode(self, x):
        return self.encoder(x)---END-OF-FILE---


"co_ai\evaluator\scores_to_training.py"
---START-OF-FILE---
class MRQScoresToTraining:
    def __init__(self, memory, logger, min_score_diff=0.1):
        self.memory = memory
        self.logger = logger
        self.min_score_diff = min_score_diff

    def get_training_pairs(self, context: dict = None) -> list:
        goal_groups = {}
        scores = self.memory.evaluations.get_all()

        for score in scores:
            hypothesis = self.memory.hypotheses.get_by_id(score.get("hypothesis_id"))
            if not hypothesis or not hypothesis.prompt or not hypothesis.text:
                continue

            goal_id = hypothesis.goal_id
            goal_groups.setdefault(goal_id, []).append({
                "prompt": hypothesis.prompt.prompt_text,
                "response": hypothesis.text,
                "score": score.get("score"),
                "hypothesis_id": hypothesis.id,
            })

        training_pairs = []
        for goal_id, items in goal_groups.items():
            sorted_items = sorted(items, key=lambda x: x["score"], reverse=True)
            for i in range(len(sorted_items)):
                for j in range(i + 1, len(sorted_items)):
                    hi = sorted_items[i]
                    hj = sorted_items[j]
                    if abs(hi["score"] - hj["score"]) >= self.min_score_diff:
                        training_pairs.append({
                            "prompt": hi["prompt"],
                            "output_a": hi["response"],
                            "output_b": hj["response"],
                            "preferred": "a",
                            "goal_id": goal_id,
                            "hypothesis_a_id": hi["hypothesis_id"],
                            "hypothesis_b_id": hj["hypothesis_id"],
                        })

        if self.logger:
            self.logger.log("MRQPairsGenerated", {"count": len(training_pairs)})

        if context is not None:
            context["mrq_training_pairs"] = training_pairs

        return training_pairs
---END-OF-FILE---


"co_ai\evaluator\scores_training.py"
---START-OF-FILE---
---END-OF-FILE---


"co_ai\evaluator\text_encoder.py"
---START-OF-FILE---
import torch
import torch.nn as nn
import torch.nn.functional as F


# TextEncoder for embedding prompts and hypotheses
class TextEncoder(nn.Module):
    def __init__(self, embedding_dim=1024, zs_dim=512, za_dim=256, zsa_dim=512, hdim=1024):
        super().__init__()
        self.zs_mlp = nn.Sequential(
            nn.Linear(embedding_dim, hdim),
            nn.ReLU(),
            nn.Linear(hdim, zs_dim)
        )
        self.za_mlp = nn.Sequential(
            nn.Linear(embedding_dim, hdim),
            nn.ReLU(),
            nn.Linear(hdim, za_dim)
        )
        self.zsa_mlp = nn.Sequential(
            nn.Linear(zs_dim + za_dim, zsa_dim),
            nn.ReLU(),
            nn.Linear(zsa_dim, zsa_dim)
        )

    def forward(self, prompt_emb, response_emb):
        zs = F.relu(self.zs_mlp(prompt_emb))
        za = F.relu(self.za_mlp(response_emb))
        zsa = self.zsa_mlp(torch.cat([zs, za], dim=1))
        return zsa
---END-OF-FILE---


"co_ai\interface\__init__.py"
---START-OF-FILE---
"""User interaction modules (e.g., CLI input)"""
from .cli import get_user_goal
---END-OF-FILE---


"co_ai\interface\cli.py"
---START-OF-FILE---
# co_ai/interface/cli.py

def get_user_goal() -> str:
    return input("Enter your research goal: ").strip()
---END-OF-FILE---


"co_ai\logs\__init__.py"
---START-OF-FILE---
"""Main logger for app"""
from .icons import get_event_icon
from .json_logger import JSONLogger
---END-OF-FILE---


"co_ai\logs\console_logger.py"
---START-OF-FILE---
import sys
from datetime import datetime, timezone

from co_ai.logs.icons import get_event_icon


class ConsoleLogger:
    def __init__(self, stream=None):
        self.stream = stream or sys.stdout  # default to stdout

    def log(self, event_type: str, data: dict):
        icon = get_event_icon(event_type)
        truncated = str(data)[:100]

        log_entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event_type": event_type,
            "data": data,
        }

        try:
            print(f"{icon} [{event_type}] {truncated}", file=self.stream)
            print(f"ðŸ•’  {log_entry['timestamp']}", file=self.stream)
        except Exception as e:
            print("âŒ [ConsoleLogger] Failed to print log entry.", file=sys.stderr)
            print(f"ðŸ› ï¸  Event Type: {event_type}", file=sys.stderr)
            print(f"ðŸªµ  Error: {e}", file=sys.stderr)
            print(f"ðŸ§±  Data: {repr(data)[:200]}", file=sys.stderr)
---END-OF-FILE---


"co_ai\logs\icons.py"
---START-OF-FILE---
# co_ai/logs/icons.py


def get_event_icon(event_type: str) -> str:
    """
    Get the icon associated with a specific event type.
    """
    return EVENT_ICONS.get(event_type, "â“")


EVENT_ICONS = {
    # General System & Initialization
    "AgentInitialized": "ðŸ› ï¸",
    "ContextLoaded": "ðŸ“‚",
    "ContextSaved": "ðŸ’¾",
    "ContextAfterStage": "ðŸ—ƒï¸",
    "ContextYAMLDumpSaved": "ðŸ“„",
    "debug": "ðŸž",
    "NoHypothesesInContext": "ðŸ¤·â€â™‚ï¸",
    "SimilarHypothesesFound": "â™»ï¸ðŸ’­",
    "StageContext": "ðŸŽ¯",
    "AgentInit": "ðŸ¤–",
    "NodeDebug": "ðŸŒ²ðŸ”",
    "NodeSummary": "ðŸªµðŸ“‹",
    "CorDimensionEvaluated": "ðŸ“âœ…",
    "EvaluatorInit": "ðŸ§ª",
    "RuleApplicationUpdated": "ðŸ§©",      # Suggests a symbolic piece being modified
    "MRQScoringComplete": "ðŸ“ˆ",          # Indicates successful scoring/completion
    "NoSymbolicAgentRulesApplied": "ðŸš«", # Signifies nothing matched/applied
    "RuleApplicationsScored": "ðŸŽ¯",      # Represents target scoring of rule usage
    "RuleApplicationCount": "ðŸ”¢",        # Suggests counting or tracking quantity
    # Pipeline Execution
    "StoreRegistered": "ðŸ›’",
    "SupervisorInit": "ðŸ§‘â€ðŸ«",
    "PipelineStart": "ðŸ”¬",
    "PipelineStageStart": "ðŸš€",
    "PipelineStageEnd": "ðŸ",
    "PipelineStageSkipped": "â­ï¸",
    "PipelineIterationStart": "ðŸ”„",
    "PipelineIterationEnd": "ðŸ”âœ…ðŸ”š",
    "PipelineRunInserted": "ðŸ”ðŸ—ƒï¸",
    "PipelineSuccess": "âœ…",
    "PipelineError": "âŒ",
    "IterationStart": "ðŸ”„",
    "IterationEnd": "ðŸ”š",
    "AgentRunStarted": "ðŸš€",
    "AgentRunCompleted": "ðŸ",
    "AgentRanSuccessfully": "âœ…",
    "TrainingEpoch": "ðŸ‹ï¸â€â™‚ï¸",
    "EarlyStopping": "â¹ï¸â³",
    "TrainingComplete": "ðŸŽ‰âœ…",
    "SymbolicAgentOverride": "ðŸ› ï¸",
    "RuleApplicationLogged": "Roland who they are they are",
    "ScoreParsed": "ðŸ“Š",
    "SymbolicRulesFound": "ðŸ§©",
    "DuplicateSymbolicRuleSkipped": "â™»ï¸",
    "SymbolicAgentRulesFound": "ðŸ”Ž",
    "PromptLookup": "ðŸ“š",
    "PipelineJudgeAgentStart": "âš–ï¸ðŸš¦",
    "HypothesesReceived": "ðŸ§ ðŸ“¥",
    "PromptLoaded": "ðŸ“",
    "JudgementReceived": "ðŸ—£ï¸",
    "ScoreSaved": "ðŸ’¾",
    "PipelineJudgeAgentEnd": "ðŸ›‘âš–ï¸",
    "PipelineScoreSummary": "ðŸ“ˆðŸ§®",
    "SymbolicPipelineSuggestion": "ðŸ§ ðŸ’¡",

    # Prompt Processing & Tuning
    "Prompt": "ðŸ“œ",
    "PromptGenerated": "ðŸ“",
    "PromptStored": "ðŸ—ƒðŸ—ƒï¸",
    "PromptLogged": "ðŸ§¾",
    "PromptFileNotFound": "ðŸš«",
    "PromptLoadFailed": "â“",
    "PromptParseFailed": "âš ï¸",
    "PromptEvaluationFailed": "âŒ",
    "PromptComparisonResult": "ðŸ",
    "PromptComparisonNoMatch": "ðŸ§ªðŸ“„âŒ",
    "PromptAResponseGenerated": "ðŸ…°ï¸",
    "PromptBResponseGenerated": "ðŸ…±ï¸",
    "PromptABResponseGenerated": "ðŸ…°ï¸",
    "PromptQualityCompareStart": "âš–ï¸",
    "PromptTuningCompleted": "ðŸ§ªâœ¨",
    "PromptTuningSkipped": "â­ï¸",
    "PromptTuningExamples": "ðŸ“š",
    "TunedPromptStored": "ðŸ—ƒï¸",
    "TunedPromptGenerationFailed": "âŒ",
    "ComparisonPromptConstructed": "ðŸ› ï¸",
    "ComparisonResponseReceived": "ðŸ“©",
    "LLMCacheHit": "âœ…",
    "MRQTrainingStart": "So it's very",
    "MRQTrainingEpoch": "ðŸ“ˆ",
    "MRQTrainingComplete": "ðŸ",
    "MRQTraining": "ðŸ“ŠðŸ› ï¸",
    "MRQTrainingDataLoaded": "ðŸ§ ðŸ“¥",
    "MRQPipelineSuggested": "ðŸ§ ðŸ›¤ï¸",
    # goals
    "GoalCreated": "ðŸŽ¯ðŸ’¾",
    # Hypotheses Generation
    "GenerationAgent": "ðŸ§ª",
    "GeneratedHypotheses": "ðŸ’¡",
    "GenerationStart": "âœ¨",
    "GenerationStarted": "ðŸŽ¯",
    "DatasetLoading": "â³ðŸ“¦",
    "DatasetLoaded": "âœ…ðŸ“‚",
    "DPOGenerated": "ðŸ”ðŸ§ ",
    "TrainingStarted": "ðŸš€ðŸ“Š",
    "AdaptiveReasoningResponse": "ðŸ¤–ðŸª„",
    "GenerationCompleted": "âœ…",
    "HypothesisStored": "ðŸ’¾",
    "HypothesisStoreFailed": "âŒ",
    "HypothesisInserted": "ðŸ’¡ðŸ“¥",
    # Hypotheses Evaluation & Ranking
    "RankingAgent": "ðŸ†",
    "RankedHypotheses": "ðŸ…",
    "RankingStored": "ðŸ—ƒï¸",
    "RankingUpdated": "ðŸ”",
    "GoalContextOverride": "ðŸŽ¯",
    "DimensionEvaluated": "ðŸ“",
    "ScoreLinkedToRuleApplications": "ðŸ”—",
    "ScoreSavedToMemory": "ðŸ’¾",
    "HypothesisScoreComputed": "ðŸ§®",
    "NotEnoughHypothesesForRanking": "âš ï¸",
    "LLMJudgeResult": "âš–ï¸",
    "EvaluationCompleted": "ðŸ“Š",
    "ScoreComputed": "ðŸ§®ðŸ“Šâœ…",
    "ReviewScoreComputed": "ðŸ§‘â€âš–ï¸ðŸ“Š",
    "ReflectionScoreComputed": "ðŸªžðŸ“Šâœ…",
    "ScoreStored": "ðŸ’¾",
    # Evolution
    "EvolutionAgent": "ðŸ§¬",
    "EvolvingTopHypotheses": "ðŸ”„",
    "EvolvedHypotheses": "ðŸŒ±",
    "EvolvedParsedHypotheses": "ðŸ§¬",
    "EvolutionCompleted": "ðŸ¦¾",
    "EvolutionError": "âš ï¸",
    "AdaptiveModeDecision": "ðŸ§ âš–ï¸",
    "GraftingPair": "ðŸŒ¿",
    # Review & Reflection
    "ReflectionAgent": "ðŸªž",
    "ReflectionStart": "ðŸ¤”",
    "ReflectionStored": "ðŸ’¾",
    "ReflectionDeltaInserted": "ðŸ§©ðŸ“ˆ",
    "ReflectionDeltaLogged": "ðŸ”ðŸ“",
    "MetaReviewAgent": "ðŸ§ ",
    "MetaReviewInput": "ðŸ“‰",
    "MetaReviewSummary": "ðŸ“˜",
    "RawMetaReviewOutput": "ðŸ“œ",
    "GeneratedReviews": "ðŸ§¾",
    "ReviewStored": "ðŸ’¬",
    "SharpenedHypothesisSaved": "ðŸª“ðŸ’¾",
    "SharpenedGoalSaved": "ðŸª“ðŸ†",
    "IdeaSharpenedAndSaved": "ðŸ’¡ðŸª“ðŸ’¾",
    "SummaryLogged": "ðŸ“",
    "RefinedSkipped": "â­ï¸",
    "RefinedUpdated": "ðŸ”„",
    "CoTGenerated": "ðŸ§ ðŸ”—ðŸ“",
    # Refiner Agent
    "RefinerStart": "ðŸ”„",
    "RefinerPromptGenerated": "ðŸ’¡",
    "RefinerEvaluationPromptGenerated": "ðŸ’¬",
    "RefinerResponseGenerated": "ðŸ’¬",
    "RefinerEvaluationResponse": "ðŸ“Š",
    "RefinerHypothesesExtracted": "ðŸ”",
    "RefinerImprovementPromptLoaded": "ðŸ“œ",
    "RefinerNoHistoryFound": "ðŸš«",
    "RefinerError": "âŒ",
    # Literature & Research
    "LiteratureAgentInit": "ðŸ“š",
    "LiteratureQuery": "ðŸ“š",
    "LiteratureQueryFailed": "ðŸ“šâŒ",
    "LiteratureSearchCompleted": "ðŸ“šâœ…",
    "LiteratureSearchSkipped": "ðŸ“šâ­ï¸",
    "NoResultsFromWebSearch": "ðŸŒðŸš«",
    "ProximityGraphComputed": "ðŸ—ºï¸",
    "SearchQuery": "ðŸ”",
    "SearchingWeb": "ðŸŒ",
    "DatabaseHypothesesMatched": "ðŸ”",
    "SearchResult": "ðŸ”ŽðŸ“„",
    "LLMPromptGenerated_SearchQuery": "ðŸ§ ðŸ”",
    "LLMResponseReceived_SearchQuery": "ðŸ“¥ðŸ”",
    "LLMPromptGenerated_Summarize": "ðŸ§ ðŸ“„",
    "LLMResponseReceived_Summarize": "ðŸ“¥ðŸ“„",
    # Reporting
    "ReportGenerated": "ðŸ“Š",
    "GoalFetchedByText": "ðŸ“„ðŸ”",
    "GoalExists": "âœ”ï¸ðŸ“Œ",
    "BatchProcessingStart": "ðŸ“¥",
    # Rubric Patterns
    "RubricPatternsStored": "ðŸ“šðŸ§©ðŸ’¾",
    "PatternStatsStored": "ðŸ“ŠðŸ§©ðŸ’¾",
    "RubricClassified": "ðŸ“Œ",
    "PromptFileLoading": "ðŸ—‚ï¸ðŸ“¥",
    "PromptFileLoaded": "âœ…ðŸ“„",
    "ProximityAnalysisScored": "ðŸ—ºï¸ðŸ“Š",
    "DifficultySummary": "ðŸ“‹ðŸ§©",
    "SampleByDifficulty": "ðŸ§ªðŸ“š",
    "PreferencePairSaveError": "âŒðŸ’¾",
    "TrainingError": "ðŸ”§ðŸ’¥",
    "ClassificationStarted": "ðŸ”",
    "ClassificationCompleted": "ðŸ“‹",
    # SQL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "SQLQuery": "ðŸ§®",
}
---END-OF-FILE---


"co_ai\logs\json_logger.py"
---START-OF-FILE---
import json
from datetime import datetime, timezone
from pathlib import Path

from co_ai.logs.icons import get_event_icon


class JSONLogger:
    def __init__(self, log_path="logs/pipeline_log.jsonl"):
        self.log_path = Path(log_path)
        self.log_path.parent.mkdir(parents=True, exist_ok=True)

    def log(self, event_type: str, data: dict):
        icon = get_event_icon(event_type)
        print(f"{icon} [{event_type}] {str(data)[:100]}")

        log_entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event_type": event_type,
            "data": data,
        }

        try:
            with self.log_path.open("a", encoding="utf-8") as f:
                json.dump(log_entry, f, default=str)
                f.write("\n")
        except (TypeError, ValueError) as e:
            print("âŒ [Logger] Failed to serialize log entry.")
            print(f"ðŸ› ï¸  Event Type: {event_type}")
            print(f"ðŸªµ  Error: {e}")
            print(f"ðŸ§±  Data: {repr(data)[:200]}")

    def get_logs_by_type(self, event_type: str) -> list:
        """
        Retrieve all logs of a specific type from the log file
        
        Args:
            event_type: The type of event to filter by
            
        Returns:
            List of matching log entries
        """
        if not self.log_path.exists():
            return []
            
        logs = []
        try:
            with self.log_path.open("r", encoding="utf-8") as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        if entry["event_type"] == event_type:
                            logs.append(entry)
                    except json.JSONDecodeError:
                        continue  # Skip invalid lines
        except Exception as e:
            print(f"âŒ [Logger] Failed to read logs: {str(e)}")
            return []
            
        return logs

    def get_all_logs(self) -> list:
        """
        Retrieve all logs from the file
        
        Returns:
            List of all log entries
        """
        if not self.log_path.exists():
            return []
            
        logs = []
        try:
            with self.log_path.open("r", encoding="utf-8") as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        logs.append(entry)
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"âŒ [Logger] Failed to read logs: {str(e)}")
            return []
            
        return logs---END-OF-FILE---


"co_ai\memory\__init__.py"
---START-OF-FILE---
"""Memory management and embedding tools"""
from .base import BaseStore
from .context_store import ContextStore
from .embedding_store import EmbeddingStore
from .evaluation_store import EvaluationStore
from .goal_store import GoalStore
from .hypothesis_store import HypothesisStore
from .idea_store import IdeaStore
from .lookahead_store import LookaheadStore
from .memory_tool import MemoryTool
from .pattern_store import PatternStatStore
from .pipeline_run_store import PipelineRunStore
from .prompt_store import PromptStore
from .report_logger import ReportLogger
from .rule_application_store import RuleApplicationStore
from .search_result_store import SearchResultStore
from .sharpening_store import SharpeningStore
from .symbolic_rule_store import SymbolicRuleStore
---END-OF-FILE---


"co_ai\memory\base.py"
---START-OF-FILE---
# co_ai/memory/base_store.py
from abc import ABC, abstractmethod


class BaseStore(ABC):

    def __init__(self, db, logger=None):
        self.db = db
        self.logger = logger

    @property
    @abstractmethod
    def name(self) -> str:
        pass

    def setup(self):
        """Optional: Setup logic for the store."""
        pass

    def teardown(self):
        """Optional: Cleanup logic for the store."""
        pass
---END-OF-FILE---


"co_ai\memory\context_store.py"
---START-OF-FILE---
# stores/context_store.py
import json
import os
from datetime import datetime, timezone
from typing import Optional

import yaml
from sqlalchemy.orm import Session

from co_ai.models.context_state import ContextStateORM


class ContextStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "context"
        self.dump_dir = logger.log_path if logger else None
        if self.dump_dir:
            self.dump_dir = os.path.dirname(self.dump_dir)

    def save(self, run_id: str, stage: str, context: dict, preferences: dict = None, extra_data: dict = None):
        """
        Saves the current pipeline context to database and optionally to disk.
        Increments version and marks it as current for this stage/run.
        """
        try:
            # Deactivate previous versions
            prev_versions = self.session.query(ContextStateORM).filter_by(run_id=run_id, stage_name=stage).all()
            for state in prev_versions:
                state.is_current = False

            # Get latest version number
            latest_version = max((s.version for s in prev_versions), default=0)
            new_version = latest_version + 1

            # Create new context state
            db_context = ContextStateORM(
                run_id=run_id,
                stage_name=stage,
                version=new_version,
                is_current=True,
                context=json.dumps(context),
                preferences=json.dumps(preferences) if preferences else None,
                extra_data=json.dumps(extra_data or {}),
                timestamp=datetime.now(timezone.utc)
            )

            self.session.add(db_context)
            self.session.flush()  # To get ID immediately

            if self.dump_dir:
                self._dump_to_yaml(run_id, stage, context)

            if self.logger:
                self.logger.log("ContextSaved", {
                    "run_id": run_id,
                    "stage": stage,
                    "version": new_version,
                    "timestamp": db_context.timestamp.isoformat(),
                    "is_current": True
                })

        except Exception as e:
            self.session.rollback()
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("ContextSaveFailed", {"error": str(e)})
            raise

    def has_completed(self, run_id: str, stage_name: str) -> bool:
        """Check if this stage has already been run"""
        count = (
            self.session.query(ContextStateORM)
            .filter_by(run_id=run_id, stage_name=stage_name)
            .count()
        )
        return count > 0

    def load(self, run_id: str, stage: Optional[str] = None) -> dict:
        try:
            session = self.session if self.session.is_active else self.sessionmaker()

            if stage:
                states = (
                    session.query(ContextStateORM)
                    .filter_by(run_id=run_id, stage_name=stage)
                    .order_by(ContextStateORM.timestamp.asc())
                    .all()
                )
            else:
                states = session.query(ContextStateORM).filter_by(run_id=run_id).all()

            result = {}
            for state in states:
                result.update(json.loads(state.context))

            return result

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.session.rollback()
            if self.logger:
                self.logger.log("ContextLoadFailed", {"error": str(e)})
            return {}

    def _dump_to_yaml(self, run_id: str, stage: str, context: dict):
        os.makedirs(self.dump_dir, exist_ok=True)
        timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S")
        filename = f"{run_id}_{stage}_{timestamp}.yaml"
        path = os.path.join(self.dump_dir, filename)

        try:
            with open(path, "w", encoding="utf-8") as f:
                yaml.dump(context, f, allow_unicode=True, sort_keys=False)

            if self.logger:
                self.logger.log("ContextYAMLDumpSaved", {"path": path})

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("ContextYAMLDumpFailed", {"error": str(e)})---END-OF-FILE---


"co_ai\memory\embedding_store.py"
---START-OF-FILE---
import hashlib

from co_ai.memory import BaseStore
from co_ai.tools import get_embedding
from co_ai.utils.lru_cache import SimpleLRUCache


class EmbeddingStore(BaseStore):
    def __init__(self, cfg, conn, db, logger=None, cache_size=10000):
        super().__init__(db, logger)
        self.cfg = cfg
        self.conn = conn
        self.name = "embedding"
        self._cache = SimpleLRUCache(max_size=cache_size)


    def __repr__(self):
        return f"<{self.name} connected={self.db is not None} cfg={self.cfg}>"

    def name(self) -> str:
        return "embedding"

    def get_or_create(self, text: str):
        text_hash = self.get_text_hash(text)

        cached = self._cache.get(text_hash)
        if cached:
            return cached

        try:
            with self.conn.cursor() as cur:
                cur.execute("SELECT embedding FROM embeddings WHERE text_hash  = %s", (text_hash,))
                row = cur.fetchone()
                if row:
                    return row[0]  # Force conversion to list of floats
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("EmbeddingFetchFailed", {"error": str(e)})

        embedding = get_embedding(text, self.cfg)
        try:
            with self.conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO embeddings (text, text_hash, embedding)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (text_hash) DO NOTHING
                    RETURNING text_hash;
                """,
                    (text, text_hash, embedding),
                )
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("EmbeddingInsertFailed", {"error": str(e)})
        self._cache.set(text_hash, embedding)
        return embedding


    def search_related(self, query: str, top_k: int = 5):
        try:
            embedding = get_embedding(query, self.cfg)
            with self.conn.cursor() as cur:
                cur.execute(
                    """
                        SELECT 
                            h.text,
                            g.goal_text AS goal,
                            h.confidence,
                            h.review
                        FROM hypotheses h
                        JOIN goals g ON h.goal_id = g.id
                        ORDER BY h.embedding <-> %s
                        LIMIT %s;
                    """,
                    (embedding, top_k)
                )
                results = cur.fetchall()

            if self.logger:
                self.logger.log("HypothesesSearched", {
                    "query": query,
                    "top_k": top_k,
                    "result_count": len(results)
                })

            return results
        except Exception as e:
            if self.logger:
                self.logger.log("HypothesesSearchFailed", {
                    "error": str(e),
                    "query": query
                })
            else:
                print(f"[VectorMemory] Search failed: {e}")
            return []

    def get_text_hash(self, text: str) -> str:
        return hashlib.sha256(text.encode("utf-8")).hexdigest()---END-OF-FILE---


"co_ai\memory\evaluation_store.py"
---START-OF-FILE---
# stores/score_store.py
import json
from typing import Optional

from sqlalchemy.orm import Session

from co_ai.models import RuleApplicationORM
from co_ai.models.evaluation import EvaluationORM
from co_ai.models.evaluation_rule_link import EvaluationRuleLinkORM
from co_ai.models.goal import GoalORM


class EvaluationStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "evaluations"
        self.table_name = "evaluations"

    def insert(self, evaluation: EvaluationORM):
        """
        Inserts a new score into the database.
        Accepts a dictionary (e.g., from Score dataclass).
        """
        try:
            self.session.add(evaluation)
            self.session.flush()  # To get ID immediately

            if self.logger:
                self.logger.log(
                    "ScoreStored",
                    {
                        "evaluation_id": evaluation.id,
                        "goal_id": evaluation.goal_id,
                        "hypothesis_id": evaluation.hypothesis_id,
                        "agent": evaluation.agent_name,
                        "model": evaluation.model_name,
                        "scores": evaluation.scores,
                        "timestamp": evaluation.created_at.isoformat(),
                    },
                )

            # Link score to rule application if possible
            if evaluation.pipeline_run_id and evaluation.goal_id:
                rule_apps = (
                    self.session.query(RuleApplicationORM)
                    .filter_by(pipeline_run_id=evaluation.pipeline_run_id, goal_id=evaluation.goal_id)
                    .all()
                )
                for ra in rule_apps:
                    link = EvaluationRuleLinkORM(evaluation_id=evaluation.id, rule_application_id=ra.id)
                    self.session.add(link)
                self.logger.log(
                    "ScoreLinkedToRuleApplications",
                    {
                        "score_id": evaluation.id,
                        "linked_rule_application_ids": [ra.id for ra in rule_apps],
                    },
                )

            self.session.refresh(evaluation)
            self.session.commit()
            return evaluation.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("ScoreInsertFailed", {"error": str(e)})
            raise

    def get_by_goal_id(self, goal_id: int) -> list[dict]:
        """Returns all scores associated with a specific goal."""
        results = self.session.query(EvaluationORM).join(GoalORM).filter(GoalORM.id == goal_id).all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_goal_type(self, goal_type: str) -> list[dict]:
        """Returns all scores associated with a specific goal."""
        results = self.session.query(EvaluationORM).join(GoalORM).filter(GoalORM.goal_type == goal_type).all()
        return [self._orm_to_dict(r) for r in results]


    def get_by_hypothesis_id(
        self,
        hypothesis_id: int,
        source: Optional[str] = None
    ) -> list[dict]:
        """Returns all scores associated with a specific hypothesis, optionally filtered by evaluator source."""
        query = self.session.query(EvaluationORM).filter(EvaluationORM.hypothesis_id == hypothesis_id)
        
        if source:
            query = query.filter(EvaluationORM.evaluator_name == source)

        results = query.all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_run_id(self, run_id: str) -> list[dict]:
        """Returns all scores associated with a specific pipeline run."""
        results = self.session.query(EvaluationORM).filter(EvaluationORM.run_id == run_id).all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_pipeline_run_id(self, pipeline_run_id: int) -> list[dict]:
        """Returns all scores associated with a specific pipeline run."""
        results = self.session.query(EvaluationORM).filter(EvaluationORM.pipeline_run_id == pipeline_run_id).all()
        return [self._orm_to_dict(r) for r in results]


    def get_by_evaluator(self, evaluator_name: str) -> list[dict]:
        """Returns all scores produced by a specific evaluator (LLM, MRQ, etc.)"""
        results = self.session.query(EvaluationORM).filter(EvaluationORM.evaluator_name == evaluator_name).all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_strategy(self, strategy: str) -> list[dict]:
        """Returns all scores generated using a specific reasoning strategy."""
        results = self.session.query(EvaluationORM).filter(EvaluationORM.strategy == strategy).all()
        return [self._orm_to_dict(r) for r in results]

    def get_all(self, limit: int = 100) -> list[dict]:
        """Returns the most recent scores up to a limit."""
        results = self.session.query(EvaluationORM).order_by(EvaluationORM.created_at.desc()).limit(limit).all()
        return [self._orm_to_dict(r) for r in results]

    def _orm_to_dict(self, row: EvaluationORM) -> dict:
        """Converts an ORM object back to a dictionary format"""
        return {
            "id": row.id,
            "goal_id": row.goal_id,
            "hypothesis_id": row.hypothesis_id,
            "agent_name": row.agent_name,
            "model_name": row.model_name,
            "evaluator_name": row.evaluator_name,
            "scores": (
                row.scores if isinstance(row.scores, dict) else json.loads(row.scores)
            ) if row.scores else {},
            "strategy": row.strategy,
            "reasoning_strategy": row.reasoning_strategy,
            "pipeline_run_id": row.pipeline_run_id,
            "extra_data": (
                row.extra_data if isinstance(row.extra_data, dict) else json.loads(row.extra_data)
            ) if row.extra_data else {},
            "created_at": row.created_at,
        }
    
    def get_rules_for_score(self, score_id: int) -> list[int]:
        links = (
            self.session.query(EvaluationRuleLinkORM.rule_application_id)
            .filter_by(score_id=score_id)
            .all()
        )
        return [rid for (rid,) in links]
    
    def get_by_hypothesis_ids(self, hypothesis_ids: list[int]) -> list[EvaluationORM]:
        if not hypothesis_ids:
            return []
        try:
            return (
                self.session.query(EvaluationORM)
                .filter(EvaluationORM.hypothesis_id.in_(hypothesis_ids))
                .all()
            )
        except Exception as e:
            if self.logger:
                self.logger.log("EvaluationStoreError", {
                    "method": "get_by_hypothesis_ids",
                    "error": str(e),
                    "hypothesis_ids": hypothesis_ids,
                })
            return []---END-OF-FILE---


"co_ai\memory\goal_store.py"
---START-OF-FILE---
# stores/goal_store.py
from datetime import datetime, timezone

from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import Session

from co_ai.models.goal import GoalORM


class GoalStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "goals"
    
    def name(self) -> str:
        return "goals"

    def get_from_text(self, goal_text: str):
        return self.session.query(GoalORM).filter(GoalORM.goal_text == goal_text).first()

    def create(self, goal_dict: dict):
        try:
            new_goal = GoalORM(
                goal_text=goal_dict["goal_text"],
                goal_type=goal_dict.get("goal_type"),
                focus_area=goal_dict.get("focus_area"),
                strategy=goal_dict.get("strategy"),
                llm_suggested_strategy=goal_dict.get("llm_suggested_strategy"),
                source=goal_dict.get("source", "user"),
                created_at=goal_dict.get("created_at") or datetime.now(timezone.utc),
            )
            self.session.add(new_goal)
            self.session.commit()
            self.session.refresh(new_goal)

            if self.logger:
                self.logger.log("GoalCreated", {
                    "goal_id": new_goal.id,
                    "goal_text": new_goal.goal_text[:100],
                    "source": new_goal.source
                })

            return new_goal

        except IntegrityError:
            self.session.rollback()
            return self.get_by_text(goal_dict["goal_text"])

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("GoalCreateFailed", {"error": str(e)})
            raise

    def get_or_create(self, goal_dict: dict):
        """
        Returns existing goal or creates a new one.
        """
        goal_text = goal_dict.get("goal_text")
        if not goal_text:
            raise ValueError("Missing 'goal_text' in input")

        existing = self.get_from_text(goal_text)
        if existing:
            return existing

        return self.create(goal_dict)
---END-OF-FILE---


"co_ai\memory\hypothesis_store.py"
---START-OF-FILE---
# stores/hypothesis_store.py
from difflib import SequenceMatcher
from typing import Optional

import numpy as np
from sqlalchemy import text
from sqlalchemy.orm import Session

from co_ai.models.goal import GoalORM
from co_ai.models.hypothesis import HypothesisORM


class HypothesisStore:
    def __init__(self, session: Session, logger=None, embedding_store=None):
        self.session = session
        self.logger = logger
        self.embedding_store = embedding_store  # Optional embedding model
        self.name = "hypotheses"
    
    def name(self) -> str:
        return "hypotheses"

    def insert(self, hypothesis: HypothesisORM) -> int:
        """
        Inserts a new hypothesis into the database.
        Assumes goal and prompt are already resolved to IDs.
        """
        try:
            self.session.add(hypothesis)
            self.session.flush()  # To get ID before commit

            if self.logger:
                self.logger.log("HypothesisInserted", {
                    "hypothesis_id": hypothesis.id,
                    "goal_id": hypothesis.goal_id,
                    "strategy": hypothesis.strategy,
                    "length": len(hypothesis.text),
                    "timestamp": hypothesis.created_at.isoformat()
                })

            return hypothesis.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("HypothesisInsertFailed", {"error": str(e)})
            raise

    def update_review(self, hyp_id: int, review: str):
        """
        Updates the review field for a hypothesis.
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.review = review
        self.session.commit()

        if self.logger:
            self.logger.log("ReviewStored", {
                "hypothesis_id": hyp_id,
                "review_snippet": (review or "")[:100]
            })

    def update_reflection(self, hyp_id: int, reflection: str):
        """
        Updates the reflection field for a hypothesis.
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.reflection = reflection
        self.session.commit()

        if self.logger:
            self.logger.log("ReflectionStored", {
                "hypothesis_id": hyp_id,
                "reflection_snippet": (reflection or "")[:100]
            })

    def update_elo_rating(self, hyp_id: int, new_rating: float):
        """
        Updates the ELO rating of a hypothesis after pairwise comparison.
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.elo_rating = new_rating
        self.session.commit()

        if self.logger:
            self.logger.log("HypothesisEloUpdated", {
                "hypothesis_id": hyp_id,
                "elo_rating": new_rating
            })

    def soft_delete(self, hyp_id: int):
        """
        Soft-deletes a hypothesis by setting enabled = False
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.enabled = False
        self.session.commit()

        if self.logger:
            self.logger.log("HypothesisSoftDeleted", {"hypothesis_id": hyp_id})

    def get_by_goal(
        self, goal_text: str, limit: int = 10, source=None
    ) -> list[HypothesisORM]:
        """
        Returns all hypotheses for a given goal.
        """
        query = (
            self.session.query(HypothesisORM)
            .join(GoalORM)
            .filter(GoalORM.goal_text == goal_text)
        )

        if source:
            from co_ai.models import EvaluationORM
            query = query.join(EvaluationORM).filter(EvaluationORM.source == source)

        return query.limit(limit).all()

    def get_latest(self, goal_text: str, limit: int = 10) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).join(GoalORM).filter(
            GoalORM.goal_text == goal_text
        ).order_by(HypothesisORM.created_at.desc()).limit(limit).all()

    def get_unreflected(self, goal_text: str, limit: int = 10) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).join(GoalORM).filter(
            GoalORM.goal_text == goal_text,
            HypothesisORM.reflection.is_(None)
        ).limit(limit).all()

    def get_unreviewed(self, goal_text: str, limit: int = 10) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).join(GoalORM).filter(
            GoalORM.goal_text == goal_text,
            HypothesisORM.review.is_(None)
        ).limit(limit).all()

    def get_from_text(self, query: str, threshold: float = 0.95) -> Optional[HypothesisORM]:
        """
        Finds exact or fuzzy match for hypothesis text.
        """
        result = self.session.query(HypothesisORM).filter(HypothesisORM.text == query).first()
        if result:
            return result

        # Fallback to similarity search if needed
        # This requires pg_trgm extension in PostgreSQL
        result = self.session.query(HypothesisORM).filter(
            HypothesisORM.text.ilike(f"%{query}%")
        ).first()

        if result and result.text:
            sim = SequenceMatcher(None, result.text, query).ratio()
            if sim >= threshold:
                return result

        return None

    def get_by_id(self, hyp_id: int) -> Optional[HypothesisORM]:
        return self.session.get(HypothesisORM, hyp_id)

    def get_all(self, limit: int = 100) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).order_by(HypothesisORM.created_at.desc()).limit(limit).all()
    
    def get_similar(self, query: str, limit: int = 3) -> list[str]:
        """
        Get top N hypotheses similar to the given prompt using semantic similarity.

        Args:
            query (str): New hypothesis or idea
            limit (int): Number of similar items to return

        Returns:
            list: Top N similar hypotheses
        """
        try:
            query_embedding = self.embedding_store.get_or_create(query)

            results = []
            with self.embedding_store.conn.cursor() as cur:
                cur.execute(
                    "SELECT text FROM hypotheses ORDER BY embedding <-> %s LIMIT %s",
                    (np.array(query_embedding), limit),
                )
                results = [row[0] for row in cur.fetchall()]

            if self.logger:
                self.logger.log("SimilarHypothesesFound", {
                    "query": query[:100],
                    "matches": [r[:100] for r in results]
                })

            return results

        except Exception as e:
            if self.logger:
                self.logger.log("SimilarHypothesesSearchFailed", {"error": str(e)})
            return []---END-OF-FILE---


"co_ai\memory\idea_store.py"
---START-OF-FILE---
# stores/idea_store.py
from co_ai.memory import BaseStore
from co_ai.models.idea import IdeaORM


class IdeaStore(BaseStore):
    def __init__(self, session, logger):
        super().__init__(session, logger)
        self.name = "ideas"

    def name(self) -> str:
        return "ideas"

    def add_idea(self, idea_data: dict) -> IdeaORM:
        """
        Add a single idea to the database.
        """
        idea = IdeaORM(**idea_data)
        self.db.add(idea)
        self.db.commit()
        self.db.refresh(idea)
        return idea

    def bulk_add_ideas(self, ideas_data: list[dict]) -> list[IdeaORM]:
        """
        Add multiple ideas at once.
        """
        ideas = [IdeaORM(**data) for data in ideas_data]
        self.db.bulk_save_objects(ideas)
        self.db.commit()
        return ideas

    def get_by_goal_id(self, goal_id: int) -> list[IdeaORM]:
        """
        Retrieve all ideas associated with a specific goal.
        """
        return self.db.query(IdeaORM).filter(IdeaORM.goal_id == goal_id).all()

    def get_top_ranked_ideas(self, limit: int = 5) -> list[IdeaORM]:
        """
        Get top-ranked ideas based on score or other criteria.
        (This assumes you have a scoring system stored in extra_data or another table)
        """
        # Example: Filter by novelty + feasibility scores from extra_data
        return self.db.query(IdeaORM).order_by(
            IdeaORM.extra_data["novelty_score"].desc(),
            IdeaORM.extra_data["feasibility_score"].desc()
        ).limit(limit).all()

    def get_by_focus_area_and_strategy(self, focus_area: str, strategy: str) -> list[IdeaORM]:
        """
        Retrieve ideas filtered by domain and strategy.
        """
        return self.db.query(IdeaORM).filter(
            IdeaORM.focus_area == focus_area,
            IdeaORM.strategy == strategy
        ).all()

    def get_by_source(self, source: str) -> list[IdeaORM]:
        """
        Retrieve ideas by their origin (e.g., 'llm', 'survey_agent', 'evolved').
        """
        return self.db.query(IdeaORM).filter(IdeaORM.source == source).all()

    def delete_by_goal_id(self, goal_id: int) -> None:
        """
        Delete all ideas linked to a given goal.
        """
        self.db.query(IdeaORM).filter(IdeaORM.goal_id == goal_id).delete()
        self.db.commit()

    def clear_all(self) -> None:
        """
        Clear all ideas â€” useful for testing.
        """
        self.db.query(IdeaORM).delete()
        self.db.commit()---END-OF-FILE---


"co_ai\memory\lookahead_store.py"
---START-OF-FILE---
# stores/lookahead_store.py
import json
from datetime import datetime, timezone
from typing import List, Optional

from sqlalchemy.orm import Session

from co_ai.models.lookahead import LookaheadORM


class LookaheadStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "lookahead"
    
    def name(self) -> str:
        return "lookahead"

    def insert(self, goal_id: int, result: LookaheadORM):
        """
        Inserts a new lookahead result into the database.
        Assumes goal already exists.
        """
        try:
            # Build ORM object
            db_lookahead = LookaheadORM(
                goal_id=goal_id,
                agent_name=result.agent_name,
                model_name=result.model_name,
                input_pipeline=result.input_pipeline,
                suggested_pipeline=result.suggested_pipeline,
                rationale=result.rationale,
                reflection=result.reflection,
                backup_plans=json.dumps(result.backup_plans) if result.backup_plans else None,
                extra_data=json.dumps(result.extra_data or {}),
                run_id=result.run_id,
                created_at=result.created_at or datetime.now(timezone.utc),
            )

            self.session.add(db_lookahead)
            self.session.flush()  # To get ID immediately

            if self.logger:
                self.logger.log(
                    "LookaheadInserted",
                    {
                        "goal_id": goal_id,
                        "agent": result.agent_name,
                        "model": result.model_name,
                        "pipeline": result.input_pipeline,
                        "suggested_pipeline": result.suggested_pipeline,
                        "rationale_snippet": (result.rationale or "")[:100],
                    },
                )

            return db_lookahead.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("LookaheadInsertFailed", {"error": str(e)})
            raise

    def list_all(self, limit: int = 100) -> List[LookaheadORM]:
        """Returns all stored lookaheads, converted back to dataclass"""
        db_results = self.session.query(LookaheadORM).order_by(LookaheadORM.created_at.desc()).limit(limit).all()
        return [self._orm_to_dataclass(result) for result in db_results]

    def get_by_goal_id(self, goal_id: int) -> List[LookaheadORM]:
        results = (
            self.session.query(LookaheadORM)
            .filter_by(goal_id=goal_id)
            .order_by(LookaheadORM.created_at.desc())
            .all()
        )
        return [self._orm_to_dataclass(r) for r in results]

    def get_by_run_id(self, run_id: str) -> Optional[LookaheadORM]:
        result = self.session.query(LookaheadORM).filter_by(run_id=run_id).first()
        return self._orm_to_dataclass(result) if result else None

    def _orm_to_dataclass(self, row: LookaheadORM) -> LookaheadORM:
        return LookaheadORM(
            goal=row.goal_id,
            agent_name=row.agent_name,
            model_name=row.model_name,
            input_pipeline=row.input_pipeline,
            suggested_pipeline=row.suggested_pipeline,
            rationale=row.rationale,
            reflection=row.reflection,
            backup_plans=json.loads(row.backup_plans) if row.backup_plans else [],
            metadata=json.loads(row.extra_data) if row.extra_data else {},
            run_id=row.run_id,
            created_at=row.created_at,
        )---END-OF-FILE---


"co_ai\memory\memory_tool.py"
---START-OF-FILE---
from typing import Any, Optional

import psycopg2
from pgvector.psycopg2 import register_vector
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import Session, sessionmaker

from co_ai.logs import JSONLogger
from co_ai.memory.context_store import ContextStore
from co_ai.memory.embedding_store import EmbeddingStore
from co_ai.memory.evaluation_store import EvaluationStore
from co_ai.memory.goal_store import GoalStore
from co_ai.memory.hypothesis_store import HypothesisStore
from co_ai.memory.idea_store import IdeaStore
from co_ai.memory.lookahead_store import LookaheadStore
from co_ai.memory.method_plan_store import MethodPlanStore
from co_ai.memory.mrq_store import MRQStore
from co_ai.memory.pattern_store import PatternStatStore
from co_ai.memory.pipeline_run_store import PipelineRunStore
from co_ai.memory.prompt_program_store import PromptProgramStore
from co_ai.memory.prompt_store import PromptStore
from co_ai.memory.reflection_delta_store import ReflectionDeltaStore
from co_ai.memory.rule_application_store import RuleApplicationStore
from co_ai.memory.rule_effect_store import RuleEffectStore
from co_ai.memory.score_store import ScoreStore
from co_ai.memory.search_result_store import SearchResultStore
from co_ai.memory.sharpening_store import SharpeningStore
from co_ai.memory.symbolic_rule_store import SymbolicRuleStore
from co_ai.models.base import engine  # From your SQLAlchemy setup


class MemoryTool:
    def __init__(self, cfg, logger: Optional[JSONLogger] = None):
        self.cfg = cfg
        self.logger = logger
        self._stores = {}  # name -> Store instance

        # Create a new session
        self.session_maker = sessionmaker(bind=engine)
        self.session: Session = self.session_maker()

        # Create connection
        conn = psycopg2.connect(
            dbname=self.cfg.get("db").get("name"),
            user=self.cfg.get("db").get("user"),
            password=self.cfg.get("db").get("password"),
            host=self.cfg.get("db").get("host"),
            port=self.cfg.get("db").get("port"),
        )
        conn.autocommit = True
        register_vector(conn)  # Register pgvector extension

        # Register stores
        self.register_store(GoalStore(self.session, logger))
        embedding_store = EmbeddingStore(self.cfg, conn, self.session, logger)
        self.register_store(embedding_store)
        self.register_store(HypothesisStore(self.session, logger, embedding_store))
        self.register_store(PromptStore(self.session, logger))
        self.register_store(EvaluationStore(self.session, logger))
        self.register_store(PipelineRunStore(self.session, logger))
        self.register_store(LookaheadStore(self.session, logger))
        self.register_store(ContextStore(self.session, logger))
        self.register_store(ReflectionDeltaStore(self.session, logger))
        self.register_store(PatternStatStore(self.session, logger))
        self.register_store(SearchResultStore(self.session, logger))
        self.register_store(IdeaStore(self.session, logger))
        self.register_store(MethodPlanStore(self.session, logger))
        self.register_store(MRQStore(cfg, self.session, logger))
        self.register_store(SharpeningStore(self.session, logger))
        self.register_store(SymbolicRuleStore(self.session, logger))
        self.register_store(RuleEffectStore(self.session, logger))
        self.register_store(RuleApplicationStore(self.session, logger))
        self.register_store(PromptProgramStore(self.session, logger))
        self.register_store(ScoreStore(self.session, logger))


        # Register extra stores if defined in config
        if cfg.get("extra_stores"):
            for store_class in cfg.get("extra_stores", []):
                self.register_store(store_class(self.session, logger))

    def register_store(self, store):
        store_name = getattr(store, "name", store.__class__.__name__)
        if store_name in self._stores:
            raise ValueError(f"A store named '{store_name}' is already registered.")
        self._stores[store_name] = store

        if self.logger:
            self.logger.log("StoreRegistered", {"store": store_name})

    def get(self, name: str) -> Optional[Any]:
        return self._stores.get(name)

    def __getattr__(self, name: str):
        if name in self._stores:
            return self._stores[name]
        raise AttributeError(f"'MemoryTool' has no attribute '{name}'")

    def commit(self):
        """Commit any pending changes"""
        try:
            self.session.commit()
        except SQLAlchemyError as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("SessionRollback", {"error": str(e)})
            raise

    def close(self):
        """Close session at end of run"""
        try:
            self.session.close()
        except SQLAlchemyError as e:
            if self.logger:
                self.logger.log("SessionCloseFailed", {"error": str(e)})
            self.session = self.session_maker()  # Reopen session on failure

    def begin_nested(self):
        """Start nested transaction (for safe rollback during complex ops)"""
        return self.session.begin_nested()

    def refresh_session(self):
        """Closes current session and creates a fresh one"""
        try:
            self.session.rollback()
            self.session.close()
        finally:
            self.session = self.session_maker()
            if self.logger:
                self.logger.log("SessionRefreshed", {"new_session_id": id(self.session)})---END-OF-FILE---


"co_ai\memory\method_plan_store.py"
---START-OF-FILE---
# stores/method_plan_store.py
from sqlalchemy.orm import Session

from co_ai.memory import BaseStore
from co_ai.models.method_plan import MethodPlanORM


class MethodPlanStore(BaseStore):
    def __init__(self, session, logger):
        super().__init__(session, logger)
        self.name = "method_plans"

    def name(self) -> str:
        return "method_plans"

    def add_method_plan(self, plan_data: dict) -> MethodPlanORM:
        """
        Adds a new method plan to the database.

        Args:
            plan_data (dict): Must include all fields from MethodPlanORM

        Returns:
            MethodPlanORM: The saved ORM object
        """
        required_fields = ["idea_text"]
        missing = [f for f in required_fields if plan_data.get(f) is None]

        if missing:
            self.logger.log(
                "MissingRequiredFields",
                {"missing_fields": missing, "raw_input": plan_data},
            )
            raise ValueError(
                f"Cannot save method plan. Missing required fields: {missing}"
            )

        plan = MethodPlanORM(**plan_data)
        self.db.add(plan)
        self.db.commit()
        self.db.refresh(plan)
        return plan

    def get_by_idea_text(self, idea_text: str) -> list[MethodPlanORM]:
        """
        Retrieves all method plans generated from a specific idea

        Args:
            idea_text (str): Text of the idea used to generate this method

        Returns:
            List of MethodPlanORM objects
        """
        return (
            self.db.query(MethodPlanORM)
            .filter(MethodPlanORM.idea_text.ilike(f"%{idea_text}%"))
            .all()
        )

    def get_by_goal_id(self, goal_id: int) -> list[MethodPlanORM]:
        """
        Retrieves all method plans linked to a specific research goal

        Args:
            goal_id (int): GoalORM.id

        Returns:
            List of MethodPlanORM objects
        """
        return (
            self.db.query(MethodPlanORM).filter(MethodPlanORM.goal_id == goal_id).all()
        )

    def get_top_scoring(self, limit: int = 5) -> list[MethodPlanORM]:
        """
        Get top N method plans ranked by composite score

        Returns:
            List of MethodPlanORM objects
        """
        return (
            self.db.query(MethodPlanORM)
            .order_by(
                (
                    MethodPlanORM.score_novelty * 0.3
                    + MethodPlanORM.score_feasibility * 0.2
                    + MethodPlanORM.score_impact * 0.3
                    + MethodPlanORM.score_alignment * 0.2
                ).desc()
            )
            .limit(limit)
            .all()
        )

    def update_method_plan(self, plan_id: int, updates: dict) -> MethodPlanORM:
        """
        Updates an existing method plan with new values

        Args:
            plan_id (int): ID of the plan to update
            updates (dict): Fields to update (score_novelty, code_plan, etc.)

        Returns:
            Updated MethodPlanORM object
        """
        plan = self.db.query(MethodPlanORM).get(plan_id)
        if not plan:
            raise ValueError(f"No method plan found with id {plan_id}")

        for key, value in updates.items():
            setattr(plan, key, value)

        self.db.commit()
        self.db.refresh(plan)
        return plan

    def delete_by_goal_id(self, goal_id: int) -> None:
        """
        Delete all method plans associated with a given goal

        Args:
            goal_id (int): GoalORM.id
        """
        self.db.query(MethodPlanORM).filter(MethodPlanORM.goal_id == goal_id).delete()
        self.db.commit()

    def clear_all(self) -> None:
        """
        Clear all method plans â€” useful for testing
        """
        self.db.query(MethodPlanORM).delete()
        self.db.commit()---END-OF-FILE---


"co_ai\memory\mrq_store.py"
---START-OF-FILE---
# stores/mrq_store.py
import json
from datetime import datetime, timezone

from sqlalchemy import text
from sqlalchemy.orm import Session

from co_ai.models import (MRQMemoryEntryORM, MRQPreferencePairORM,
                          ReflectionDeltaORM)


class MRQStore:
    def __init__(self, cfg: dict, session: Session, logger=None):
        self.db = session
        self.logger = logger
        self.name = "mrq"
        self.cfg = cfg

    def log_evaluations(self):
        return self.cfg.get("log_evaluations", True)

    def add(
        self,
        goal: str,
        strategy: str,
        prompt: str,
        response: str,
        reward: float,
        metadata: dict = None,
    ):
        """
        Adds a new entry to MRQ memory for symbolic learning or training.
        """
        try:
            db_entry = MRQMemoryEntryORM(
                goal=goal,
                strategy=strategy,
                prompt=prompt,
                response=response,
                reward=reward,
                embedding=None,  # optional: compute from prompt/response
                features=None,  # optional: extract features from metadata
                source="manual",
                run_id=metadata.get("run_id") if metadata else None,
                metadata_=json.dumps(metadata or {}),
                created_at=datetime.now(timezone.utc),
            )

            self.db.add(db_entry)
            self.db.flush()  # Get ID before commit

            if self.logger:
                self.logger.log(
                    "MRQMemoryEntryInserted",
                    {
                        "goal_snippet": goal[:100],
                        "prompt_snippet": prompt[:100],
                        "strategy": strategy,
                        "reward": reward,
                        "timestamp": db_entry.created_at.isoformat(),
                    },
                )

            return db_entry.id

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log("MRQMemoryInsertFailed", {"error": str(e)})
            raise

    def get_similar_prompt(self, prompt: str, top_k: int = 5) -> list:
        """
        Gets similar prompts based on text match.
        Future: can use vector similarity instead of trigram search.
        """
        try:
            results = (
                self.db.query(MRQMemoryEntryORM)
                .filter(MRQMemoryEntryORM.prompt.ilike(f"%{prompt}%"))
                .limit(top_k)
                .all()
            )

            return results

        except Exception as e:
            if self.logger:
                self.logger.log("MRQSimilarPromptSearchFailed", {"error": str(e)})
            return []

    def get_by_strategy(self, strategy: str, limit: int = 100) -> list:
        """Returns all entries generated using a specific strategy."""
        return (
            self.db.query(MRQMemoryEntryORM)
            .filter_by(strategy=strategy)
            .limit(limit)
            .all()
        )

    def get_all(self, limit: int = 100) -> list:
        """Returns most recent MRQ memory entries."""
        return (
            self.db.query(MRQMemoryEntryORM)
            .order_by(MRQMemoryEntryORM.created_at.desc())
            .limit(limit)
            .all()
        )

    def train_from_reflection_deltas(self):
        """Train ranker from reflection deltas (symbolic_ranker example)"""
        deltas = self.db.query(ReflectionDeltaORM).all()
        examples = []

        for d in deltas:
            a = d.pipeline_a
            b = d.pipeline_b
            score_a = d.score_a
            score_b = d.score_b

            if not isinstance(a, list) or not isinstance(b, list):
                continue
            if score_a is None or score_b is None:
                continue
            if abs(score_a - score_b) < 0.05:
                continue  # Skip small differences

            label = "b" if score_b > score_a else "a"
            examples.append(
                {
                    "goal_text": d.goal.goal_text,
                    "pipeline_a": a,
                    "pipeline_b": b,
                    "score_a": score_a,
                    "score_b": score_b,
                    "label": label,
                }
            )

        self.training_data = examples
        self.trained_ranker = self.symbolic_ranker()

        if self.logger:
            self.logger.log("MRQTrainingDataLoaded", {"count": len(examples)})

    def symbolic_ranker(self):
        """Simple rule-based ranker used until we train a learned one"""

        def score_pipeline(pipeline: list):
            base_score = len(pipeline) * 0.3
            if "verifier" in pipeline:
                base_score += 1.5
            if "reviewer" in pipeline:
                base_score += 1.2
            if "retriever" in pipeline:
                base_score += 1.0
            if "cot_generator" in pipeline:
                base_score += 0.8
            return base_score

        return score_pipeline

    def get_training_pairs(
        self, goal: str, limit: int = 100, agent_name="generation"
    ) -> list[dict]:
        try:
            sql = text("""
                WITH top_h AS (
                    SELECT DISTINCT ON (p.id)
                        p.id AS prompt_id,
                        g.goal_text AS goal,
                        p.prompt_text,
                        h.text AS output_a,
                        h.elo_rating AS rating_a
                    FROM prompts p
                    JOIN goals g ON p.goal_id = g.id
                    JOIN hypotheses h ON h.prompt_id = p.id
                    WHERE h.enabled = TRUE
                    AND h.goal_id = g.id
                    AND p.agent_name = :agent_name
                    ORDER BY p.id, h.elo_rating DESC
                ),
                bottom_h AS (
                    SELECT DISTINCT ON (p.id)
                        p.id AS prompt_id,
                        h.text AS output_b,
                        h.elo_rating AS rating_b
                    FROM prompts p
                    JOIN hypotheses h ON h.prompt_id = p.id
                    JOIN goals g ON p.goal_id = g.id
                    WHERE h.enabled = TRUE
                    AND h.goal_id = g.id
                    AND p.agent_name = :agent_name
                    ORDER BY p.id, h.elo_rating ASC
                )
                SELECT 
                    top_h.prompt_id,
                    top_h.goal,
                    top_h.prompt_text,
                    top_h.output_a,
                    top_h.rating_a,
                    bottom_h.output_b,
                    bottom_h.rating_b
                FROM top_h
                JOIN bottom_h ON top_h.prompt_id = bottom_h.prompt_id
                WHERE top_h.rating_a != bottom_h.rating_b
                LIMIT :limit;
            """)

            result = self.db.execute(
                sql, {"goal": goal, "agent_name": agent_name, "limit": limit}
            )
            rows = result.fetchall()

            return [
                {
                    "prompt": row[2],
                    "output_a": row[3],
                    "output_b": row[5],
                    "preferred": "a" if row[4] > row[6] else "b",
                    "rating_a": row[4],
                    "rating_b": row[6],
                }
                for row in rows
            ]

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log(
                    "GetMRQTrainingPairsFailed", {"error": str(e), "goal": goal}
                )
            return []

    def add_preference_pair(
        self,
        goal: str,
        prompt: str,
        output_a: str,
        output_b: str,
        preferred: str,
        fmt_a: str,
        fmt_b: str,
        difficulty: str,
        source: str = "arm_dataloader",
        run_id: str = None
    ):
        """
        Save preference pair to database with precomputed embeddings.
        Args:
            goal: Task name or group key (e.g., "arm_dpo")
            prompt: Input question or instruction
            output_a: First response (chosen or rejected)
            output_b: Second response
            preferred: Either "a" or "b"
            prompt_emb: Precomputed embedding of the prompt
            output_a_emb: Precomputed embedding of output_a
            output_b_emb: Precomputed embedding of output_b
        """
        try:
            entry = MRQPreferencePairORM(
                goal=goal,
                prompt=prompt,
                output_a=output_a,
                output_b=output_b,
                preferred=preferred,
                fmt_a=fmt_a,
                fmt_b=fmt_b,
                difficulty=difficulty,
                source=source,
                run_id=run_id,
            )
            self.db.add(entry)
            self.db.commit()
        except Exception as e:
            self.db.rollback()
            raise RuntimeError(f"Failed to save preference pair: {str(e)}")
        finally:
            self.db.close()

    def get_training_preferece_pairs(self, goal: str, limit: int = 1000) -> list[dict]:
        try:
            query = self.db.query(MRQPreferencePairORM).filter(
                MRQPreferencePairORM.goal == goal
            )
            results = query.limit(limit).all()
            return [
                {
                    "prompt": r.prompt,
                    "output_a": r.output_a,
                    "output_b": r.output_b,
                    "preferred": r.preferred,
                    "fmt_a": r.fmt_a,
                    "fmt_b": r.fmt_b,
                }
                for r in results
            ]
        except Exception as e:
            raise RuntimeError(
                f"Failed to load preference pairs for goal '{goal}': {str(e)}"
            )
        finally:
            self.db.close()
---END-OF-FILE---


"co_ai\memory\pattern_store.py"
---START-OF-FILE---
# stores/pattern_stat_store.py
from datetime import datetime

from co_ai.models.pattern_stat import PatternStatORM


class PatternStatStore:
    def __init__(self, session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "pattern_stats"

    def insert(self, stats: list[PatternStatORM]):
        """Insert multiple pattern stats at once"""
        try:
            self.session.bulk_save_objects(stats)
            self.session.commit()

            if self.logger:
                self.logger.log("PatternStatsStored", {
                    "goal_id": stats[0].goal_id,
                    "hypothesis_id": stats[0].hypothesis_id,
                    "agent": stats[0].agent_name,
                    "model": stats[0].model_name,
                    "count": len(stats),
                    "timestamp": datetime.utcnow().isoformat()
                })

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("PatternStatsInsertFailed", {"error": str(e)})
            raise---END-OF-FILE---


"co_ai\memory\pipeline_run_store.py"
---START-OF-FILE---
# stores/pipeline_run_store.py
import json
from datetime import datetime
from typing import Optional

from sqlalchemy.orm import Session

from co_ai.models.pipeline_run import PipelineRunORM


class PipelineRunStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "pipeline_runs"

    def insert(self, run_dict: dict) -> int:
        """
        Inserts a new pipeline run record into the database.
        
        :param run_dict: Dictionary containing fields like run_id, goal_id, pipeline, etc.
        :return: The inserted record's ID
        """
        try:
            # Convert dictionary to ORM object
            db_run = PipelineRunORM(
                goal_id=run_dict.get("goal_id"),
                run_id=run_dict.get("run_id"),
                pipeline=run_dict.get("pipeline"),
                name=run_dict.get("name"),
                tag=run_dict.get("tag"),
                description=run_dict.get("description"),
                strategy=run_dict.get("strategy"),
                model_name=run_dict.get("model_name"),
                run_config=run_dict.get("run_config"),
                lookahead_context=run_dict.get("lookahead_context"),
                symbolic_suggestion=run_dict.get("symbolic_suggestion"),
                extra_data=run_dict.get("extra_data"),
            )

            self.session.add(db_run)
            self.session.flush()  # Get ID before commit
            run_id = db_run.id

            if self.logger:
                self.logger.log("PipelineRunInserted", {
                    "run_id": db_run.run_id,
                    "goal_id": db_run.goal_id,
                    "pipeline": db_run.pipeline,
                    "strategy": db_run.strategy,
                    "model": db_run.model_name,
                    "timestamp": db_run.created_at if db_run.created_at else None
                })

            return run_id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("PipelineRunInsertFailed", {"error": str(e)})
            raise

    def get_by_run_id(self, run_id: str) -> Optional[PipelineRunORM]:
        """
        Fetches a single pipeline run by its unique run_id.
        """
        result = self.session.query(PipelineRunORM).filter(PipelineRunORM.run_id == run_id).first()
        return result

    def get_by_goal_id(self, goal_id: int) -> list[PipelineRunORM]:
        """
        Fetches all pipeline runs associated with a given goal.
        """
        return self.session.query(PipelineRunORM).filter(PipelineRunORM.goal_id == goal_id).all()

    def get_all(self, limit: int = 100) -> list[PipelineRunORM]:
        """
        Returns the most recent pipeline runs up to a limit.
        """
        return self.session.query(PipelineRunORM).order_by(PipelineRunORM.created_at.desc()).limit(limit).all()

    def find(self, filters: dict) -> list[PipelineRunORM]:
        """
        Generic search method for pipeline runs.

        :param filters: Dictionary of filter conditions
        :return: Matching PipelineRun instances
        """
        query = self.session.query(PipelineRunORM)

        if "goal_id" in filters:
            query = query.filter(PipelineRunORM.goal_id == filters["goal_id"])
        if "name" in filters:
            query = query.filter(PipelineRunORM.name == filters["name"])
        if "tag" in filters:
            query = query.filter(PipelineRunORM.tag == filters["tag"])
        if "strategy" in filters:
            query = query.filter(PipelineRunORM.strategy == filters["strategy"])
        if "model_name" in filters:
            query = query.filter(PipelineRunORM.model_name == filters["model_name"])
        if "since" in filters:
            query = query.filter(PipelineRunORM.created_at >= filters["since"])

        return query.order_by(PipelineRunORM.created_at.desc()).all()
---END-OF-FILE---


"co_ai\memory\prompt_program_store.py"
---START-OF-FILE---
from typing import List, Optional

from sqlalchemy.orm import Session

from co_ai.models.prompt_program import \
    PromptProgramORM  # adjust if path differs


class PromptProgramStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "prompt_programs"
        self.table_name = "prompt_programs"

    def add_prompt(self, prompt: PromptProgramORM) -> PromptProgramORM:
        self.session.add(prompt)
        self.session.commit()
        self.session.refresh(prompt)
        return prompt

    def get_by_id(self, prompt_id: str) -> Optional[PromptProgramORM]:
        return self.session.query(PromptProgramORM).filter_by(id=prompt_id).first()

    def get_all_prompts(self) -> List[PromptProgramORM]:
        return self.session.query(PromptProgramORM).order_by(PromptProgramORM.version.desc()).all()

    def get_prompts_for_goal(self, goal_text: str) -> List[PromptProgramORM]:
        return (
            self.session.query(PromptProgramORM)
            .filter(PromptProgramORM.goal == goal_text)
            .order_by(PromptProgramORM.version.desc())
            .all()
        )

    def get_top_prompts(self, goal_text: str, min_score: float = 0.0, top_k: int = 5) -> List[PromptProgramORM]:
        return (
            self.session.query(PromptProgramORM)
            .filter(
                PromptProgramORM.goal == goal_text,
                PromptProgramORM.score >= min_score,
            )
            .order_by(PromptProgramORM.score.desc().nullslast())
            .limit(top_k)
            .all()
        )

    def get_prompt_lineage(self, prompt_id: str) -> List[PromptProgramORM]:
        prompt = self.get_by_id(prompt_id)
        if not prompt:
            return []
        lineage = [prompt]
        while prompt.parent_id:
            prompt = self.get_by_id(prompt.parent_id)
            if prompt:
                lineage.insert(0, prompt)
            else:
                break
        return lineage

    def get_latest_prompt(self, goal_text: str) -> Optional[PromptProgramORM]:
        return (
            self.session.query(PromptProgramORM)
            .filter(PromptProgramORM.goal == goal_text)
            .order_by(PromptProgramORM.version.desc())
            .first()
        )
---END-OF-FILE---


"co_ai\memory\prompt_store.py"
---START-OF-FILE---
# stores/prompt_store.py
import json
import re
from difflib import SequenceMatcher
from typing import Optional

from sqlalchemy import text
from sqlalchemy.dialects.postgresql import dialect
from sqlalchemy.orm import Session

from co_ai.models.goal import GoalORM
from co_ai.models.prompt import PromptORM


class PromptStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "prompt"

    def get_or_create_goal(self, goal_text: str, goal_type: str = None,
                           focus_area: str = None, strategy: str = None,
                           source: str = "user") -> GoalORM:
        """
        Returns existing goal or creates a new one.
        """
        try:
            # Try to find by text
            goal = self.session.query(GoalORM).filter_by(goal_text=goal_text).first()
            if not goal:
                # Create new
                goal = GoalORM(
                    goal_text=goal_text,
                    goal_type=goal_type,
                    focus_area=focus_area,
                    strategy=strategy,
                    llm_suggested_strategy=None,
                    source=source
                )
                self.session.add(goal)
                self.session.flush()  # Get ID before commit

                if self.logger:
                    self.logger.log("GoalCreated", {
                        "goal_id": goal.id,
                        "goal_text": goal_text[:100],
                        "source": source
                    })

            return goal

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("GoalGetOrCreateFailed", {"error": str(e)})
            raise

    def save(self, goal: dict, agent_name: str, prompt_key: str, prompt_text: str,
             response: Optional[str] = None, strategy: str = "default", pipeline_run_id: Optional[int] = None,
             extra_data: dict = None, version: int = 1):
        """
        Saves a prompt to the database and marks it as current for its key/agent.
        """
        try:
            goal_text = goal.get("goal_text", "")
            goal_type=goal.get("goal_type")
            # Get or create the associated goal
            goal_orm = self.get_or_create_goal(goal_text=goal_text, goal_type=goal_type)

            # Deactivate previous versions of this prompt key/agent combo
            self.session.query(PromptORM).filter_by(
                agent_name=agent_name,
                prompt_key=prompt_key
            ).update({"is_current": False})

            # Build ORM object
            db_prompt = PromptORM(
                goal_id=goal_orm.id,
                pipeline_run_id=pipeline_run_id,
                agent_name=agent_name,
                prompt_key=prompt_key,
                prompt_text=prompt_text,
                response_text=response,
                strategy=strategy,
                version=version,
                extra_data=json.dumps(extra_data or {})
            )

            self.session.add(db_prompt)
            self.session.flush()  # Get ID immediately

            if self.logger:
                self.logger.log("PromptStored", {
                    "prompt_id": db_prompt.id,
                    "prompt_key": prompt_key,
                    "goal_id": goal_orm.id,
                    "agent": agent_name,
                    "strategy": strategy,
                    "length": len(prompt_text),
                    "timestamp": db_prompt.timestamp.isoformat()
                })

            return db_prompt.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log(
                    "PromptStoreFailed", {"error": str(e), "prompt_key": prompt_key}
                )
            raise

    def get_from_text(
        self,
        prompt_text: str
    ) -> Optional[PromptORM]:
        """
        Retrieve a prompt from the DB based on its exact prompt_text.
        Optionally filter by agent_name and/or strategy.
        """
        try:
            query = self.session.query(PromptORM).filter(
                PromptORM.prompt_text == prompt_text
            )

            prompt = query.order_by(PromptORM.timestamp.desc()).first()

            if self.logger:
                self.logger.log(
                    "PromptLookup",
                    {
                        "matched": bool(prompt),
                        "text_snippet": prompt_text[:100],
                    },
                )

            return prompt

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log(
                    "PromptLookupFailed",
                    {"error": str(e), "text_snippet": prompt_text[:100]},
                )
            return None

    def get_id_from_response(
        self,
        response_text: str
    ) -> Optional[PromptORM]:
        """
        Retrieve a prompt from the DB based on its exact prompt_text.
        Optionally filter by agent_name and/or strategy.
        """
        try:
            query = self.session.query(PromptORM).filter(
                PromptORM.response_text == response_text
            )

            prompt = query.order_by(PromptORM.timestamp.desc()).first()

            if self.logger:
                self.logger.log(
                    "PromptLookup",
                    {
                        "matched": bool(prompt),
                        "text_snippet": response_text[:100],
                    },
                )

            return prompt.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log(
                    "PromptLookupFailed",
                    {"error": str(e)},
                )
            return None

    def find_matching(self, agent_name, prompt_text, strategy=None):
        query = self.session.query(PromptORM).filter_by(
            agent_name=agent_name, prompt_text=prompt_text
        )
        if strategy:
            query = query.filter_by(strategy=strategy)

        return [p.to_dict() for p in query.limit(10).all()]

    from difflib import SequenceMatcher

    def find_similar_prompt(
        self, agent_name, prompt_text, strategy=None, similarity_threshold=0.7
    ):
        def normalize(text):
            return re.sub(r"\s+", " ", text.strip().lower())

        text_a = normalize(prompt_text or "")
        query = self.session.query(PromptORM).filter_by(agent_name=agent_name)
        if strategy:
            query = query.filter_by(strategy=strategy)

        candidates = query.limit(100).all()
        if not strategy:
            return [p.to_dict() for p in candidates]

        matches = []
        for p in candidates:
            text_b = normalize(p.prompt_text or "")
            if not text_a or not text_b:
                continue

            similarity = SequenceMatcher(None, text_a, text_b).ratio()
            if similarity >= similarity_threshold:
                matches.append((similarity, p))
            elif similarity >= 0.5:
                print(f"âš ï¸ Near miss ({similarity:.2f}): {text_b[:80]}")

        matches.sort(reverse=True, key=lambda x: x[0])
        return [p.to_dict() for similarity, p in matches]

    def get_prompt_training_set(self, goal: str, limit: int = 500) -> list[dict]:
        try:
            sql = text("""
                SELECT 
                    p.id,
                    g.goal_text AS goal,
                    p.prompt_text,
                    p.prompt_key,
                    p.timestamp,
                    h.text AS hypothesis_text,
                    h.elo_rating,
                    h.review
                FROM goals g
                JOIN prompts p ON p.goal_id = g.id
                JOIN hypotheses h ON h.prompt_id = p.id AND h.goal_id = g.id
                WHERE g.goal_text = :goal
                AND h.enabled = TRUE
                ORDER BY p.id, h.elo_rating DESC, h.updated_at DESC
                LIMIT :limit
            """)
            print("\nðŸ” Final SQL Query:")
            print(sql.compile(dialect=dialect(), compile_kwargs={"literal_binds": True}).string)

            result = self.session.execute(sql, {
                'goal': goal,
                'limit': limit
            })

            rows = result.fetchall()
            return [
                {
                    "id": row[0],
                    "goal": row[1],
                    "prompt_text": row[2],
                    "prompt_key": row[3],
                    "timestamp": row[4],
                    "hypothesis_text": row[5],
                    "elo_rating": row[6],
                    "review": row[7],
                }
                for row in rows
            ]

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("GetLatestPromptsFailed", {
                    "error": str(e),
                    "goal": goal
                })
            return []
---END-OF-FILE---


"co_ai\memory\reflection_delta_store.py"
---START-OF-FILE---
from typing import Optional

from co_ai.memory.base import BaseStore
from co_ai.models.reflection_delta import ReflectionDeltaORM


class ReflectionDeltaStore(BaseStore):
    def __init__(self, db, logger=None):
        super().__init__(db, logger)
        self.name = "reflection_deltas"

    def __repr__(self):
        return f"<{self.name} connected={self.db is not None}>"

    def name(self) -> str:
        return "reflection_deltas"

    def insert(self, delta: ReflectionDeltaORM) -> int:
        try:
            self.db.add(delta)
            self.db.flush()
            self.db.commit()

            if self.logger:
                self.logger.log("ReflectionDeltaInserted", {
                    "delta_id": delta.id,
                    "goal_id": delta.goal_id,
                    "run_id_a": delta.run_id_a,
                    "run_id_b": delta.run_id_b,
                    "score_a": delta.score_a,
                    "score_b": delta.score_b,
                    "score_delta": delta.score_delta,
                    "strategy_diff": delta.strategy_diff,
                    "model_diff": delta.model_diff,
                })

            return delta.id

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log("ReflectionDeltaInsertFailed", {"error": str(e)})
            raise

    def get_by_goal_id(self, goal_id: int) -> list[ReflectionDeltaORM]:
        try:
            return self.db.query(ReflectionDeltaORM).filter_by(goal_id=goal_id).order_by(ReflectionDeltaORM.created_at.desc()).all()
        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltasFetchFailed", {"error": str(e)})
            return []

    def get_by_run_ids(self, run_id_a: str, run_id_b: str) -> Optional[ReflectionDeltaORM]:
        try:
            return self.db.query(ReflectionDeltaORM).filter_by(run_id_a=run_id_a, run_id_b=run_id_b).first()
        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltaFetchFailed", {"error": str(e)})
            return None

    def get_all(self, limit: int = 100) -> list[ReflectionDeltaORM]:
        try:
            return self.db.query(ReflectionDeltaORM).order_by(ReflectionDeltaORM.created_at.desc()).limit(limit).all()
        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltasFetchFailed", {"error": str(e)})
            return []

    def find(self, filters: dict) -> list[ReflectionDeltaORM]:
        try:
            query = self.db.query(ReflectionDeltaORM)

            if "goal_id" in filters:
                query = query.filter(ReflectionDeltaORM.goal_id == filters["goal_id"])

            if "run_id_a" in filters and "run_id_b" in filters:
                query = query.filter(
                    ReflectionDeltaORM.run_id_a == filters["run_id_a"],
                    ReflectionDeltaORM.run_id_b == filters["run_id_b"]
                )

            if "score_delta_gt" in filters:
                query = query.filter(ReflectionDeltaORM.score_delta > filters["score_delta_gt"])

            if "strategy_diff" in filters:
                query = query.filter(ReflectionDeltaORM.strategy_diff == filters["strategy_diff"])

            return query.order_by(ReflectionDeltaORM.created_at.desc()).all()

        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltasFetchFailed", {"error": str(e)})
            return []
---END-OF-FILE---


"co_ai\memory\report_logger.py"
---START-OF-FILE---
from co_ai.memory import BaseStore


class ReportLogger(BaseStore):
    def __init__(self, db, logger=None):
        super().__init__(db, logger)
        self.name = "report"

    def __repr__(self):
        return f"<{self.name} connected={self.db is not None}>"

    def name(self) -> str:
        return "report"

    def log(self, run_id, goal, summary, path):
        try:
            with self.db.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO reports (run_id, goal, summary, path)
                    VALUES (%s, %s, %s, %s)
                    """,
                    (run_id, goal, summary, path)
                )
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("ReportLogFailed", {"error": str(e)})---END-OF-FILE---


"co_ai\memory\rule_application_store.py"
---START-OF-FILE---
from typing import List, Optional

from sqlalchemy import and_, desc
from sqlalchemy.orm import Session

from co_ai.models.rule_application import RuleApplicationORM


class RuleApplicationStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "rule_applications"
        self.table_name = "rule_applications"

    def add(self, application: RuleApplicationORM) -> RuleApplicationORM:
        try:
            self.session.add(application)
            self.session.commit()
            self.session.refresh(application)
            if self.logger:
                self.logger.log("RuleApplicationAdded", {"rule_application_id": application.id})
            return application
        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("RuleApplicationAddFailed", {"error": str(e)})
            raise

    def get_by_id(self, application_id: int) -> Optional[RuleApplicationORM]:
        return self.session.query(RuleApplicationORM).get(application_id)

    def get_all(self) -> List[RuleApplicationORM]:
        return self.session.query(RuleApplicationORM).order_by(desc(RuleApplicationORM.created_at)).all()

    def get_by_goal(self, goal_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.goal_id == goal_id)
            .order_by(desc(RuleApplicationORM.created_at))
            .all()
        )

    def get_by_hypothesis(self, hypothesis_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.hypothesis_id == hypothesis_id)
            .order_by(desc(RuleApplicationORM.created_at))
            .all()
        )

    def get_by_pipeline_run(self, pipeline_run_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.pipeline_run_id == pipeline_run_id)
            .order_by(desc(RuleApplicationORM.applied_at))
            .all()
        )

    def get_latest_for_run(self, pipeline_run_id: int) -> Optional[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.pipeline_run_id == pipeline_run_id)
            .order_by(desc(RuleApplicationORM.applied_at))
            .first()
        )

    def get_for_goal_and_hypothesis(self, goal_id: int, hypothesis_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(
                RuleApplicationORM.goal_id == goal_id,
                RuleApplicationORM.hypothesis_id == hypothesis_id,
            )
            .order_by(desc(RuleApplicationORM.applied_at))
            .all()
        )
---END-OF-FILE---


"co_ai\memory\rule_effect_store.py"
---START-OF-FILE---
from datetime import datetime, timezone
from typing import Dict, List, Optional

from sqlalchemy.orm import Session

from co_ai.models.rule_application import RuleApplicationORM


class RuleEffectStore:
    def __init__(self, db: Session, logger=None):
        self.db = db
        self.logger = logger
        self.name = "rule_effects"
        self.table_name = "rule_applications"

    def insert(
        self,
        rule_id: int,
        goal_id: int,
        pipeline_run_id: Optional[int] = None,
        hypothesis_id: Optional[int] = None,
        result_score: Optional[float] = None,
        change_type: Optional[str] = None,
        agent_name: Optional[str] = None,
        notes: Optional[str] = None,
        details: Optional[Dict] = None,
        stage_details: Optional[Dict] = None,
        context_hash: Optional[str] = None,
    ) -> RuleApplicationORM:
        """Insert a new rule application record into the database."""
        try:
            application = RuleApplicationORM(
                rule_id=rule_id,
                goal_id=goal_id,
                pipeline_run_id=pipeline_run_id,
                hypothesis_id=hypothesis_id,
                post_score=result_score,
                change_type=change_type,
                agent_name=agent_name,
                notes=notes,
                details=details,
                stage_details=stage_details,
                context_hash=context_hash,
            )
            self.db.add(application)
            self.db.commit()
            self.db.refresh(application)

            if self.logger:
                self.logger.log("RuleApplicationLogged", application.to_dict())

            return application

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log("RuleApplicationError", {"error": str(e)})
            raise

    def get_by_rule(self, rule_id: int) -> List[RuleApplicationORM]:
        """Retrieve all applications for a given rule."""
        return self.db.query(RuleApplicationORM).filter_by(rule_id=rule_id).all()

    def get_recent(self, limit: int = 50) -> List[RuleApplicationORM]:
        """Get the most recent rule applications."""
        return (
            self.db.query(RuleApplicationORM)
            .order_by(RuleApplicationORM.created_at.desc())
            .limit(limit)
            .all()
        )

    def get_feedback_summary(self, rule_id: int) -> Dict[str, int]:
        """Return a count of feedback labels for a specific rule."""
        results = (
            self.db.query(RuleApplicationORM.result_label)
            .filter(RuleApplicationORM.rule_id == rule_id)
            .all()
        )
        summary = {}
        for (label,) in results:
            if label:
                summary[label] = summary.get(label, 0) + 1
        return summary


    def get_by_run_and_goal(self, run_id: int, goal_id: int) -> List[RuleApplicationORM]:
        """
        Retrieve all rule applications for a specific pipeline run and goal.

        Args:
            run_id (int): The ID of the pipeline run.
            goal_id (int): The ID of the goal.

        Returns:
            List[RuleApplicationORM]: Matching rule applications.
        """
        if not run_id or not goal_id:
            if self.logger:
                self.logger.log("InvalidInputForRuleFetch", {
                    "reason": "Missing run_id or goal_id",
                    "run_id": run_id,
                    "goal_id": goal_id
                })
            return []

        try:
            applications = (
                self.db.query(RuleApplicationORM)
                .filter(
                    RuleApplicationORM.pipeline_run_id == int(run_id),
                    RuleApplicationORM.goal_id == int(goal_id)
                )
                .all()
            )

            if self.logger and len(applications) > 0:
                self.logger.log("RuleApplicationsFetched", {
                    "run_id": run_id,
                    "goal_id": goal_id,
                    "count": len(applications)
                })

            return applications

        except Exception as e:
            if self.logger:
                self.logger.log("RuleApplicationFetchError", {
                    "error": str(e),
                    "run_id": run_id,
                    "goal_id": goal_id
                })
            return []---END-OF-FILE---


"co_ai\memory\score_store.py"
---START-OF-FILE---
from typing import List, Optional

from sqlalchemy.orm import Session

from co_ai.models.score import ScoreORM


class ScoreStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "scores"
        self.table_name = "scores"

    def add_score(self, score: ScoreORM) -> ScoreORM:
        self.session.add(score)
        self.session.commit()
        self.session.refresh(score)
        return score

    def add_scores_bulk(self, scores: List[ScoreORM]):
        self.session.add_all(scores)
        self.session.commit()

    def get_scores_for_evaluation(self, evaluation_id: int) -> List[ScoreORM]:
        return (
            self.session.query(ScoreORM)
            .filter_by(evaluation_id=evaluation_id)
            .order_by(ScoreORM.dimension.asc())
            .all()
        )

    def get_scores_for_hypothesis(self, hypothesis_id: int) -> List[ScoreORM]:
        return (
            self.session.query(ScoreORM)
            .filter(ScoreORM.evaluation.has(hypothesis_id=hypothesis_id))
            .order_by(ScoreORM.dimension.asc())
            .all()
        )

    def get_scores_by_dimension(self, dimension: str, top_k: int = 100) -> List[ScoreORM]:
        return (
            self.session.query(ScoreORM)
            .filter_by(dimension=dimension)
            .order_by(ScoreORM.score.desc().nullslast())
            .limit(top_k)
            .all()
        )

    def delete_scores_for_evaluation(self, evaluation_id: int):
        self.session.query(ScoreORM).filter_by(evaluation_id=evaluation_id).delete()
        self.session.commit()

    def get_all(self, limit: Optional[int] = None) -> List[ScoreORM]:
        query = self.session.query(ScoreORM).order_by(ScoreORM.id.desc())
        if limit:
            query = query.limit(limit)
        return query.all()

    def get_by_id(self, score_id: int) -> Optional[ScoreORM]:
        return self.session.query(ScoreORM).filter_by(id=score_id).first()

    def get_by_evaluation_ids(self, evaluation_ids: list[int]) -> list[ScoreORM]:
        if not evaluation_ids:
            return []
        try:
            return (
                self.session.query(ScoreORM)
                .filter(ScoreORM.evaluation_id.in_(evaluation_ids))
                .all()
            )
        except Exception as e:
            if self.logger:
                self.logger.log("ScoreStoreError", {
                    "method": "get_by_evaluation_ids",
                    "error": str(e),
                    "evaluation_ids": evaluation_ids,
                })
            return []---END-OF-FILE---


"co_ai\memory\search_result_store.py"
---START-OF-FILE---
# stores/search_result_store.py
from datetime import datetime
from typing import Dict, List, Optional

from co_ai.memory import BaseStore
from co_ai.models.search_result import SearchResultORM


class SearchResultStore(BaseStore):
    def __init__(self, session, logger):
        super().__init__(session, logger)
        self.name = "search_results"

    def name(self) -> str:
        return "search_results"

    def add_result(
        self,
        *,
        query: str,
        source: str,
        result_type: str,
        title: str,
        summary: str,
        url: str,
        author: Optional[str] = None,
        published_at: Optional[datetime] = None,
        tags: Optional[List[str]] = None,
        goal_id: Optional[int] = None,
        parent_goal: Optional[str] = None,
        strategy: Optional[str] = None,
        focus_area: Optional[str] = None,
        extra_data: Optional[Dict] = None
    ) -> SearchResultORM:
        """
        Add a single search result to the database.
        """
        result = SearchResultORM(
            query=query,
            source=source,
            result_type=result_type,
            title=title,
            summary=summary,
            url=url,
            author=author,
            published_at=published_at,
            tags=tags,
            goal_id=goal_id,
            parent_goal=parent_goal,
            strategy=strategy,
            focus_area=focus_area,
            extra_data=extra_data
        )
        self.session.add(result)
        self.session.commit()
        self.session.refresh(result)
        return result

    def bulk_add_results(self, results: List[Dict]) -> List[SearchResultORM]:
        """
        Add multiple search results at once.
        Each dict should contain the required fields.
        """
        orm_objects = [SearchResultORM(**result) for result in results]
        self.db.bulk_save_objects(orm_objects)
        self.db.commit()
        return orm_objects

    def get_by_goal_id(self, goal_id: int) -> List[SearchResultORM]:
        """
        Retrieve all search results associated with a specific goal.
        """
        return self.db.query(SearchResultORM).filter(
            SearchResultORM.goal_id == goal_id
        ).all()

    def get_by_strategy_and_focus(self, strategy: str, focus_area: str) -> List[SearchResultORM]:
        """
        Get results filtered by strategy and focus area.
        """
        return self.db.query(SearchResultORM).filter(
            SearchResultORM.strategy == strategy,
            SearchResultORM.focus_area == focus_area
        ).all()

    def get_by_source_and_type(self, source: str, result_type: str) -> List[SearchResultORM]:
        """
        Get results filtered by source and type (e.g., arxiv/paper).
        """
        return self.db.query(SearchResultORM).filter(
            SearchResultORM.source == source,
            SearchResultORM.result_type == result_type
        ).all()

    def delete_by_goal_id(self, goal_id: int) -> None:
        """
        Delete all search results linked to a given goal.
        """
        self.db.query(SearchResultORM).filter(
            SearchResultORM.goal_id == goal_id
        ).delete()
        self.db.commit()

    def clear_all(self) -> None:
        """
        Delete all records â€” useful for testing.
        """
        self.db.query(SearchResultORM).delete()
        self.db.commit()---END-OF-FILE---


"co_ai\memory\sharpening_store.py"
---START-OF-FILE---
# stores/sharpening_store.py
from sqlalchemy.orm import Session

from co_ai.models.sharpening_prediction import SharpeningPredictionORM


class SharpeningStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "sharpening"

    def insert_sharpening_prediction(self, prediction_dict: dict, goal: dict):
        """
        Inserts a new sharpening comparison from A/B hypothesis testing
        """
        prediction = SharpeningPredictionORM(**prediction_dict)
        prediction.goal_id = goal.get("id")  # Ensure correct goal linkage

        self.session.add(prediction)
        self.session.commit()
        self.session.refresh(prediction)

        return prediction.id---END-OF-FILE---


"co_ai\memory\symbolic_rule_store.py"
---START-OF-FILE---
from typing import List

import yaml
from sqlalchemy import and_, or_
from sqlalchemy.orm import Session

from co_ai.constants import PIPELINE
from co_ai.models.evaluation import EvaluationORM
from co_ai.models.symbolic_rule import SymbolicRuleORM


class SymbolicRuleStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "symbolic_rules"
        self.table_name = "symbolic_rules"

    def insert(self, rule: SymbolicRuleORM):
        self.session.add(rule)
        self.session.commit()
        self.session.refresh(rule)
        return rule

    def get_all_rules(self) -> List[SymbolicRuleORM]:
        return self.session.query(SymbolicRuleORM).all()

    def get_rules_for_goal(self, goal) -> List[SymbolicRuleORM]:
        return (
            self.session.query(SymbolicRuleORM)
            .filter(SymbolicRuleORM.goal_id == goal.id)
            .all()
        )

    def get_applicable_rules(
        self, goal: dict, pipeline_run_id: int = None, config: dict = {}
    ):
        match_priority = config.get(
            "match_priority", ["goal_id", "pipeline_run_id", "metadata"]
        )
        metadata_mode = config.get("metadata_match_mode", "partial")
        allow_fallback = config.get("allow_fallback", True)

        filters = []

        if "goal_id" in match_priority and goal.get("id"):
            filters.append(SymbolicRuleORM.goal_id == goal["id"])

        if "pipeline_run_id" in match_priority and pipeline_run_id:
            filters.append(SymbolicRuleORM.pipeline_run_id == pipeline_run_id)

        if "metadata" in match_priority and allow_fallback:
            goal_type = goal.get("goal_type")
            goal_category = goal.get("goal_category")
            difficulty = goal.get("difficulty")

            if metadata_mode == "exact":
                filters.append(
                    and_(
                        SymbolicRuleORM.goal_type == goal_type,
                        SymbolicRuleORM.goal_category == goal_category,
                        SymbolicRuleORM.difficulty == difficulty,
                    )
                )
            elif metadata_mode == "partial":
                filters += [
                    and_(
                        SymbolicRuleORM.goal_type == goal_type,
                        SymbolicRuleORM.goal_category == None,
                        SymbolicRuleORM.difficulty == None,
                    ),
                    and_(
                        SymbolicRuleORM.goal_category == goal_category,
                        SymbolicRuleORM.goal_type == None,
                        SymbolicRuleORM.difficulty == None,
                    ),
                    and_(
                        SymbolicRuleORM.difficulty == difficulty,
                        SymbolicRuleORM.goal_type == None,
                        SymbolicRuleORM.goal_category == None,
                    ),
                ]

        if not filters:
            return []

        query = self.session.query(SymbolicRuleORM).filter(or_(*filters))
        return query.all()

    def find_matching_rules(self, goal) -> List[SymbolicRuleORM]:
        return (
            self.session.query(SymbolicRuleORM)
            .filter(
                SymbolicRuleORM.goal_id == goal.id
                # Add logic here if you want partial matches by goal_type, etc.
            )
            .order_by(SymbolicRuleORM.score.desc().nullslast())
            .all()
        )

    def update_rule_score(self, rule_id: int):
        scores = (
            self.session.query(EvaluationORM.score)
            .filter(EvaluationORM.symbolic_rule_id == rule_id)
            .all()
        )
        scores = [s[0] for s in scores if s[0] is not None]

        if scores:
            avg_score = sum(scores) / len(scores)
            rule = self.session.query(SymbolicRuleORM).get(rule_id)
            rule.score = avg_score
            self.session.commit()
            return avg_score
        return None

    def get_top_rules(self, top_k=10) -> List[SymbolicRuleORM]:
        return (
            self.session.query(SymbolicRuleORM)
            .order_by(SymbolicRuleORM.score.desc().nullslast())
            .limit(top_k)
            .all()
        )

    def track_pipeline_stage(self, stage_dict: dict, context: dict):
        # Only create if not already exists
        goal = context.get("goal")
        goal_id = goal.get("id")
        pipeline_run_id = context.get("pipeline_run_id")
        agent_name = stage_dict.get("name", "default_agent")
        rule_filter = {"agent_name": agent_name, "goal_id": goal_id}
        context_hash = SymbolicRuleORM.compute_context_hash(stage_dict, rule_filter)
        existing = (
            self.session.query(SymbolicRuleORM)
            .filter_by(
                goal_id=goal_id, context_hash=context_hash, agent_name=agent_name
            )
            .first()
        )

        if not existing:
            rule = SymbolicRuleORM(
                goal_id=goal_id,
                pipeline_run_id=pipeline_run_id,
                agent_name=agent_name,
                target="pipeline",
                attributes=stage_dict,
                filter=rule_filter,
                goal_type=goal.get("goal_type"),
                goal_category=goal.get("goal_category") or goal.get("focus_area"),
                difficulty=goal.get("difficulty"),
                context_hash=context_hash,
                source="pipeline_stage",
            )
            self.insert(rule)

    def load_from_yaml(self, path: str):
        with open(path, "r") as f:
            data = yaml.safe_load(f)

        for rule in data.get("rules", []):
            exists = (
                self.session.query(SymbolicRuleORM)
                .filter_by(
                    goal_type=rule.get("goal_type"),
                    agent_name=rule.get("agent_name"),
                    rule_text=rule.get("rule_text"),
                )
                .first()
            )
            if not exists:
                new_rule = SymbolicRuleORM(**rule)
                self.session.add(new_rule)
        self.session.commit()

    def get_all(self) -> list[SymbolicRuleORM]:
        try:
            rules = self.session.query(SymbolicRuleORM).order_by(SymbolicRuleORM.created_at.desc()).all()
            if self.logger:
                self.logger.log("SymbolicRulesFetched", {"count": len(rules)})
            return rules
        except Exception as e:
            if self.logger:
                self.logger.log("SymbolicRulesFetchError", {"error": str(e)})
            return []

    def get_by_id(self, rule_id: int):
        try:
            return self.session.query(SymbolicRuleORM).filter_by(id=rule_id).first()
        except Exception as e:
            if self.logger:
                self.logger.log("SymbolicRuleGetByIdError", {"rule_id": rule_id, "error": str(e)})
            return None
    
    def exists_by_signature(self, signature: str) -> bool:
        """
        Checks if a symbolic rule exists by its signature hash.
        """
        return self.session.query(SymbolicRuleORM).filter_by(context_hash=signature).first() is not None
---END-OF-FILE---


"co_ai\models\__init__.py"
---START-OF-FILE---
from .context_state import ContextStateORM
from .evaluation import EvaluationORM
from .evaluation_rule_link import EvaluationRuleLinkORM
from .goal import GoalORM
from .hypothesis import HypothesisORM
from .idea import IdeaORM
from .lookahead import LookaheadORM
from .method_plan import MethodPlanORM
from .mrq_memory_entry import MRQMemoryEntryORM
from .mrq_preference_pair import MRQPreferencePairORM
from .node_orm import NodeORM
from .pattern_stat import PatternStatORM
from .pipeline_run import PipelineRunORM
from .prompt import PromptORM
from .reflection_delta import ReflectionDeltaORM
from .rule_application import RuleApplicationORM
from .score import ScoreORM
from .score_dimension import ScoreDimensionORM
from .search_result import SearchResultORM
from .sharpening_prediction import SharpeningPredictionORM
from .sharpening_result import SharpeningResultORM
from .symbolic_rule import SymbolicRuleORM
from .unified_mrq import UnifiedMRQModelORM
from .comparison_preference import ComparisonPreferenceORM
---END-OF-FILE---


"co_ai\models\base.py"
---START-OF-FILE---
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base, sessionmaker

# Replace with your actual DB URL
engine = create_engine("postgresql://co:co@localhost:5432/co")
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()---END-OF-FILE---


"co_ai\models\comparison_preference.py"
---START-OF-FILE---

# models/comparison_preference.py
from datetime import datetime

from sqlalchemy import (JSON, Column, DateTime, Float, Integer,
                        String, Text)

from co_ai.models.base import Base


class ComparisonPreferenceORM(Base):
    __tablename__ = "comparison_preferences"
    id = Column(Integer, primary_key=True)
    goal_id = Column(Integer)
    preferred_tag = Column(String)
    rejected_tag = Column(String)
    preferred_run_id = Column(Integer)
    rejected_run_id = Column(Integer)
    preferred_score = Column(Float)
    rejected_score = Column(Float)
    dimension_scores = Column(JSON)
    reason = Column(Text)
    source = Column(String, default="pipeline_comparison")
    created_at = Column(DateTime, default=datetime.utcnow)
---END-OF-FILE---


"co_ai\models\context_state.py"
---START-OF-FILE---
# models/context_state.py
from datetime import datetime, timezone

from sqlalchemy import JSON, Boolean, Column, DateTime, Integer, String, Text

from co_ai.models.base import Base


class ContextStateORM(Base):
    __tablename__ = "context_states"

    id = Column(Integer, primary_key=True)
    run_id = Column(String, nullable=False)
    stage_name = Column(String, nullable=False)
    version = Column(Integer, nullable=False)
    is_current = Column(Boolean, default=True)
    context = Column(JSON, nullable=False)  # Stored as JSONB or TEXT
    preferences = Column(JSON)
    extra_data = Column(JSON)
    timestamp = Column(DateTime, default=lambda: datetime.now(timezone.utc))---END-OF-FILE---


"co_ai\models\document.py"
---START-OF-FILE---
# co_ai/models/document.py

from sqlalchemy import Column, Integer, String, Text, DateTime
from sqlalchemy.sql import func
from co_ai.models.base import Base

class DocumentORM(Base):
    __tablename__ = "documents"

    id = Column(Integer, primary_key=True)
    title = Column(String, nullable=False)
    source = Column(String, nullable=False)         # e.g., "arxiv", "huggingface", "github"
    external_id = Column(String, nullable=True)     # e.g., "2505.19590" (arXiv ID)
    url = Column(String, nullable=True)             # full paper URL
    content = Column(Text, nullable=True)           # full extracted paper text
    date_added = Column(DateTime(timezone=True), server_default=func.now())
---END-OF-FILE---


"co_ai\models\evaluation_rule_link.py"
---START-OF-FILE---
from sqlalchemy import Column, DateTime, ForeignKey, Integer, func

from co_ai.models.base import Base


class EvaluationRuleLinkORM(Base):
    __tablename__ = "evaluation_rule_links"

    id = Column(Integer, primary_key=True)
    evaluation_id = Column(Integer, ForeignKey("evaluations.id", ondelete="CASCADE"), nullable=False)
    rule_application_id = Column(Integer, ForeignKey("rule_applications.id", ondelete="CASCADE"), nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
---END-OF-FILE---


"co_ai\models\evaluation.py"
---START-OF-FILE---
# models/score.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Column, DateTime, Float, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class EvaluationORM(Base):
    __tablename__ = "evaluations"

    id = Column(Integer, primary_key=True)
    goal_id = Column(Integer, ForeignKey("goals.id"))
    hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"))
    symbolic_rule_id = Column(Integer, ForeignKey("symbolic_rules.id"), nullable=True)
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id"), nullable=True)
    agent_name = Column(String, nullable=False)
    model_name = Column(String, nullable=False)
    evaluator_name = Column(String, nullable=False)
    strategy = Column(String)
    reasoning_strategy = Column(String)
    
    scores = Column(JSON, default={})
    
    extra_data = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="scores")
    hypothesis = relationship("HypothesisORM", back_populates="scores")
    symbolic_rule = relationship("SymbolicRuleORM", back_populates="scores")
    pipeline_run = relationship("PipelineRunORM", back_populates="scores")
    dimension_scores = relationship("ScoreORM", back_populates="evaluation", cascade="all, delete-orphan")

    def to_dict(self, include_relationships: bool = False) -> dict:
        data = {
            "id": self.id,
            "goal_id": self.goal_id,
            "hypothesis_id": self.hypothesis_id,
            "symbolic_rule_id": self.symbolic_rule_id,
            "pipeline_run_id": self.pipeline_run_id,
            "agent_name": self.agent_name,
            "model_name": self.model_name,
            "evaluator_name": self.evaluator_name,
            "strategy": self.strategy,
            "reasoning_strategy": self.reasoning_strategy,
            "scores": self.scores,
            "extra_data": self.extra_data,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }

        if include_relationships:
            data["goal"] = (
                self.goal.to_dict()
                if self.goal and hasattr(self.goal, "to_dict")
                else None
            )
            data["hypothesis"] = (
                self.hypothesis.to_dict()
                if self.hypothesis and hasattr(self.hypothesis, "to_dict")
                else None
            )

        return data
---END-OF-FILE---


"co_ai\models\goal.py"
---START-OF-FILE---
# models/goal.py
from datetime import datetime

from sqlalchemy import Column, DateTime, Integer, String
from sqlalchemy.orm import relationship

from .base import Base


class GoalORM(Base):
    __tablename__ = "goals"

    id = Column(Integer, primary_key=True)
    goal_text = Column(String, nullable=False)
    goal_type = Column(String)
    focus_area = Column(String)
    strategy = Column(String)
    llm_suggested_strategy = Column(String)
    source = Column(String, default="user")
    difficulty = Column(String, default="medium")
    goal_category = Column(String, default="analyze")

    created_at = Column(DateTime, default=datetime.utcnow)

    prompts = relationship("PromptORM", back_populates="goal")
    hypotheses = relationship("HypothesisORM", back_populates="goal")
    pipeline_runs = relationship("PipelineRunORM", back_populates="goal")
    scores = relationship("EvaluationORM", back_populates="goal")
    lookaheads = relationship("LookaheadORM", back_populates="goal")
    reflection_deltas = relationship("ReflectionDeltaORM", back_populates="goal")
    ideas = relationship("IdeaORM", back_populates="goal")
    method_plans = relationship("MethodPlanORM", back_populates="goal")
    sharpening_predictions = relationship(
        "SharpeningPredictionORM", back_populates="goal"
    )
    symbolic_rules = relationship("SymbolicRuleORM", back_populates="goal")

    rule_applications = relationship(
        "RuleApplicationORM", back_populates="goal", cascade="all, delete-orphan"
    )

    def __repr__(self):
        return f"<GoalORM(id={self.id}, goal_text='{self.goal_text[:50]}')>"

    def to_dict(self):
        return {
            "id": self.id,
            "goal_text": self.goal_text,
            "goal_type": self.goal_type,
            "focus_area": self.focus_area,
            "strategy": self.strategy,
            "llm_suggested_strategy": self.llm_suggested_strategy,
            "source": self.source,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
---END-OF-FILE---


"co_ai\models\hypothesis.py"
---START-OF-FILE---
# models/hypothesis.py
from datetime import datetime

from sqlalchemy import (JSON, Boolean, Column, DateTime, Float, ForeignKey,
                        Integer, String)
from sqlalchemy.orm import relationship

from .base import Base


class HypothesisORM(Base):
    __tablename__ = "hypotheses"

    id = Column(Integer, primary_key=True)
    text = Column(String, nullable=False)
    goal_id = Column(Integer, ForeignKey("goals.id"))
    prompt_id = Column(Integer, ForeignKey("prompts.id"))
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id"))
    source_hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"))
    strategy = Column(String)
    confidence = Column(Float, default=0.0)
    review = Column(String)
    reflection = Column(String)
    elo_rating = Column(Float, default=750.0)
    embedding = Column(JSON)  # Use pgvector later for better support
    features = Column(JSON)   # For structured metadata
    source = Column(String)
    pipeline_signature = Column(String)
    enabled = Column(Boolean, default=True)
    version = Column(Integer, default=1)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationships
    goal = relationship("GoalORM", back_populates="hypotheses")
    prompt = relationship("PromptORM", back_populates="hypotheses")
    source_hypothesis = relationship("HypothesisORM", remote_side=[id])
    scores = relationship("EvaluationORM", back_populates="hypothesis")
    pipeline_run = relationship("PipelineRunORM", back_populates="hypotheses")

    rule_applications = relationship(
        "RuleApplicationORM",
        back_populates="hypothesis",
        cascade="all, delete-orphan",
    )

    def to_dict(self, include_related=False):
        return {
            "id": self.id,
            "text": self.text,
            "goal_id": self.goal_id,
            "prompt_id": self.prompt_id,
            "strategy": self.strategy,
            "confidence": self.confidence,
            "review": self.review,
            "reflection": self.reflection,
            "elo_rating": self.elo_rating,
            "embedding": self.embedding,
            "features": self.features,
            "source_hypothesis_id": self.source_hypothesis_id,
            "source": self.source,
            "pipeline_signature": self.pipeline_signature,
            "pipeline_id": self.pipeline_run_id,
            "enabled": self.enabled,
            "version": self.version,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
            # Optional related objects
            "goal": self.goal.to_dict() if include_related and self.goal else None,
            "prompt": self.prompt.to_dict()
            if include_related and self.prompt
            else None,
            "scores": [s.to_dict() for s in self.scores]
            if include_related and self.scores
            else None,
        }

    @staticmethod
    def from_dict(data: dict):
        return HypothesisORM(
            id=data.get("id"),
            text=data.get("text", ""),
            goal_id=data.get("goal_id"),
            prompt_id=data.get("prompt_id"),
            strategy=data.get("strategy"),
            confidence=data.get("confidence", 0.0),
            review=data.get("review"),
            reflection=data.get("reflection"),
            elo_rating=data.get("elo_rating", 750.0),
            embedding=data.get("embedding"),
            features=data.get("features"),
            source_hypothesis_id=data.get("source_hypothesis_id"),
            source=data.get("source"),
            pipeline_signature=data.get("pipeline_signature"),
            pipeline_id=data.get("pipeline_id"),
            enabled=data.get("enabled", True),
            version=data.get("version", 1),
            created_at=data.get("created_at"),
            updated_at=data.get("updated_at"),
        )
---END-OF-FILE---


"co_ai\models\idea.py"
---START-OF-FILE---
# models/idea.py
from datetime import datetime

from sqlalchemy import JSON, Column, DateTime, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from .base import Base


class IdeaORM(Base):
    __tablename__ = "ideas"

    id = Column(Integer, primary_key=True, autoincrement=True)
    idea_text = Column(String, nullable=False)
    parent_goal = Column(String)
    focus_area = Column(String)
    strategy = Column(String)
    source = Column(String)
    origin = Column(String)
    extra_data = Column(JSON)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationship to GoalORM
    goal = relationship("GoalORM", back_populates="ideas")

    def to_dict(self):
        return {
            "id": self.id,
            "idea_text": self.idea_text,
            "parent_goal": self.parent_goal,
            "focus_area": self.focus_area,
            "strategy": self.strategy,
            "source": self.source,
            "origin": self.origin,
            "extra_data": self.extra_data or {},  # Avoid NoneType issues
            "goal_id": self.goal_id,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }---END-OF-FILE---


"co_ai\models\lookahead.py"
---START-OF-FILE---
# models/lookahead.py
from datetime import datetime

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class LookaheadORM(Base):
    __tablename__ = "lookaheads"

    id = Column(Integer, primary_key=True)

    # Goal info
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)

    # Agent metadata
    agent_name = Column(String, nullable=False)
    model_name = Column(String, nullable=False)

    # Pipeline information
    input_pipeline = Column(String)  # Store as JSON string if needed
    suggested_pipeline = Column(String)  # Same here
    rationale = Column(Text)
    reflection = Column(Text)
    backup_plans = Column(Text)  # List[str] stored as JSON or newline-separated
    extra_data = Column("metadata", Text)  # Renamed to avoid conflict with SQLAlchemy
    run_id = Column(String)

    created_at = Column(DateTime, default=datetime.utcnow)

    goal = relationship("GoalORM", back_populates="lookaheads")
---END-OF-FILE---


"co_ai\models\method_plan.py"
---START-OF-FILE---
# models/method_plan.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Boolean, Column, DateTime, Float, ForeignKey,
                        Integer, String)
from sqlalchemy.orm import relationship

from .base import Base


class MethodPlanORM(Base):
    __tablename__ = "method_plans"

    id = Column(Integer, primary_key=True)

    # Idea source
    idea_text = Column(String, nullable=False)
    idea_id = Column(Integer, ForeignKey("ideas.id"), nullable=True)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)

    # Research design fields
    research_objective = Column(String, nullable=False, default="research")
    key_components = Column(JSON)  # List of technical components or modules
    experimental_plan = Column(String)
    hypothesis_mapping = Column(JSON)  # e.g., {"H1": "handled by introspection module"}
    search_strategy = Column(String)
    knowledge_gaps = Column(String)
    next_steps = Column(String)

    # Supporting metadata
    task_description = Column(String)
    baseline_method = Column(String)
    literature_summary = Column(String)
    code_plan = Column(String)  # Optional pseudocode or starter code
    focus_area = Column(String)  # e.g., chemistry, nlp, cv, meta_learning
    strategy = Column(String)  # e.g., graph_attention_with_positional_embeddings

    # Scoring system
    score_novelty = Column(Float)
    score_feasibility = Column(Float)
    score_impact = Column(Float)
    score_alignment = Column(Float)

    # Evolution tracking
    evolution_level = Column(Integer, default=0)
    parent_plan_id = Column(Integer, ForeignKey("method_plans.id"), nullable=True)
    is_refinement = Column(Boolean, default=False)

    # Timestamps
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    # Relationships
    goal = relationship("GoalORM", back_populates="method_plans")
    parent_plan = relationship("MethodPlanORM", remote_side=[id], backref="refinements")

    def to_dict(self, include_relationships: bool = False) -> dict:
        result = {
            "id": self.id,
            "idea_text": self.idea_text,
            "idea_id": self.idea_id,
            "goal_id": self.goal_id,
            "research_objective": self.research_objective,
            "key_components": self.key_components,
            "experimental_plan": self.experimental_plan,
            "hypothesis_mapping": self.hypothesis_mapping,
            "search_strategy": self.search_strategy,
            "knowledge_gaps": self.knowledge_gaps,
            "next_steps": self.next_steps,
            "baseline_method": self.baseline_method,
            "literature_summary": self.literature_summary,
            "code_plan": self.code_plan,
            "focus_area": self.focus_area,
            "strategy": self.strategy,
            "score_novelty": self.score_novelty,
            "score_feasibility": self.score_feasibility,
            "score_impact": self.score_impact,
            "score_alignment": self.score_alignment,
            "evolution_level": self.evolution_level,
            "parent_plan_id": self.parent_plan_id,
            "is_refinement": self.is_refinement,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }

        if include_relationships and self.goal:
            result["goal"] = self.goal.to_dict()
            result["refinements"] = [r.to_dict() for r in self.refinements]

        return result---END-OF-FILE---


"co_ai\models\mrq_memory_entry.py"
---START-OF-FILE---
# models/mrq_memory_entry.py
from datetime import datetime, timezone

from sqlalchemy import JSON, Column, DateTime, Float, Integer, String, Text
from sqlalchemy.dialects.postgresql import ARRAY, REAL

from co_ai.models.base import Base


class MRQMemoryEntryORM(Base):
    __tablename__ = "mrq_memory"

    id = Column(Integer, primary_key=True)
    goal = Column(Text, nullable=False)
    strategy = Column(String, nullable=False)
    prompt = Column(Text, nullable=False)
    response = Column(Text, nullable=False)
    reward = Column(Float, nullable=False)

    # Optional: Use these if storing embeddings
    embedding = Column(ARRAY(REAL))  # Or use pgvector.ARRAY(Float)
    features = Column(JSON)   # Additional extracted features

    source = Column(String)   # e.g., manual, agent, refinement
    run_id = Column(String)
    metadata_ = Column("metadata", JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))---END-OF-FILE---


"co_ai\models\mrq_preference_pair.py"
---START-OF-FILE---
from datetime import datetime, timezone

from sqlalchemy import JSON, Column, DateTime, Integer, Text

from co_ai.models.base import Base


class MRQPreferencePairORM(Base):
    __tablename__ = "mrq_preference_pairs"

    id = Column(Integer, primary_key=True)

    goal = Column(Text, nullable=False)
    prompt = Column(Text, nullable=False)

    output_a = Column(Text, nullable=False)
    output_b = Column(Text, nullable=False)
    preferred = Column(Text, nullable=False)  # 'a' or 'b'

    fmt_a = Column(Text)  # e.g., direct, short_cot, code, long_cot
    fmt_b = Column(Text)
    
    difficulty = Column(Text)
    
    features = Column(JSON)  # Optional: extra metadata
    run_id = Column(Text)
    source = Column(Text)  # e.g., arm_dataloader, user, 
    
    created_at = Column(DateTime, default=datetime.now(timezone.utc))


    def to_dict(self):
        return {
            "id": self.id,
            "goal": self.goal,
            "prompt": self.prompt,
            "output_a": self.output_a,
            "output_b": self.output_b,
            "preferred": self.preferred,
            "fmt_a": self.fmt_a,
            "fmt_b": self.fmt_b,
            "difficulty": self.difficulty,
            "features": self.features or {},
            "run_id": self.run_id,
            "source": self.source,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
---END-OF-FILE---


"co_ai\models\node_orm.py"
---START-OF-FILE---
# models/node_orm.py ---

from datetime import datetime

from sqlalchemy import JSON, Boolean, Column, DateTime, Float, Integer, String
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class NodeORM(Base):
    __tablename__ = "nodes"

    id = Column(Integer, primary_key=True)
    goal_id = Column(String, index=True)
    pipeline_run_id = Column(Integer, index=True)
    stage_name = Column(String, index=True)
    config = Column(JSON)
    hypothesis = Column(String)  # output or intermediate result
    metric = Column(Float)
    valid = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    def to_dict(self):
        return {
            "id": self.id,
            "goal_id": self.goal_id,
            "pipeline_run_id": self.pipeline_run_id,
            "stage_name": self.stage_name,
            "config": self.config,
            "hypothesis": self.hypothesis,
            "metric": self.metric,
            "valid": self.valid,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
    ---END-OF-FILE---


"co_ai\models\pattern_stat.py"
---START-OF-FILE---
# models/pattern_stat.py
from datetime import datetime, timezone

from sqlalchemy import Column, DateTime, Float, ForeignKey, Integer, String

from co_ai.models.base import Base


class PatternStatORM(Base):
    __tablename__ = "cot_pattern_stats"

    id = Column(Integer, primary_key=True)

    # Foreign keys
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)
    hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"), nullable=False)

    # Agent/Model Info
    model_name = Column(String, nullable=False)
    agent_name = Column(String, nullable=False)

    # Rubric Dimension + Label
    dimension = Column(String, nullable=False)  # e.g., "Inference Style"
    label = Column(String, nullable=False)      # e.g., "Deductive"
    confidence_score = Column(Float)           # Optional numeric score

    # Timestamps
    created_at = Column(DateTime, default=datetime.now(timezone.utc))---END-OF-FILE---


"co_ai\models\pipeline_run.py"
---START-OF-FILE---
# models/pipeline_run.py
from datetime import datetime, timezone

from sqlalchemy import JSON, Column, DateTime, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from .base import Base


class PipelineRunORM(Base):
    __tablename__ = "pipeline_runs"

    id = Column(Integer, primary_key=True)
    run_id = Column(String, unique=True, nullable=False)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)
    pipeline = Column(JSON)  # Stored as JSONB or TEXT[]
    name = Column(String)
    tag = Column(String)
    description = Column(String)
    strategy = Column(String)
    model_name = Column(String)
    run_config = Column(JSON)
    lookahead_context = Column(JSON)
    symbolic_suggestion = Column(JSON)
    extra_data = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="pipeline_runs")
    prompts = relationship("PromptORM", back_populates="pipeline_run", cascade="all, delete-orphan")
    hypotheses = relationship("HypothesisORM", back_populates="pipeline_run")
    symbolic_rules = relationship("SymbolicRuleORM", back_populates="pipeline_run")

    rule_applications = relationship(
        "RuleApplicationORM",
        back_populates="pipeline_run",
        cascade="all, delete-orphan",
    )
    scores = relationship(
        "EvaluationORM", back_populates="pipeline_run", cascade="all, delete-orphan"
    )---END-OF-FILE---


"co_ai\models\prompt_program.py"
---START-OF-FILE---
import uuid

from sqlalchemy import JSON, Column, Float, ForeignKey, Integer, String, Text
from sqlalchemy.orm import declarative_base, relationship

Base = declarative_base()

def generate_uuid():
    return str(uuid.uuid4())

class PromptProgramORM(Base):
    __tablename__ = "prompt_programs"

    id = Column(String, primary_key=True, default=generate_uuid)
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id", ondelete="SET NULL"), nullable=True)
    prompt_id = Column(Integer, ForeignKey("prompts.id", ondelete="SET NULL"), nullable=True)
    goal = Column(Text, nullable=False)
    template = Column(Text, nullable=False)
    inputs = Column(JSON, default={})
    version = Column(Integer, default=1)
    parent_id = Column(String, ForeignKey("prompt_programs.id"), nullable=True)
    strategy = Column(String, default="default")
    prompt_text = Column(Text, nullable=True)
    hypothesis = Column(Text, nullable=True)
    score = Column(Float, nullable=True)
    rationale = Column(Text, nullable=True)
    mutation_type = Column(String, nullable=True)
    execution_trace = Column(Text, nullable=True)
    extra_data = Column(JSON, default={})

 
    parent = relationship("PromptProgramORM", remote_side=[id], backref="children")
    prompt = relationship("PromptORM", backref="prompt_programs")
    pipeline_run = relationship("PipelineRunORM", back_populates="prompt_programs")

    def to_dict(self):
        return {
            "id": self.id,
            "goal": self.goal,
            "template": self.template,
            "inputs": self.inputs,
            "version": self.version,
            "parent_id": self.parent_id,
            "prompt_id": self.prompt_id,
            "propipeline_run_idmpt_id": self.pipeline_run_id,
            "strategy": self.strategy,
            "prompt_text": self.prompt_text,
            "hypothesis": self.hypothesis,
            "score": self.score,
            "rationale": self.rationale,
            "mutation_type": self.mutation_type,
            "execution_trace": self.execution_trace,
            "extra_data": self.extra_data,
        }
---END-OF-FILE---


"co_ai\models\prompt.py"
---START-OF-FILE---
# models/prompt.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Boolean, Column, DateTime, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class PromptORM(Base):
    __tablename__ = "prompts"

    id = Column(Integer, primary_key=True)

    # Agent and prompt metadata
    goal_id = Column(Integer, ForeignKey("goals.id"))
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id", ondelete="SET NULL"), nullable=True)
    agent_name = Column(String, nullable=False)
    prompt_key = Column(String, nullable=False)  # e.g., generation_goal_aligned.txt
    prompt_text = Column(Text, nullable=False)
    response_text = Column(Text)  # Optional â€” if storing model output too
    source = Column(String)  # e.g., manual, dsp_refinement, feedback_injection
    strategy = Column(String)  # e.g., goal_aligned, out_of_the_box
    version = Column(Integer, default=1)
    is_current = Column(Boolean, default=False)
    extra_data = Column(JSON)
    timestamp = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="prompts")
    hypotheses = relationship("HypothesisORM", back_populates="prompt")
    symbolic_rules = relationship("SymbolicRuleORM", back_populates="prompt")
    pipeline_run = relationship("PipelineRunORM", back_populates="prompts")

    def to_dict(self, include_relationships: bool = False) -> dict:
        data = {
            "id": self.id,
            "agent_name": self.agent_name,
            "prompt_key": self.prompt_key,
            "prompt_text": self.prompt_text,
            "response_text": self.response_text,
            "goal_id": self.goal_id,
            "source": self.source,
            "strategy": self.strategy,
            "version": self.version,
            "is_current": self.is_current,
            "extra_data": self.extra_data,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None,
        }

        if include_relationships:
            data["goal"] = self.goal.to_dict() if self.goal else None
            data["hypotheses"] = [h.to_dict() for h in self.hypotheses] if self.hypotheses else []

        return data
---END-OF-FILE---


"co_ai\models\reflection_delta.py"
---START-OF-FILE---
# models/reflection_delta.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Boolean, Column, DateTime, Float, ForeignKey,
                        Integer, String)
from sqlalchemy.orm import relationship

from .base import Base


class ReflectionDeltaORM(Base):
    __tablename__ = "reflection_deltas"

    id = Column(Integer, primary_key=True)
    goal_id = Column(Integer, ForeignKey("goals.id"))
    run_id_a = Column(String, nullable=False)
    run_id_b = Column(String, nullable=False)
    score_a = Column(Float)
    score_b = Column(Float)
    score_delta = Column(Float)
    pipeline_a = Column(JSON)
    pipeline_b = Column(JSON)
    pipeline_diff = Column(JSON)
    strategy_diff = Column(Boolean, default=False)
    model_diff = Column(Boolean, default=False)
    rationale_diff = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="reflection_deltas")---END-OF-FILE---


"co_ai\models\rule_application.py"
---START-OF-FILE---
# models/rule_application.py

from datetime import datetime, timezone

from sqlalchemy import (JSON, Column, DateTime, Float, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from .base import Base


class RuleApplicationORM(Base):
    __tablename__ = "rule_applications"

    id = Column(Integer, primary_key=True)
    rule_id = Column(Integer, ForeignKey("symbolic_rules.id", ondelete="CASCADE"))
    goal_id = Column(Integer, ForeignKey("goals.id", ondelete="CASCADE"))
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id", ondelete="CASCADE"))
    hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"), nullable=True)
    context_hash = Column(String, index=True)

    applied_at = Column(DateTime, default=datetime.now(timezone.utc))
    agent_name = Column(String, nullable=True)
    change_type = Column(String, nullable=True)  # e.g., pipeline_override, hint, etc.
    details = Column(JSON, nullable=True)        # Can store {old:..., new:...}
    stage_details = Column(JSON, nullable=True)

    post_score = Column(Float, nullable=True)
    pre_score = Column(Float, nullable=True)
    delta_score = Column(Float, nullable=True)
    evaluator_name = Column(String, nullable=True)
    rationale = Column(Text, nullable=True)
    notes = Column(Text, nullable=True)

    # Relationships (optional, if you want to access related objects)
    goal = relationship("GoalORM", back_populates="rule_applications")
    rule = relationship("SymbolicRuleORM", back_populates="applications")
    pipeline_run = relationship("PipelineRunORM", back_populates="rule_applications")
    hypothesis = relationship("HypothesisORM", back_populates="rule_applications")


    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "rule_id": self.rule_id,
            "goal_id": self.goal_id,
            "pipeline_run_id": self.pipeline_run_id,
            "hypothesis_id": self.hypothesis_id,
            "applied_at": self.applied_at.isoformat() if self.applied_at else None,
            "agent_name": self.agent_name,
            "change_type": self.change_type,
            "details": self.details,
            "post_score": self.post_score,
            "pre_score": self.pre_score,
            "delta_score": self.delta_score,
            "evaluator_name": self.evaluator_name,
            "rationale": self.rationale,
            "notes": self.notes
        }
---END-OF-FILE---


"co_ai\models\score_dimension.py"
---START-OF-FILE---
# co_ai/db/orm/score_dimension.py
from sqlalchemy import JSON, Column, Float, Integer, String, Text
from sqlalchemy.orm import declarative_base, relationship

Base = declarative_base()

class ScoreDimensionORM(Base):
    __tablename__ = "score_dimensions"

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True, nullable=False)  # e.g., 'clarity'
    stage = Column(String, nullable=True)               # e.g., 'review', 'reflection'
    prompt_template = Column(Text, nullable=False)      # Template with {goal}, {hypothesis}, etc.
    weight = Column(Float, default=1.0)

    # Optional relationships or metadata fields
    notes = Column(Text, nullable=True)
    extra_data = Column(JSON, nullable=True)  # Flexible config extension

    # def to_score_dimension(self, parser_fn):
    #     from mrq.scoring import ScoreDimension
    #     return ScoreDimension(
    #         name=self.name,
    #         prompt_template=self.prompt_template,
    #         weight=self.weight,
    #         parser=parser_fn,
    #     )
    
    def to_dict(self):
        return {
            "id": self.id,
            "name": self.name,
            "stage": self.stage,
            "prompt_template": self.prompt_template,
            "weight": self.weight,
            "notes": self.notes,
            "extra_data": self.extra_data or {},
        }
    ---END-OF-FILE---


"co_ai\models\score.py"
---START-OF-FILE---
# models/score.py

from sqlalchemy import Column, Float, ForeignKey, Integer, String, Text
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class ScoreORM(Base):
    __tablename__ = "scores"

    id = Column(Integer, primary_key=True)
    evaluation_id = Column(Integer, ForeignKey("evaluations.id", ondelete="CASCADE"), nullable=False)
    dimension = Column(String, nullable=False)
    score = Column(Float)
    weight = Column(Float)
    rationale = Column(Text)

    evaluation = relationship("EvaluationORM", back_populates="dimension_scores")

    def to_dict(self):
        return {
            "id": self.id,
            "evaluation_id": self.evaluation_id,
            "dimension": self.dimension,
            "score": self.score,
            "weight": self.weight,
            "rationale": self.rationale
        }
---END-OF-FILE---


"co_ai\models\search_result.py"
---START-OF-FILE---
# models/search_result.py
from datetime import datetime, timezone

from sqlalchemy import (ARRAY, JSON, Column, DateTime, ForeignKey, Integer,
                        String, Text)

from .base import Base


class SearchResultORM(Base):
    __tablename__ = "search_results"

    id = Column(Integer, primary_key=True)
    query = Column(Text, nullable=False)
    source = Column(String, nullable=False)
    result_type = Column(String)
    title = Column(Text)
    summary = Column(Text)
    url = Column(Text)
    author = Column(String)
    published_at = Column(DateTime)
    tags = Column(ARRAY(String))
    goal_id = Column(Integer, ForeignKey("goals.id"))
    parent_goal = Column(Text)
    strategy = Column(String)
    focus_area = Column(String)
    extra_data = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    # ðŸ” New Fields Below â€” For Knowledge Refinement & Idea Linking
    key_concepts = Column(ARRAY(String))
    technical_insights = Column(ARRAY(String))
    relevance_score = Column(Integer)  # 1â€“10 score for how relevant this is to the goal
    novelty_score = Column(Integer)  # Estimated novelty vs. prior knowledge
    related_ideas = Column(ARRAY(String))  # List of idea IDs or descriptions
    refined_summary = Column(Text)  # A concise, processed summary for downstream agents
    extracted_methods = Column(
        ARRAY(String)
    )  # Techniques or methods described in the result
    domain_knowledge_tags = Column(
        ARRAY(String)
    )  # e.g., "self-modifying", "graph transformer"
    critique_notes = Column(Text)  # Feedback from evaluator agent (Mr Q), if any

    def to_dict(self, include_relationships: bool = False) -> dict:
        return {
            "id": self.id,
            "query": self.query,
            "source": self.source,
            "result_type": self.result_type,
            "title": self.title,
            "summary": self.summary,
            "url": self.url,
            "author": self.author,
            "published_at": self.published_at.isoformat()
            if self.published_at
            else None,
            "tags": self.tags,
            "goal_id": self.goal_id,
            "parent_goal": self.parent_goal,
            "strategy": self.strategy,
            "focus_area": self.focus_area,
            "key_concepts": self.key_concepts,
            "technical_insights": self.technical_insights,
            "relevance_score": self.relevance_score,
            "novelty_score": self.novelty_score,
            "related_ideas": self.related_ideas,
            "extracted_methods": self.extracted_methods,
            "critique_notes": self.critique_notes,
            "extra_data": self.extra_data,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
---END-OF-FILE---


"co_ai\models\sharpening_prediction.py"
---START-OF-FILE---
# models/sharpening_prediction.py
from datetime import datetime, timezone

from sqlalchemy import Column, DateTime, Float, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class SharpeningPredictionORM(Base):
    __tablename__ = "sharpening_predictions"

    id = Column(Integer, primary_key=True)

    # Goal context
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)

    # Prompt that led to the comparison
    prompt_text = Column(String, nullable=False)

    # Original and evolved outputs
    output_a = Column(String, nullable=False)
    output_b = Column(String, nullable=False)

    # Evaluation results
    preferred = Column(String)  # 'a' or 'b'
    predicted = Column(String)  # 'a' or 'b'

    value_a = Column(Float, nullable=False)
    value_b = Column(Float, nullable=False)

    created_at = Column(DateTime, default=datetime.now(timezone.utc))
    
    # Relationships
    goal = relationship("GoalORM", back_populates="sharpening_predictions")

    def to_dict(self, include_relationships: bool = False) -> dict:
        """
        Convert this ORM instance to a dictionary representation.

        Args:
            include_relationships (bool): Whether to include related objects like GoalORM

        Returns:
            dict: Dictionary of all fields
        """
        result = {
            "id": self.id,
            "goal_id": self.goal_id,
            "prompt_text": self.prompt_text,
            "output_a": self.output_a,
            "output_b": self.output_b,
            "preferred": self.preferred,
            "predicted": self.predicted,
            "value_a": self.value_a,
            "value_b": self.value_b,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }

        if include_relationships:
            result["goal"] = self.goal.to_dict() if self.goal else None

        return result---END-OF-FILE---


"co_ai\models\sharpening_result.py"
---START-OF-FILE---
from datetime import datetime

from sqlalchemy import Boolean, Column, DateTime, Float, String, Text

from co_ai.models.base import \
    Base  # Adjust this import to match your actual Base


class SharpeningResultORM(Base):
    __tablename__ = 'sharpening_results'

    id = Column(String, primary_key=True)  # Optional: Add UUID or auto-increment ID
    goal = Column(Text, nullable=False)
    prompt = Column(Text, nullable=False)
    template = Column(Text, nullable=False)
    original_output = Column(Text, nullable=False)
    sharpened_output = Column(Text, nullable=False)
    preferred_output = Column(Text, nullable=False)
    winner = Column(String, nullable=False)
    improved = Column(Boolean, nullable=False)
    comparison = Column(Text, nullable=False)
    score_a = Column(Float, nullable=False)
    score_b = Column(Float, nullable=False)
    score_diff = Column(Float, nullable=False)
    best_score = Column(Float, nullable=False)
    prompt_template = Column(Text, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    def to_dict(self, include_nulls=False):
        data = {
            "id": self.id,
            "goal": self.goal,
            "prompt": self.prompt,
            "template": self.template,
            "original_output": self.original_output,
            "sharpened_output": self.sharpened_output,
            "preferred_output": self.preferred_output,
            "winner": self.winner,
            "improved": self.improved,
            "comparison": self.comparison,
            "score_a": self.score_a,
            "score_b": self.score_b,
            "score_diff": self.score_diff,
            "best_score": self.best_score,
            "prompt_template": self.prompt_template,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
        if not include_nulls:
            data = {k: v for k, v in data.items() if v is not None}
        return data---END-OF-FILE---


"co_ai\models\symbolic_rule.py"
---START-OF-FILE---
import hashlib
import json
from datetime import datetime

from sqlalchemy import (JSON, Column, DateTime, Float, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from .base import Base


class SymbolicRuleORM(Base):
    __tablename__ = "symbolic_rules"

    id = Column(Integer, primary_key=True)

    # General metadata
    source = Column(String)  # e.g., 'manual', 'lookahead', 'pipeline_stage'
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # New design: generalized rules
    target = Column(String, nullable=False)  # e.g., 'goal', 'agent', 'prompt', 'pipeline', 'hypothesis'
    attributes = Column(JSON)  # What to apply, e.g., {"difficulty": "hard"}
    filter = Column(JSON)  # How to match, e.g., {"goal_type": "research", "strategy": "reasoning"}
    context_hash = Column(String, index=True)  # Hash of (filters + attributes)


    goal_type = Column(String)
    goal_category = Column(String)
    difficulty = Column(String)
    focus_area = Column(String)

    # Optional linkage (for legacy/rule provenance/debugging)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=True)
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id"), nullable=True)
    prompt_id = Column(Integer, ForeignKey("prompts.id"), nullable=True)
    agent_name = Column(String, nullable=True)

    # Optional scoring/description
    score = Column(Float)
    rule_text = Column(Text)

    # Relationships
    goal = relationship("GoalORM", back_populates="symbolic_rules", lazy="joined")
    pipeline_run = relationship("PipelineRunORM", back_populates="symbolic_rules", lazy="joined")
    prompt = relationship("PromptORM", back_populates="symbolic_rules", lazy="joined")
    scores = relationship("EvaluationORM", back_populates="symbolic_rule")
    applications = relationship(
        "RuleApplicationORM", back_populates="rule", cascade="all, delete-orphan"
    )

    def __str__(self):
        return (
            f"<SymbolicRuleORM id={self.id} "
            f"target={self.target} "
            f"filter={json.dumps(self.filter, sort_keys=True)} "
            f"attributes={json.dumps(self.attributes, sort_keys=True)} "
            f"context_hash={self.context_hash[:8]}... "
            f"source={self.source} "
            f"agent={self.agent_name}>"
        )

    @staticmethod
    def compute_context_hash(filters: dict, attributes: dict) -> str:
        merged = {"filters": filters or {}, "attributes": attributes or {}}
        canonical_str = json.dumps(merged, sort_keys=True)
        return hashlib.sha256(canonical_str.encode("utf-8")).hexdigest()

    def to_dict(self, include_relationships=False):
        return {
            "id": self.id,
            "target": self.target,
            "filter": self.filter,
            "attributes": self.attributes,
            "context_hash": self.context_hash,
            "source": self.source,
            "goal_id": self.goal_id,
            "pipeline_run_id": self.pipeline_run_id,
            "prompt_id": self.prompt_id,
            "agent_name": self.agent_name,
            "score": self.score,
            "rule_text": self.rule_text,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
        }
---END-OF-FILE---


"co_ai\models\unified_mrq.py"
---START-OF-FILE---
from datetime import datetime

from sqlalchemy import JSON, Column, DateTime, Integer, String, Text

from co_ai.models.base import Base


class UnifiedMRQModelORM(Base):
    __tablename__ = "unified_mrq_models"

    id = Column(Integer, primary_key=True)
    dimension = Column(String, nullable=False)
    model_path = Column(Text, nullable=False)
    trained_on = Column(DateTime, default=datetime.utcnow)
    pair_count = Column(Integer)
    trainer_version = Column(String)
    notes = Column(Text)
    context = Column(JSON)

    def __repr__(self):
        return f"<UnifiedMRQModelORM(dimension='{self.dimension}', trained_on={self.trained_on})>"
---END-OF-FILE---


"co_ai\parsers\__init__.py"
---START-OF-FILE---
"""Parser utils"""
from .utils import extract_hypotheses
---END-OF-FILE---


"co_ai\parsers\utils.py"
---START-OF-FILE---
import re


def extract_hypotheses(text: str):
    # First attempt: Try precise regex-based extraction
    pattern = re.compile(
        r"(# Hypothesis\s+\d+\s*\n(?:.*?))(?:\n(?=# Hypothesis\s+\d+)|\Z)",
        re.IGNORECASE | re.DOTALL,
    )
    matches = list(pattern.finditer(text))

    if matches:
        return [match.group(1).strip() for match in matches]

    # Fallback (if needed)
    split_parts = re.split(r"\bHypothesis\s+\d+\b", text, flags=re.IGNORECASE)
    if len(split_parts) <= 1:
        return [text]

    hypotheses = []
    for i, part in enumerate(split_parts[1:], start=1):
        cleaned = part.strip()
        if cleaned:
            hypotheses.append(f"Hypothesis {i} {cleaned}")

    return hypotheses
---END-OF-FILE---


"co_ai\prompts\__init__.py"
---START-OF-FILE---
from .prompt_loader import PromptLoader
from .strategy_resolver import StrategyResolver
---END-OF-FILE---


"co_ai\prompts\prompt_loader.py"
---START-OF-FILE---
import os

from jinja2 import Template

from co_ai.constants import (DEFAULT, FILE, NAME, PROMPT_DIR, PROMPT_FILE,
                             PROMPT_MODE, STRATEGY)


def get_text_from_file(file_path: str) -> str:
    """Reads and returns stripped text from a file."""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()


class PromptLoader:
    def __init__(self, memory=None, logger=None):
        self.memory = memory
        self.logger = logger

    def load_prompt(self, config: dict, context: dict) -> str:
        """
        Load and render a prompt based on the strategy defined in the agent config.
        Supports: file, template, tuning, or static.
        """
        prompt_type = config.get(PROMPT_MODE, FILE)
        prompts_dir = context.get(PROMPT_DIR, "prompts")

        if not os.path.isdir(prompts_dir):
            raise FileNotFoundError(f"Prompt directory not found: {prompts_dir}")

        merged = self._merge_context(config, context)

        try:
            if prompt_type == "static":
                prompt_text = config.get("prompt_text")
                if not prompt_text:
                    raise ValueError("Missing 'prompt_text' in static config.")
                return Template(prompt_text).render(**merged)

            if prompt_type == "tuning":
                agent_name = config.get(NAME, "default")
                return self._load_best_version(agent_name, context.get("goal", ""), merged)

            return self._load_from_file(merged)

        except Exception as e:
            print(f"âŒ Exception:  {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("PromptLoadFailed", {
                    "agent": config.get(NAME, DEFAULT),
                    "error": str(e),
                    "config_used": config
                })
            return self._fallback_prompt(context.get("goal", ""))

    def from_file(self, file_name: str, config: dict, context: dict) -> str:
        """Manually load and render a prompt file."""
        path = self.get_file_path(file_name, config, context)
        prompt_text = get_text_from_file(path)
        merged = self._merge_context(config, context)
        try:
            return Template(prompt_text).render(**merged)
        except KeyError as ke:
            if self.logger:
                self.logger.log("PromptFormattingError", {
                    "exception": ke,
                    "prompt_text": prompt_text,
                })

    @staticmethod
    def get_file_path(file_name: str, cfg: dict, context: dict) -> str:
        """Builds full prompt file path."""
        prompts_dir = context.get(PROMPT_DIR, "prompts")
        filename = file_name if file_name.endswith(".txt") else f"{file_name}.txt"
        return os.path.join(prompts_dir, cfg.get("name", "default"), filename)

    def _load_from_file(self, config: dict) -> str:
        """Loads and renders a prompt file based on config."""
        prompt_dir = config.get(PROMPT_DIR, "prompts")
        file_key = config.get(PROMPT_FILE) or config.get(STRATEGY) or DEFAULT
        file_name = f"{file_key}.txt" if not file_key.endswith(".txt") else file_key
        path = os.path.join(prompt_dir, config.get(NAME, "default"), file_name)

        self.logger.log("PromptFileLoading", {
            "file_key": file_key,
            "resolved_file": file_name,
            "path": path
        })

        if not os.path.exists(path):
            if self.logger:
                self.logger.log("PromptFileNotFound", {"path": path, "agent": config.get(NAME, DEFAULT)})
            raise FileNotFoundError(f"Prompt file not found: {path}")

        try:
            prompt_text = get_text_from_file(path)
            rendered =  Template(prompt_text).render(**self._merge_context(config, {}))
            self.logger.log("PromptFileLoaded", {
                "path": path,
                "rendered_preview": rendered[:100]
            })
            return rendered
        except KeyError as ke:
            if self.logger:
                self.logger.log("PromptFormattingError", {
                    "missing_key": str(ke),
                    "path": path
                })
            return prompt_text  # Fallback: return raw

    def _load_best_version(self, agent_name: str, goal: str, config: dict) -> str:
        """Load a tuned version of the prompt if available."""
        best_prompt = self.memory.prompt.get_best_prompt_for_agent(
            agent_name=agent_name,
            strategy=config.get(STRATEGY, DEFAULT),
            goal=goal
        )
        if best_prompt:
            return best_prompt["prompt_text"]

        if self.logger:
            self.logger.log("UsingFallbackPrompt", {"reason": "no_tuned_prompt_found"})

        return self._load_from_file(config)

    def _fallback_prompt(self, goal: str = "") -> str:
        """Minimal backup prompt if nothing else works."""
        return f"Generate hypothesis for goal: {goal or '[unspecified goal]'}"

    @staticmethod
    def _merge_context(config: dict, context: dict) -> dict:
        """Merges agent config and pipeline context."""
        return {**context, **config}
---END-OF-FILE---


"co_ai\prompts\strategy_resolver.py"
---START-OF-FILE---
import os

from co_ai.constants import GOAL


class StrategyResolver:
    def __init__(self, config):
        self.base_prompt_dir = getattr(config, "prompt_dir", "prompts/general_reasoner")
        self.default_strategy = getattr(config, "strategy", "cot")
        self.agent_name = getattr(config, "name", "general_reasoner")

    def resolve(self, context):
        """
        Determines the reasoning strategy and prompt path based on the context and config.
        Context should include goal metadata such as type, strategy, etc.

        Parameters:
            context (dict): Contains the goal object and optionally agent metadata.

        Returns:
            (str, str): Tuple of (strategy, prompt_file_path)
        """
        goal = context.get(GOAL).get("goal_text")
        is_reasoning_agent = context.get("agent_type", "reasoning") == "reasoning"

        if is_reasoning_agent:
            strategy = goal.get("reasoning_strategy", self.default_strategy)
        else:
            strategy = self.default_strategy

        filename = f"{strategy}_{self.agent_name}.j2"
        full_path = os.path.join(self.base_prompt_dir, filename)

        if not os.path.isfile(full_path):
            raise FileNotFoundError(f"Prompt template not found for strategy '{strategy}': {full_path}")

        return strategy, full_path
---END-OF-FILE---


"co_ai\registry\__init__.py"
---START-OF-FILE---
from .agent_registry import AgentRegistry---END-OF-FILE---


"co_ai\registry\agent_registry.py"
---START-OF-FILE---
# --- co_ai/registry/agent_registry.py ---

from co_ai.agents.automind import AutoMindAgent
from co_ai.agents.self_aware_planner import SelfAwarePlannerAgent


class AgentRegistry:
    def __init__(self, config):
        self.config = config
        self.agents = {
            "automind": lambda: AutoMindAgent(config.agents.automind),
            "selfaware": lambda: SelfAwarePlannerAgent(config.agents.selfaware),
            # Add more agents here as needed
        }

    def get(self, name):
        if name not in self.agents:
            raise ValueError(f"Agent '{name}' not found in registry.")
        return self.agents[name]()
---END-OF-FILE---


"co_ai\reports\__init__.py"
---START-OF-FILE---
from .formatter import ReportFormatter
from .general_reason import save_markdown_report
---END-OF-FILE---


"co_ai\reports\formatter.py"
---START-OF-FILE---
import re
from datetime import datetime, timezone
from pathlib import Path


class ReportFormatter:
    def __init__(self, output_dir="reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def format_report(self, context):
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

        item = context.get("goal")
        if isinstance(item, str):
            goal = item
        else:
            goal = item.get("goal_text", "Error No Goal")

        safe_goal = sanitize_goal_for_filename(goal)
        file_name = f'{safe_goal}_{timestamp}_report.md'
        file_path = self.output_dir / file_name

        content = f"""# ðŸ§ª AI Co-Research Summary Report

**ðŸ—‚ï¸ Run ID:** `{context.get("run_id", "Error No Run_id")}`  
**ðŸŽ¯ Goal:** *{goal}*  
**ðŸ“… Timestamp:** {timestamp}

---

### ðŸ”¬ Literature:
{self._format_list(context.get("literature", []))}



### ðŸ”¬ Hypotheses Generated:
{self._format_list([h if isinstance(h, str) else h.get("text", "") for h in context.get("hypotheses", [])])}
OK so 


---

### ðŸªž Reflections:
{self._format_reflections(context.get("reflections", []))}


---

### ðŸ§  Persona Reviews:
{self._format_reviews(context.get("reviews", []))}

---

### ðŸ§¬ Evolution Outcome:
- {len(context.get("evolved", []))} hypotheses evolved.

---

### ðŸ“˜ Meta-Review Summary:
> {context.get("meta_review", "")}


### ðŸ“˜ Feedback:
{context.get("feedback", "")}

### ðŸ“˜ DB Matches:
{context.get("proximity", {}).get("database_matches", [])}


---
"""

        file_path.write_text(content, encoding="utf-8")
        return str(file_path)

    def _format_list(self, items):
        return "\n".join(f"1. **{item.strip()}**" for item in items)

    def _format_reviews(self, reviews):
        if not reviews:
            return "No reviews recorded."
        formatted = []
        for r in reviews:
            persona = r.get("persona", "Unknown")
            review = r.get("review", "")
            formatted.append(f"**{persona}:**\n> {review}")
        return "\n\n".join(formatted)

    def _format_reflections(self, reflections):
        if not reflections:
            return "No reflections recorded."
        formatted = []
        for r in reflections:
            reflection = r.get("reflection", "")
            formatted.append(f"**Reflection:**\n> {reflection}")
        return "\n\n".join(formatted)



def sanitize_goal_for_filename(goal: str, length:int=40) -> str:
    """
    Converts a goal string into a safe filename:
    - Replaces non-alphanumeric characters with underscores
    - Truncates to 100 characters
    - Appends a UTC timestamp
    """
    safe = re.sub(r'[^a-zA-Z0-9]', '_', goal)  # Replace non-alphanumeric
    safe = safe[:length]                                       # Limit to (len) characters
    return safe
---END-OF-FILE---


"co_ai\reports\general_reason.py"
---START-OF-FILE---
from datetime import datetime, timezone
from pathlib import Path


def save_markdown_report(result: dict, out_dir: str = "./reports"):
    goal = result["goal"]["goal_text"]
    run_id = result.get("run_id", datetime.now(timezone.utc).isoformat())
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")

    report_md = f"""# General Reasoner Run Report

**Run ID:** {run_id}  
**Timestamp:** {timestamp}  
**Goal:** {goal}

---

## ðŸ§  Hypotheses

"""
    for hyp in result["hypotheses"]:
        strategy = hyp.get("strategy", hyp["features"].get("strategy", "unknown"))
        report_md += f"""### Strategy: `{strategy}`  
{hyp['text']}

---
"""

    report_md += "\n## ðŸ§ª Judgments\n\n"
    for score in result["scoring"]:
        strategy_a = score["hypothesis_a"][:50].replace('\n', ' ') + "..."
        strategy_b = score["hypothesis_b"][:50].replace('\n', ' ') + "..."
        winner = score["winner"]
        reason = score["reason"]
        report_md += f"""- **Winner:** `{winner}`  
  - A: {strategy_a}  
  - B: {strategy_b}  
  - Reason: {reason}\n\n"""

    # Optional: Add per-strategy summary
    from collections import defaultdict
    score_stats = defaultdict(list)
    for score in result["scoring"]:
        score_stats[score["winner"]].append(score.get("score_b", 0))  # use score_b as winner

    report_md += "\n## ðŸ“Š Strategy Performance\n\n"
    report_md += "| Strategy | Judged Wins | Avg Score |\n|----------|--------------|------------|\n"
    for strategy, scores in score_stats.items():
        avg_score = round(sum(scores) / len(scores), 2)
        report_md += f"| {strategy} | {len(scores)} | {avg_score} |\n"

    # Save the markdown
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    out_path = Path(out_dir) / f"general_reasoner_report_{timestamp}.md"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(report_md)

    print(f"âœ… Markdown report saved to {out_path}")
---END-OF-FILE---


"co_ai\rules\__init__.py"
---START-OF-FILE---
from .rule_tuner import RuleTuner
from .symbolic_rule_applier import SymbolicRuleApplier
---END-OF-FILE---


"co_ai\rules\rule_tuner.py"
---START-OF-FILE---

class RuleTuner:
    """
    Provides utilities to modify or tune symbolic rules based on performance analytics.
    """

    def __init__(self, memory, logger):
        self.memory = memory
        self.logger = logger

    def increase_priority(self, rule_id: int, amount: float = 0.1) -> float:
        """
        Increases the priority of the rule by a given amount. If no priority is set, defaults to 1.0.
        """
        rule = self.memory.symbolic_rules.get_by_id(rule_id)
        if not rule:
            self.logger.log("RuleNotFound", {"rule_id": rule_id})
            return None

        current_priority = rule.attributes.get("priority", 1.0)
        new_priority = round(float(current_priority) + amount, 4)
        rule.attributes["priority"] = new_priority

        self.memory.symbolic_rules.update(rule)
        self.logger.log("RulePriorityUpdated", {
            "rule_id": rule_id,
            "old_priority": current_priority,
            "new_priority": new_priority,
        })

        return new_priority

    def decrease_priority(self, rule_id: int, amount: float = 0.1) -> float:
        """
        Decreases the priority of the rule by a given amount (min 0.0).
        """
        rule = self.memory.symbolic_rules.get(rule_id)
        if not rule:
            self.logger.log("RuleNotFound", {"rule_id": rule_id})
            return None

        current_priority = rule.attributes.get("priority", 1.0)
        new_priority = max(0.0, round(float(current_priority) - amount, 4))
        rule.attributes["priority"] = new_priority

        self.memory.symbolic_rules.update(rule)
        self.logger.log("RulePriorityUpdated", {
            "rule_id": rule_id,
            "old_priority": current_priority,
            "new_priority": new_priority,
        })

        return new_priority
---END-OF-FILE---


"co_ai\rules\symbolic_rule_applier.py"
---START-OF-FILE---
import hashlib
import json
from pathlib import Path
from typing import Any, Dict

import yaml

from co_ai.memory.symbolic_rule_store import SymbolicRuleORM


class SymbolicRuleApplier:
    def __init__(self, cfg, memory, logger):
        self.cfg = cfg
        self.memory = memory
        self.logger = logger
        self.enabled = cfg.get("symbolic", {}).get("enabled", False)
        if self.enabled:
            self.rules = self._load_rules()


    def apply(self, context: dict) -> dict:
        if not self.enabled:
            return context

        goal = context.get("goal", {})
        pipeline_run_id = context.get("pipeline_run_id")
        current_pipeline = context.get("pipeline", [])

        matching_rules = [r for r in self.rules if self._matches_metadata(r, goal)]

        if not matching_rules:
            self.logger.log("NoSymbolicRulesApplied", {"goal_id": goal.get("id")})
            return context

        self.logger.log("SymbolicRulesFound", {"count": len(matching_rules)})

        for rule in matching_rules:
            if rule.rule_text and "pipeline:" in rule.rule_text:
                suggested_pipeline = (
                    rule.rule_text.split("pipeline:")[-1].strip().split(",")
                )
                suggested_pipeline = [
                    s.strip() for s in suggested_pipeline if s.strip()
                ]
                if suggested_pipeline:
                    self.logger.log(
                        "PipelineUpdatedBySymbolicRule",
                        {
                            "from": current_pipeline,
                            "to": suggested_pipeline,
                            "rule_id": rule.id,
                        },
                    )
                    context["pipeline"] = suggested_pipeline
                    context["pipeline_updated_by_symbolic_rule"] = True

            if rule.source == "lookahead" and rule.goal_type:
                context["symbolic_hint"] = f"use_{rule.goal_type.lower()}_strategy"

        return context

    from datetime import datetime

    from co_ai.models import RuleApplicationORM

    def apply_to_agent(self, cfg: Dict, context: Dict) -> Dict:
        if not self.enabled:
            return cfg

        goal = context.get("goal", {})
        pipeline_run_id = context.get("pipeline_run_id")
        agent_name = cfg.get("name")

        matching_rules = [
            r
            for r in self.rules
            if r.agent_name == agent_name and self._matches_metadata(r, goal)
        ]

        if not matching_rules:
            self.logger.log(
                "NoSymbolicAgentRulesApplied",
                {
                    "agent": agent_name,
                    "goal_id": goal.get("id"),
                },
            )
            return cfg

        self.logger.log(
            "SymbolicAgentRulesFound",
            {
                "agent": agent_name,
                "goal_id": goal.get("id"),
                "count": len(matching_rules),
            },
        )

        for rule in matching_rules:
            # Apply new-style attributes
            if rule.attributes:
                for key, value in rule.attributes.items():
                    if key in cfg:
                        self.logger.log(
                            "SymbolicAgentOverride",
                            {
                                "agent": agent_name,
                                "key": key,
                                "old_value": cfg[key],
                                "new_value": value,
                                "rule_id": rule.id,
                            },
                        )
                    else:
                        self.logger.log(
                            "SymbolicAgentNewKey",
                            {
                                "agent": agent_name,
                                "key": key,
                                "value": value,
                                "rule_id": rule.id,
                            },
                        )
                    cfg[key] = value

            # Apply legacy rule_text (optional, for backward compatibility)
            if rule.rule_text:
                entries = [e.strip() for e in rule.rule_text.split(",") if e.strip()]
                for entry in entries:
                    if ":" in entry:
                        key, value = [s.strip() for s in entry.split(":", 1)]
                        if key in cfg:
                            self.logger.log(
                                "SymbolicAgentOverride",
                                {
                                    "agent": agent_name,
                                    "key": key,
                                    "old_value": cfg[key],
                                    "new_value": value,
                                    "rule_id": rule.id,
                                },
                            )
                        else:
                            self.logger.log(
                                "SymbolicAgentNewKey",
                                {
                                    "agent": agent_name,
                                    "key": key,
                                    "value": value,
                                    "rule_id": rule.id,
                                },
                            )
                        cfg[key] = value

            # Record the application of this rule
            self.memory.rule_effects.insert(
                goal_id=goal.get("id"),
                agent_name=agent_name,
                rule_id=rule.id,
                pipeline_run_id=pipeline_run_id,
                details=rule.to_dict(),
                stage_details=cfg,
            )

        return cfg


    def apply_prompt_rules(
            self, agent_name: str, prompt_cfg: dict, context: dict
        ) -> dict:
        """
        Applies prompt-level symbolic rules to the prompt config before generation.

        Returns the updated prompt_cfg.
        """
        goal = context.get("goal", {})
        applicable_rules = [
            rule
            for rule in self.rules
            if rule.agent_name == agent_name
            # and self._matches_filter(rule.filter, goal)
        ]

        if not applicable_rules:
            self.logger.log("NoPromptRulesFound", {"agent": agent_name})
            return prompt_cfg

        for rule in applicable_rules:
            for key, value in rule.attributes.items():
                self.logger.log(
                    "PromptAttributeOverride",
                    {
                        "agent": agent_name,
                        "key": key,
                        "old_value": prompt_cfg.get(key),
                        "new_value": value,
                        "rule_id": rule.id,
                        "emoji": "ðŸ› ï¸",
                    },
                )
                self.set_nested(prompt_cfg, key, value)

            # Optional: record the rule application
            self.memory.rule_effects.insert(
                rule_id=rule.id,
                goal_id=goal.get("id"),
                pipeline_run_id=context.get("pipeline_run_id"),
                details=prompt_cfg,
            )

        return prompt_cfg

    def set_nested(self, cfg: dict, dotted_key: str, value):
        keys = dotted_key.split(".")
        d = cfg
        for k in keys[:-1]:
            if k not in d or not isinstance(d[k], dict):
                d[k] = {}
            d = d[k]
        d[keys[-1]] = value

    def apply_to_prompt(self, cfg: Dict, context: Dict) -> Dict:
        if not self.enabled:
            return cfg

        goal = context.get("goal", {})
        pipeline_run_id = context.get("pipeline_run_id")
        prompt_name = cfg.get("prompt_key", "unknown_prompt")

        matching_rules = [
            r for r in self.rules
            if r.target == "prompt" and self._matches_filter(r.filter, goal)
        ]

        if not matching_rules:
            self.logger.log("NoSymbolicPromptRulesApplied", {
                "prompt": prompt_name,
                "goal_id": goal.get("id"),
            })
            return cfg

        self.logger.log("SymbolicPromptRulesFound", {
            "prompt": prompt_name,
            "goal_id": goal.get("id"),
            "count": len(matching_rules),
        })

        for rule in matching_rules:
            for key, value in rule.attributes.items():
                if key in cfg:
                    self.logger.log("SymbolicPromptOverride", {
                        "prompt": prompt_name,
                        "key": key,
                        "old_value": cfg[key],
                        "new_value": value,
                        "rule_id": rule.id,
                    })
                else:
                    self.logger.log("SymbolicPromptNewKey", {
                        "prompt": prompt_name,
                        "key": key,
                        "value": value,
                        "rule_id": rule.id,
                    })
                cfg[key] = value

            # Track the application of the prompt-level rule
            self.memory.rule_effects.insert(
                rule_id=rule.id,
                goal_id=goal.get("id"),
                pipeline_run_id=pipeline_run_id,
                agent_name=cfg.get("name", "prompt"),
                context_hash=self.compute_context_hash(context),
                run_id=context.get("run_id"),
            )

        return cfg

    def _matches_filter(self, filter_dict: dict, target_obj: dict) -> bool:
        """Generic matcher for symbolic rule filters"""
        for key, value in filter_dict.items():
            target_value = target_obj.get(key)
            if isinstance(value, list):
                if target_value not in value:
                    return False
            else:
                if target_value != value:
                    return False
        return True


    def track_pipeline_stage(self, stage_dict: dict, context: dict):
        self.memory.symbolic_rules.track_pipeline_stage(stage_dict, context)

    def _load_rules(self):
        rules = []
        symbolic_dict = self.cfg.get("symbolic", {})
        if symbolic_dict.get("rules_file"):
            rules += self._load_rules_from_yaml(symbolic_dict.rules_file)
        if symbolic_dict.get("enable_db_rules", True):
            rules += self.memory.symbolic_rules.get_all_rules()
        return rules

    def _load_rules_from_yaml(self, path: str) -> list:
        if not Path(path).exists():
            self.logger.log("SymbolicRuleYAMLNotFound", {"path": path})
            return []

        with open(path, "r", encoding="utf-8") as f:
            raw = yaml.safe_load(f)

        rules_list = raw.get("rules", raw)

        rules = []
        existing_rules = {
            r.rule_text for r in self.memory.symbolic_rules.get_all_rules()
        }
        for item in rules_list:
            if isinstance(item, dict) and item.get("rule_text") not in existing_rules:
                rules.append(SymbolicRuleORM(**item))
            else:
                self.logger.log(
                    "DuplicateSymbolicRuleSkipped", {"rule_text": item.get("rule_text")}
                )
        return rules

    def _matches_metadata(self, rule: SymbolicRuleORM, goal: Dict[str, Any]) -> bool:
        if rule.goal_id and rule.goal_id != goal.get("id"):
            return False
        if rule.goal_type and rule.goal_type != goal.get("goal_type"):
            return False
        if rule.goal_category and rule.goal_category != goal.get("goal_category"):
            return False
        if rule.difficulty and rule.difficulty != goal.get("difficulty"):
            return False
        if hasattr(goal, "focus_area") and rule.goal_category:
            if rule.goal_category != goal.get("focus_area"):
                return False
        return True

    @staticmethod
    def compute_context_hash(context_dict: dict) -> str:
        canonical_str = json.dumps(context_dict, sort_keys=True)
        return hashlib.sha256(canonical_str.encode("utf-8")).hexdigest()
---END-OF-FILE---


"co_ai\scoring\__init__.py"
---START-OF-FILE---
from .structured_engine import StructuredScoringEngine

---END-OF-FILE---


"co_ai\scoring\base_evaluator.py"
---START-OF-FILE---
# co_ai/scoring/base_evaluator.py
from abc import ABC, abstractmethod


class BaseEvaluator(ABC):
    @abstractmethod
    def evaluate(self, hypothesis: dict, context: dict = None) -> dict:
        """Returns a structured score dict with score, rationale, etc."""
        pass
---END-OF-FILE---


"co_ai\scoring\base_score.py"
---START-OF-FILE---
from abc import ABC, abstractmethod

from co_ai.models import EvaluationORM


class BaseScore(ABC):
    name: str = "unnamed"
    default_value: float = 0.0

    def __init__(self, cfg, memory, logger, evaluator_name=None):
        self.memory = memory
        self.logger = logger
        self.agent_name = cfg.get("name")
        self.model_name = cfg.get("model", {}).get("name")
        self.evaluator_name = evaluator_name or self.name

    @abstractmethod
    def compute(self, hypothesis: dict, context:dict) -> float:
        pass

    def get_score(self, hypothesis: dict, context: dict) -> float:
        # 1. If already cached on object
        if hypothesis.get(f"{self.name}_score"):
            return hypothesis[f"{self.name}_score"]

        # 2. Compute and attach
        score = self.compute(hypothesis, context)
        hypothesis[f"{self.name}_score"] = score

        # 3. Store in scores table
        if self.memory:
            # Optional dimensions dict (can be overridden in subclass)
            dimensions = getattr(self, "dimensions", None)

            s = EvaluationORM(
                goal_id=hypothesis.get("goal_id"),
                hypothesis_id=hypothesis.get("id"),
                agent_name=self.agent_name,
                model_name=self.model_name,
                evaluator_name=self.evaluator_name,
                scores=dimensions,
                pipeline_run_id=context.get("pipeline_run_id"),
            )
            try:
                self.memory.evaluations.insert(s)
                self.memory.commit()  # Ensure session commit happens
            except Exception as e:
                self.memory.refresh_session()
                score = self.default_value
                self.logger.log("ScoreInsertFailed", {"error": str(e)})

        # 4. Log
        self.logger.log("ScoreComputed", {
            "type": self.name,
            "score": score,
            "hypothesis_id": hypothesis.get("id")
        })

        return score
---END-OF-FILE---


"co_ai\scoring\batch.py"
---START-OF-FILE---
from co_ai.agents.pipeline_judge import PipelineJudgeAgent
from co_ai.constants import GOAL, PIPELINE_RUN_ID, RUN_ID
from co_ai.models import EvaluationORM, HypothesisORM, RuleApplicationORM


def get_unscored_hypotheses(session, run_id: str = None):
    # Get hypotheses with no evaluations for this run
    subquery = session.query(EvaluationORM.hypothesis_id).distinct()
    query = session.query(HypothesisORM).filter(~HypothesisORM.id.in_(subquery))

    if run_id:
        query = query.filter(HypothesisORM.pipeline_run_id == run_id)

    return query.all()

async def score_unscored_hypotheses(memory, logger, config, run_id=None):
    session = memory.session
    unscored = get_unscored_hypotheses(session, run_id)
    agent = PipelineJudgeAgent(cfg=config, memory=memory, logger=logger)

    for hypo in unscored:
        goal = memory.goals.get_by_id(hypo.goal_id)
        rule_apps = memory.rule_effects.get_by_hypothesis(hypo.id)

        context = {
            GOAL: goal.to_dict(),
            "hypotheses": [hypo.to_dict()],
            PIPELINE_RUN_ID: hypo.pipeline_run_id,
            RUN_ID: f"batch-repair-{hypo.id}",
            "rule_applications": [ra.to_dict() for ra in rule_apps],
        }

        logger.log("ScoringUnscoredHypothesis", {
            "hypothesis_id": hypo.id,
            "goal_id": goal.id,
            "rule_count": len(rule_apps),
        })

        await agent.run(context)
---END-OF-FILE---


"co_ai\scoring\proximity.py"
---START-OF-FILE---
# co_ai/scoring/proximity.py
import re

from co_ai.scoring.base_evaluator import BaseEvaluator


class ProximityHeuristicEvaluator(BaseEvaluator):
    def evaluate(self, hypothesis: dict, context: dict = {}) -> dict:
        analysis = hypothesis.get("proximity_analysis")
        if not analysis:
            return {
                "score": 0.0,
                "rationale": "No proximity analysis available.",
                "themes": [],
                "grafts": [],
                "directions": [],
            }

        try:
            themes = self._extract_block(analysis, "Common Themes Identified")
            grafts = self._extract_block(analysis, "Grafting Opportunities")
            directions = self._extract_block(analysis, "Strategic Directions")
            score = self._heuristic_score(themes, grafts, directions)
            justification = self._generate_justification(themes, grafts, directions)

            return {
                "score": score,
                "rationale": justification,
                "themes": themes,
                "grafts": grafts,
                "directions": directions,
            }

        except Exception as e:
            return {
                "score": 0.0,
                "rationale": f"Failed to parse proximity analysis: {str(e)}",
                "themes": [],
                "grafts": [],
                "directions": [],
            }

    def _extract_block(self, text: str, section_title: str) -> list:
        pattern = rf"# {re.escape(section_title)}\n((?:- .+\n?)*)"
        match = re.search(pattern, text)
        if not match:
            return []
        block = match.group(1).strip()
        return [line.strip("- ").strip() for line in block.splitlines() if line.strip()]

    def _generate_justification(self, themes, grafts, directions) -> str:
        return (
            f"Identified {len(themes)} themes, {len(grafts)} grafting suggestions, "
            f"and {len(directions)} strategic directions."
        )

    def _fallback(self, message: str):
        return {
            "score": 0.0,
            "rationale": message,
            "dimensions": {
                "proximity_themes": {"score": 0, "weight": 0.3, "rationale": message},
                "proximity_grafts": {"score": 0, "weight": 0.3, "rationale": message},
                "proximity_directions": {"score": 0, "weight": 0.4, "rationale": message},
            }
        }

    def _heuristic_score(self, themes, grafts, directions) -> float:
        """
        Simple scoring heuristic based on the number of insights generated.
        """
        return min(100.0, 10 * len(themes) + 10 * len(grafts) + 20 * len(directions))
---END-OF-FILE---


"co_ai\scoring\structured_engine.py"
---START-OF-FILE---
# co_ai/scoring/structured_engine.py
import re
from string import Template


class StructuredScoringEngine:
    def __init__(self, dimensions, prompt_loader=None, cfg=None, logger=None):
        self.dimensions = dimensions
        self.prompt_loader = prompt_loader
        self.cfg = cfg or {}
        self.logger = logger

    def evaluate(self, hypothesis: dict, context: dict = {}, llm_fn=None) -> dict:
        if llm_fn is None:
            raise ValueError("You must provide an llm_fn (e.g., agent.call_llm)")

        results = {}
        for dim in self.dimensions:
            prompt = self._render_prompt(dim, hypothesis, context)
            response = llm_fn(prompt, context=context)
            try:
                score = dim["parser"](response)
            except Exception as e:
                score = 0.0
                if self.logger:
                    self.logger.log("ScoreParseError", {
                        "dimension": dim["name"],
                        "response": response,
                        "error": str(e)
                    })
            if self.logger:
                self.logger.log("StructuredDimensionEvaluated", {
                    "dimension": dim["name"],
                    "score": score,
                    "response": response
                })
            results[dim["name"]] = {
                "score": score,
                "rationale": response,
                "weight": dim.get("weight", 1.0),
            }

        results["final_score"] = self._aggregate(results)
        return results

    def _render_prompt(self, dim: dict, hypothesis: dict, context: dict) -> str:
        ctx = {"hypothesis": hypothesis, **context}
        if self.prompt_loader and dim.get("file"):
            return self.prompt_loader.from_file(file_name=dim["file"], config=self.cfg, context=ctx)
        else:
            return Template(dim["prompt_template"]).substitute(ctx)

    def _aggregate(self, results: dict) -> float:
        total = 0.0
        weight_sum = 0.0
        for dim, val in results.items():
            if not isinstance(val, dict):  # skip final_score key
                continue
            total += val["score"] * val.get("weight", 1.0)
            weight_sum += val.get("weight", 1.0)
        return round(total / weight_sum, 2) if weight_sum else 0.0

    @staticmethod
    def extract_score_from_last_line(response: str) -> float:
        lines = response.strip().splitlines()
        for line in reversed(lines):
            match = re.search(r"score:\s*(\d+(\.\d+)?)", line.strip(), re.IGNORECASE)
            if match:
                return float(match.group(1))
        return 0.0

    @staticmethod
    def parse_numeric_cor(response: str) -> float:
        match = re.search(r"<answer>\s*\[\[(\d+(?:\.\d+)?)\]\]\s*</answer>", response, re.IGNORECASE)
        if not match:
            raise ValueError(f"Could not extract numeric score from CoR-style answer: {response}")
        return float(match.group(1))

    @staticmethod
    def get_parser(extra_data):
        parser_type = extra_data.get("parser", "numeric")
        if parser_type == "numeric":
            return StructuredScoringEngine.extract_score_from_last_line
        if parser_type == "numeric_cor":
            return StructuredScoringEngine.parse_numeric_cor
        return lambda r: 0.0
---END-OF-FILE---


"co_ai\tools\__init__.py"
---START-OF-FILE---
"""Tools for inspecting or visualizing pipeline outputs"""
from .arxiv_tool import search_arxiv
from .embedding_tool import get_embedding
from .huggingface_tool import search_huggingface_datasets
from .view_ranking_trace import fetch_ranking_trace, plot_elo_evolution
from .web_search import WebSearchTool
from .wikipedia_tool import WikipediaTool
---END-OF-FILE---


"co_ai\tools\arxiv_tool.py"
---START-OF-FILE---
import arxiv


def search_arxiv(queries: list[str], max_results: int = 5) -> list[dict]:
    results = []
    for query in queries:
        search = arxiv.Search(query=query, max_results=max_results)
        for r in search.results():
            results.append({
                "query": query,
                "title": r.title,
                "summary": r.summary,
                "authors": [a.name for a in r.authors],
                "url": r.pdf_url
            })
    return results
---END-OF-FILE---


"co_ai\tools\cos_sim_tool.py"
---START-OF-FILE---
from typing import List, Tuple

import numpy as np


def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Compute cosine similarity between two vectors."""
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot / (norm1 * norm2 + 1e-8)  # Avoid division by zero


def get_top_k_similar(
    query: str,
    documents: List[str],
    memory,
    top_k: int = 5
) -> List[Tuple[str, float]]:
    """
    Compute similarity between query and each document, return top_k most similar.
    
    Args:
        query: The input query text.
        documents: A list of document strings.
        get_embedding: Callable that takes a string and returns a vector (np.ndarray).
        top_k: Number of top results to return.
    
    Returns:
        List of (document, similarity_score) tuples.
    """
    query_vec = memory.embedding.get_or_create(query)
    doc_vecs = [memory.embedding.get_or_create(doc) for doc in documents]

    similarities = [cosine_similarity(query_vec, vec) for vec in doc_vecs]
    scored = list(zip(documents, similarities))
    scored.sort(key=lambda x: x[1], reverse=True)

    return scored[:top_k]
---END-OF-FILE---


"co_ai\tools\ddgs_web_search.py"
---START-OF-FILE---
import asyncio

from duckduckgo_search import DDGS


class DDGSWebSearchTool:

    def search(self, query: str, max_results: int =5):
        return DDGS().text(query, max_results=max_results)

    async def search2(self, query: str, max_results: int = 5) -> list[str]:
        def _run_search():
            results = []
            with DDGS() as ddgs:
                for r in ddgs.text(query, max_results=max_results):
                    title = r.get("title", "").strip()
                    body = r.get("body", "").strip()
                    href = r.get("href", "")
                    results.append(f"{title}: {body}\n{href}")
            return results

        try:
            return await asyncio.to_thread(_run_search)
        except Exception as e:
            print(f"âŒ Exception [WebSearchTool] Search error: {type(e).__name__}: {e}")
            return [f"Search failed: {str(e)}"]
---END-OF-FILE---


"co_ai\tools\download_tool.py"
---START-OF-FILE---
# Import libraries
import requests


def download_pdf(url: str):
    response = requests.get(url)
    # Write content in pdf file
    pdf = open("pdf" + str(i) + ".pdf", "wb")
    pdf.write(response.content)
    pdf.close()
    print("File ", i, " downloaded")


---END-OF-FILE---


"co_ai\tools\embedding_tool.py"
---START-OF-FILE---
# co_ai/tools/embedding_tool.py
from collections import OrderedDict

import requests


# Simple in-memory LRU cache
class EmbeddingCache:
    def __init__(self, max_size=10000):
        self.cache = OrderedDict()
        self.max_size = max_size

    def get(self, key):
        if key in self.cache:
            # Move to the end to mark as recently used
            self.cache.move_to_end(key)
            return self.cache[key]
        return None

    def set(self, key, value):
        self.cache[key] = value
        self.cache.move_to_end(key)
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)  # Remove least recently used item

embedding_cache = EmbeddingCache(max_size=10000)

def get_embedding(text: str, cfg):
    """
    Get an embedding from Ollama using the configured model.

    Args:
        text (str): The input text to embed.
        cfg (dict)): Configuration containing 'model' and optionally 'endpoint'.

    Returns:
        list[float]: The embedding vector.
    """
    cached = embedding_cache.get(text)
    if cached is not None:
        print("ðŸ” Using cached embedding")
        return cached

    model = cfg.get("embeddings", {}).get("model", "mxbai-embed-large")
    endpoint = cfg.get("embeddings", {}).get("endpoint", "http://localhost:11434/api/embeddings")
    response = requests.post(
        endpoint,
        json={"model": model, "prompt": text},
    )
    response.raise_for_status()
    return response.json().get("embedding")
---END-OF-FILE---


"co_ai\tools\huggingface_tool.py"
---START-OF-FILE---
from huggingface_hub import HfApi
import re
import requests

def recommend_similar_papers(paper_url:str = "https://arxiv.org/pdf/2505.08827"):
    from gradio_client import Client
    client = Client("librarian-bots/recommend_similar_papers")
    result = client.predict(paper_url, None, False, api_name="/predict")
    print(result)
    paper_ids = re.findall(r"https://huggingface\.co/papers/(\d+\.\d+)", result)
    print(paper_ids)
    arxiv_pdf_urls = [f"https://arxiv.org/pdf/{pid}.pdf" for pid in paper_ids]
    print(arxiv_pdf_urls)
    for pid in paper_ids:
        url = f"https://arxiv.org/pdf/{pid}.pdf"
        response = requests.get(url)
        if response.status_code == 200:
            with open(f"{pid}.pdf", "wb") as f:
                f.write(response.content)
                print(f"Downloaded {pid}.pdf")
        else:
            print(f"Failed to download {pid}")
        return result

def search_huggingface_datasets(queries: list[str], max_results: int = 5) -> list[dict]:
    api = HfApi()
    results = []

    for query in queries:
        try:
            matches = api.list_datasets(search=query, limit=max_results)
            for ds in matches:
                results.append({
                    "name": ds.id,
                    "description": ds.cardData.get("description", "No description available") if ds.cardData else "No card data"
                })
        except Exception as e:
            results.append({
                "name": query,
                "description": f"Error searching: {str(e)}"
            })

    return results
---END-OF-FILE---


"co_ai\tools\pdf_tools.py"
---START-OF-FILE---
---END-OF-FILE---


"co_ai\tools\view_ranking_trace.py"
---START-OF-FILE---
# tools/view_ranking_trace.py
from collections import defaultdict

import matplotlib.pyplot as plt
import psycopg2
from tabulate import tabulate

DB_CONFIG = dict(
    dbname="co",
    user="co",
    password="co",
    host="localhost"
)

def fetch_ranking_trace(run_id=None):
    with psycopg2.connect(**DB_CONFIG) as conn:
        with conn.cursor() as cur:
            if run_id:
                cur.execute("""
                    SELECT winner, loser, explanation, created_at
                    FROM ranking_trace
                    WHERE run_id = %s
                    ORDER BY created_at
                """, (run_id,))
            else:
                cur.execute("""
                    SELECT winner, loser, explanation, created_at
                    FROM ranking_trace
                    ORDER BY created_at DESC LIMIT 50
                """)
            return cur.fetchall()

def fetch_elo_scores(run_id=None):
    with psycopg2.connect(**DB_CONFIG) as conn:
        with conn.cursor() as cur:
            if run_id:
                cur.execute("""
                    SELECT hypothesis, score, created_at
                    FROM elo_ranking_log
                    WHERE run_id = %s
                    ORDER BY created_at
                """, (run_id,))
            else:
                cur.execute("""
                    SELECT hypothesis, score, created_at
                    FROM elo_ranking_log
                    ORDER BY created_at DESC LIMIT 50
                """)
            return cur.fetchall()

def display_top_ranked(run_id=None):
    scores = fetch_elo_scores(run_id)
    latest_scores = {}
    for hypo, score, ts in scores:
        latest_scores[hypo] = score
    sorted_scores = sorted(latest_scores.items(), key=lambda x: x[1], reverse=True)
    print("\nTop-Ranked Hypotheses:\n")
    print(tabulate(sorted_scores, headers=["hypotheses", "ELO Score"], tablefmt="grid"))

def plot_elo_evolution(run_id=None):
    scores = fetch_elo_scores(run_id)
    time_series = defaultdict(list)
    for hypo, score, ts in scores:
        time_series[hypo].append((ts, score))

    plt.figure(figsize=(10, 6))
    for hypo, points in time_series.items():
        times, scores = zip(*points)
        plt.plot(times, scores, label=hypo[:40] + ("..." if len(hypo) > 40 else ""))

    plt.xlabel("Time")
    plt.ylabel("ELO Score")
    plt.title("ELO Evolution Over Time")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.xticks(rotation=45)
    plt.show()

def main():
    run_id = input("Enter run_id (or leave blank for latest): ").strip() or None
    print("\n--- Ranking Trace ---")
    rows = fetch_ranking_trace(run_id)
    print(tabulate(rows, headers=["Winner", "Loser", "Explanation", "Time"], tablefmt="grid"))

    print("\n--- Top-Ranked Hypotheses ---")
    display_top_ranked(run_id)

    print("\n--- Plotting ELO Evolution ---")
    plot_elo_evolution(run_id)

if __name__ == "__main__":
    main()
---END-OF-FILE---


"co_ai\tools\web_search.py"
---START-OF-FILE---
import asyncio

import httpx
import requests
from bs4 import BeautifulSoup
from readability import Document

from co_ai.utils.file_utils import write_text_to_file


class WebSearchTool:
    def __init__(self, cfg: dict, logger):
        self.base_url = f'{cfg.get("instance_url", "localhost:8080")}/search'
        self.max_results = cfg.get("max_results", 15)
        self.fetch_page = cfg.get("fetch_page", False)
        self.categories = cfg.get("categories", "general")
        self.language = cfg.get("language", "en")
        self.logger = logger

    async def search(self, query: str, max_results: int = 15) -> list[str] | None:
        max_results = max_results or self.max_results

        params = {
            "q": query,
            "categories": "general",
            "language": self.language,
            "formats": ["html", "json"]
        }

        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                resp = await client.get(self.base_url, params=params)
                resp.raise_for_status()
                html = resp.text

        except Exception as e:
            print(f"âŒ Exception:  {type(e).__name__}: {e}")
            return None

        return self.parse_searxng_results(html, max_results)

    from bs4 import BeautifulSoup

    def parse_searxng_results(self, html: str, max_results:int=20):
        soup = BeautifulSoup(html, "html.parser")
        results = []

        for i, article in enumerate(soup.find_all("article", class_="result")):
            if i > max_results:
                continue
            link_tag = article.find("a", class_="url_header")
            href = link_tag["href"] if link_tag else None

            title_tag = article.find("h3")
            title = title_tag.get_text(strip=True) if title_tag else None

            snippet_tag = article.find("p", class_="content")
            snippet = snippet_tag.get_text(strip=True) if snippet_tag else None

            cleand_page = ""
            if self.fetch_page:
                cleand_page = self.fetch_html(href)

            if href and title:
                results.append(
                    {
                        "title": title,
                        "url": href,
                        "snippet": snippet,
                        "page": cleand_page,
                    }
                )

        return results

    import requests

    def fetch_html(self, url: str) -> str | None:
        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            if self.logger:
                self.logger.log("FetchHTMLFailed", {"url": url, "error": str(e)})
            return None  # or return ""

    def fetch_and_parse_readable(self, url:str):
        html = self.fetch_html(url)
        title, clean_text = self.extract_main_text(html)
        return {"url": url, "title": title, "text": clean_text}


    def extract_main_text(self, html):
        doc = Document(html)
        title = doc.short_title()
        summary_html = doc.summary()

        # Use BeautifulSoup to clean text
        soup = BeautifulSoup(summary_html, 'html.parser')
        clean_text = soup.get_text(separator='\n', strip=True)
        return title, clean_text---END-OF-FILE---


"co_ai\tools\wikipedia_tool.py"
---START-OF-FILE---
import wikipedia

from co_ai.tools.cos_sim_tool import get_top_k_similar


class WikipediaTool:
    def __init__(self, memory, logger, lang="en", top_k=3):
        self.memory = memory
        self.logger = logger
        wikipedia.set_lang(lang)
        self.top_k = top_k

    def search(self, query: str) -> list[dict]:
        self.logger.log("WikipediaSearchStart", {"query": query})
        search_results = wikipedia.search(query)
        articles = []

        for title in search_results[:10]:
            try:
                page = wikipedia.page(title)
                summary = page.summary[:2000]
                article = {"title": title, "summary": summary, "url": page.url}
                articles.append(article)
                self.logger.log("WikipediaArticleFetched", {"article": article})
            except wikipedia.exceptions.DisambiguationError as e:
                self.logger.log("WikipediaDisambiguationSkipped", {"title": title})
                continue
            except Exception as e:
                self.logger.log(
                    "WikipediaFetchFailed", {"title": title, "error": str(e)}
                )
                continue

        self.logger.log(
            "WikipediaSearchComplete", {"query": query, "count": len(articles)}
        )
        return articles

    def find_similar(self, query: str) -> list[dict]:
        self.logger.log("WikipediaSimilaritySearchStart", {"query": query})
        raw_articles = self.search(query)
        if not raw_articles:
            self.logger.log("WikipediaNoResults", {"query": query})
            return []

        summaries = [a["summary"] for a in raw_articles]
        scored = get_top_k_similar(query, summaries, self.memory, top_k=self.top_k)
        self.logger.log(
            "WikipediaSimilarityScores",
            {"scores": [{"summary": s, "score": sc} for s, sc in scored]},
        )

        final = []
        for summary, score in scored:
            match = next((a for a in raw_articles if a["summary"] == summary), None)
            if match:
                result = match | {"score": round(score, 4)}
                final.append(result)
                self.logger.log("WikipediaMatchSelected", {"result": result})

        self.logger.log(
            "WikipediaSimilaritySearchComplete", {"query": query, "top_k": len(final)}
        )
        return final
---END-OF-FILE---


"co_ai\utils\__init__.py"
---START-OF-FILE---
"""
Utility classes
- prompt_loader
- report_formatter
"""
from .file_utils import camel_to_snake, get_text_from_file, write_text_to_file
from .goal_classifier import classify_goal_strategy
from .lru_cache import SimpleLRUCache
from .resource_extractor import extract_resources
from .run_utils import generate_run_id, get_log_file_path
from .similarity_utils import compute_similarity_matrix
---END-OF-FILE---


"co_ai\utils\file_utils.py"
---START-OF-FILE---
import re


def camel_to_snake(name):
    s1 = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name)
    return re.sub("([a-z0-9])([A-Z])", r"\1_\2", s1).lower()

def get_text_from_file(file_path: str) -> str:
    """Get text from a file"""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()

def write_text_to_file(path: str, text: str):
    try:
        with open(path, 'w', encoding='utf-8') as file:
            file.write(text)
        print(f"âœ… Successfully wrote to {path}")
    except Exception as e:
        print(f"âŒ Failed to write to {path}: {e}")
---END-OF-FILE---


"co_ai\utils\goal_classifier.py"
---START-OF-FILE---
def classify_goal_strategy(goal: dict) -> str:
    desc = goal.get("goal_text", "").lower()
    kw = " ".join(goal.get("keywords", [])).lower()

    if "dataset" in desc or "huggingface" in kw:
        return "dataset-first"
    elif "accuracy" in desc or "evaluation" in kw:
        return "evaluation-centric"
    elif "explain" in desc or "interpret" in kw:
        return "interpretability"
    elif "code" in desc or "repo" in kw:
        return "code-centric"
    elif "survey" in desc or "literature" in kw:
        return "deep_literature"
    else:
        return "default"
---END-OF-FILE---


"co_ai\utils\graph_tools.py"
---START-OF-FILE---
# co_ai/utils/graph_tools.py

def build_mermaid_graph(node, depth=0, max_depth=5, visited=None):
    """
    Recursively builds a Mermaid graph from a reasoning tree node.
    Returns a list of Mermaid-compatible graph lines.
    """
    if visited is None:
        visited = set()
    
    if depth > max_depth or id(node) in visited:
        return []

    visited.add(id(node))
    mermaid = []

    # Node ID
    node_id = f"N{node.get('id', id(node))}"

    # Extract score (fallback to reward or 0.0)
    score = node.get("score", node.get("reward", 0.0))

    # Extract last trace item (handle various types)
    trace = node.get("trace", [])
    if isinstance(trace, str):
        trace = trace.split("\n")
    elif not isinstance(trace, list):
        trace = []

    if trace:
        last_action = trace[-1][:30]
    else:
        state = node.get("state", {})
        goal_text = state.get("goal", "Root") if isinstance(state, dict) else "Root"
        last_action = goal_text[:30]

    # Label assembly
    label = f"{last_action[:20]}..."
    label += f" | Score: {score:.2f}"
    if node.get("is_terminal", False):
        label += " | TERMINAL"

    # Define the node visually
    mermaid.append(f'{node_id}["{label}"]')

    # Style node based on score
    if node.get("is_terminal", False):
        mermaid.append(f"style {node_id} fill:#e6ccff,stroke:#333")
    elif score > 0.8:
        mermaid.append(f"style {node_id} fill:#a8edc9,stroke:#333")
    elif score > 0.5:
        mermaid.append(f"style {node_id} fill:#f4f4f4,stroke:#333")
    else:
        mermaid.append(f"style {node_id} fill:#fddddd,stroke:#333")

    # Recursively add children
    children = node.get("children", [])
    for child in children[:3]:  # Limit to 3 branches
        child_lines = build_mermaid_graph(child, depth + 1, max_depth, visited)
        if child_lines:
            mermaid.extend(child_lines)
            child_id = f"N{child.get('id', id(child))}"
            mermaid.append(f"{node_id} --> {child_id}")

    return mermaid

def save_mermaid_to_file(diagram, filename="search_tree.mmd"):
    with open(filename, "w") as f:
        f.write("```mermaid\ngraph TD\n")
        f.write(diagram + "\n")
        f.write("```\n")

def compare_graphs(graph1, graph2):
    """
    Returns:
        matches: list of nodes present in both
        only_1: list of nodes only in graph1
        only_2: list of nodes only in graph2
    """
    # Ensure graphs contain full node dicts
    if not isinstance(graph1[0], dict) or not isinstance(graph2[0], dict):
        raise ValueError("compare_graphs() requires full node dicts")

    set1 = {n["id"]: n for n in graph1}
    set2 = {n["id"]: n for n in graph2}

    matches = [set1[k] for k in set1 if k in set2]
    only_1 = [n for n in graph1 if n["id"] not in set2]
    only_2 = [n for n in graph2 if n["id"] not in set1]

    return matches, only_1, only_2

def analyze_graph_impact(graph1, graph2, score_lookup_fn):
    """
    Returns a list of dictionaries summarizing overlap and score delta.
    """
    matches, only_1, only_2 = compare_graphs(graph1, graph2)
    results = []

    for node in matches:
        score1 = score_lookup_fn(node, source="graph1")
        score2 = score_lookup_fn(node, source="graph2")
        results.append({
            "node": node,
            "type": "match",
            "delta": score2 - score1
        })

    for node in only_1:
        results.append({
            "node": node,
            "type": "only_graph1",
            "score": score_lookup_fn(node, source="graph1")
        })

    for node in only_2:
        results.append({
            "node": node,
            "type": "only_graph2",
            "score": score_lookup_fn(node, source="graph2")
        })

    return results
---END-OF-FILE---


"co_ai\utils\high_score_selector.py"
---START-OF-FILE---
import json
from collections import defaultdict

from sqlalchemy.orm import joinedload

from co_ai.models.evaluation import EvaluationORM
from co_ai.models.pipeline_run import PipelineRunORM
from co_ai.models.rule_application import RuleApplicationORM
from co_ai.models.score import ScoreORM


def get_high_scoring_runs(session, dimension: str, threshold: float, min_repeat_count: int = 2):
    """
    Returns a dict of {signature: list of (evaluation, run)} for all runs that score above the threshold
    on a given dimension.
    """
    # Step 1: Query evaluations with score above threshold
    evaluations = (
        session.query(EvaluationORM)
        .join(ScoreORM, EvaluationORM.id == ScoreORM.evaluation_id)
        .filter(ScoreORM.dimension == dimension)
        .filter(ScoreORM.score >= threshold)
        .options(joinedload(EvaluationORM.dimension_scores))
        .all()
    )

    # Step 2: Filter out those with rule applications
    valid_runs = []
    for evaluation in evaluations:
        rule_applied = (
            session.query(RuleApplicationORM)
            .filter_by(hypothesis_id=evaluation.hypothesis_id)
            .first()
        )
        if rule_applied:
            continue
        run = session.get(PipelineRunORM, evaluation.pipeline_run_id)
        if run:
            valid_runs.append((evaluation, run))

    # Step 3: Group by config signature
    grouped = defaultdict(list)
    for evaluation, run in valid_runs:
        sig = make_signature(run.run_config or {})
        grouped[sig].append((evaluation, run))

    # Step 4: Filter by minimum repetition
    return {
        sig: entries
        for sig, entries in grouped.items()
        if len(entries) >= min_repeat_count
    }


def make_signature(config):
    if isinstance(config, str):
        try:
            config = json.loads(config)
        except json.JSONDecodeError:
            config = {}

    model = config.get("model", {}).get("name", "unknown")
    agent = config.get("agent", "unknown")
    goal_type = config.get("goal", {}).get("goal_type", "unknown")

    return f"{model}::{agent}::{goal_type}"---END-OF-FILE---


"co_ai\utils\loaders.py"
---START-OF-FILE---
# co_ai/utils/loaders.py

import os

from omegaconf import OmegaConf

from co_ai.logs import JSONLogger
from co_ai.memory import MemoryTool


def get_memory(config_name:str="db/postgres") -> MemoryTool:
    cfg = get_config(config_name)
    print(f"Loading memory with config: {cfg}")
    """Initialize and return a default Memory instance."""
    return MemoryTool(cfg, JSONLogger(log_path="memory_log.jsonl"))


def get_logger(file_path: str = "log.jsonl") -> object:
    """Return a logger instance with the specified name."""
    return JSONLogger(file_path)


def get_config(config_name: str ="config.yaml"):
    """
    Load a Hydra-style YAML config from the configs/agents directory.
    Example: get_config("lats") -> loads configs/agents/lats.yaml
    """
    path = os.path.join("config", f"{config_name}.yaml")
    return OmegaConf.to_container(OmegaConf.load(path), resolve=True, throw_on_missing=True)
---END-OF-FILE---


"co_ai\utils\lru_cache.py"
---START-OF-FILE---
from collections import OrderedDict


class SimpleLRUCache:
    def __init__(self, max_size=10000):
        self.cache = OrderedDict()
        self.max_size = max_size

    def get(self, key):
        if key in self.cache:
            # Move to the end to show it was recently used
            self.cache.move_to_end(key)
            return self.cache[key]
        return None

    def set(self, key, value):
        if key in self.cache:
            # Update value and mark as recently used
            self.cache.move_to_end(key)
            self.cache[key] = value
        else:
            # Evict least recently used if full
            if len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)
            self.cache[key] = value
---END-OF-FILE---


"co_ai\utils\query_generator.py"
---START-OF-FILE---
class GoalQueryGenerator:
    def generate_queries(self, goal: dict, strategy: str) -> list[str]:
        desc = goal.get("goal_text", goal.get("description", ""))
        kw = goal.get("keywords", [])

        if strategy == "deep_literature":
            return kw + [desc + " site:arxiv.org"]
        elif strategy == "dataset-first":
            return kw + [f"Hugging Face dataset {desc}"]
        elif strategy == "code-centric":
            return kw + [f"GitHub {desc}"]
        else:
            return kw or [desc]
---END-OF-FILE---


"co_ai\utils\resource_extractor.py"
---START-OF-FILE---
# co_ai/utils/resource_extractor.py
import os
from pathlib import Path
from typing import List

import pkg_resources

RESOURCE_MAP = {
    "configs": "resources/configs",
    "prompts": "resources/prompts",
    "docker": "resources/docker"
}


def extract_resources(target_dir: str = None):
    """
    Extract all bundled configs and prompts into local directories.
    
    Args:
        target_dir: Optional override of where to extract files
    """
    target_dir = Path(target_dir or os.getcwd())
    
    for resource_type, source_path in RESOURCE_MAP.items():
        src = Path(source_path)
        dest = target_dir / resource_type
        
        # Skip if already exists
        if dest.exists():
            print(f"[ResourceExtractor] {resource_type} folder found. Skipping.")
            continue
            
        print(f"[ResourceExtractor] Copying {resource_type}...")
        dest.mkdir(exist_ok=True)

        # Walk through resource tree
        for root, _, files in os.walk(pkg_resources.resource_filename(__name__, src)):
            rel_root = Path(root).relative_to(pkg_resources.resource_filename(__name__, ""))
            for f in files:
                src_file = rel_root / f
                dst_file = dest / rel_root.relative_to(resource_type) / f
                
                dst_file.parent.mkdir(parents=True, exist_ok=True)
                
                with open(src_file, "rb") as fin:
                    content = fin.read().decode("utf-8")

                with open(dst_file, "w", encoding="utf-8") as fout:
                    fout.write(content)

    print("[ResourceExtractor] Done. Resources extracted to:")
    print(f" - Configs: {target_dir / 'configs'}")
    print(f" - Prompts: {target_dir / 'prompts'}")---END-OF-FILE---


"co_ai\utils\run_utils.py"
---START-OF-FILE---
import os
import re
import uuid
from datetime import datetime, timezone

from omegaconf import DictConfig


def generate_run_id(goal: str) -> str:
    # Extract keywords from goal
    keywords = re.findall(r'\b\w{5,}\b', goal.lower())  # words with 5+ letters
    keywords = keywords[:2] if keywords else ['run']

    # Sanitize and slugify
    slug = "_".join(keywords)
    slug = re.sub(r'[^a-z0-9_]+', '', slug)

    # Add timestamp and short UUID
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S")
    short_uuid = uuid.uuid4().hex[:6]

    return f"{slug}_{timestamp}_{short_uuid}"

def get_log_file_path(run_id:str, cfg: DictConfig) -> str:
    # Get the path to the log file
    if cfg.logging.logger.get("log_file", None):
        print(f"Log file path: {cfg.logging.logger.log_file}")
        return cfg.logging.logger.log_file
    
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S")
    safe_run_id = re.sub(r"[\\W_]+", "_", run_id)  # remove/replace unsafe chars
    log_filename = f"{safe_run_id}_{timestamp}.jsonl"
    os.makedirs(cfg.logging.logger.log_path, exist_ok=True)
    log_file_path = os.path.join(cfg.logging.logger.log_path, log_filename)
    print(f"Log file path: {log_file_path}")
    return log_file_path

---END-OF-FILE---


"co_ai\utils\similarity_utils.py"
---START-OF-FILE---
# utils/similarity_utils.py
import itertools

import numpy as np


def compute_similarity_matrix(hypotheses: list[str], memory, logger) -> list[tuple[str, str, float]]:
    vectors = []
    valid_hypotheses = []
    for h in hypotheses:
        vec = memory.embedding.get_or_create(h)
        if vec is None:
            logger.log("MissingEmbedding", {"hypothesis_snippet": h[:60]})
            continue
        vectors.append(vec)
        valid_hypotheses.append(h)

    similarities = []
    for i, j in itertools.combinations(range(len(valid_hypotheses)), 2):
        h1 = valid_hypotheses[i]
        h2 = valid_hypotheses[j]
        sim = float(np.dot(vectors[i], vectors[j]) / (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j])))
        similarities.append((h1, h2, sim))

    return sorted(similarities, key=lambda x: x[2], reverse=True)
---END-OF-FILE---


"co_ai\utils\timing.py"
---START-OF-FILE---
import time
from functools import wraps
from typing import Any, Callable


def time_function(logger=None):
    """
    Decorator factory that logs execution time
    Usage: @time_function(logger=self.logger)
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            start = time.perf_counter()
            result = func(*args, **kwargs)
            duration = time.perf_counter() - start
            
            # Extract object context
            obj = args[0] if args and hasattr(args[0], '__class__') else None
            class_name = obj.__class__.__name__ if obj else "Function"
            
            # Log structured data
            log_data = {
                "function": func.__name__,
                "class": class_name,
                "duration_ms": round(duration * 1000, 2),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # Add trace context if available
            if obj and hasattr(obj, 'trace'):
                log_data["trace_length"] = len(getattr(obj, 'trace', []))
            
            # Use logger or print fallback
            if logger:
                logger.log("FunctionTiming", log_data)
            else:
                print(f"ðŸ•’ {class_name}.{func.__name__}: {log_data['duration_ms']}ms [{log_data['timestamp']}]")
            
            return result
        return wrapper
    return decorator


class TimingAnalyzer:
    def __init__(self, logger):
        self.logger = logger
    
    def analyze(self, event_type="FunctionTiming"):
        logs = self.logger.get_logs_by_type(event_type)
        
        # Group by function
        from collections import defaultdict
        function_times = defaultdict(list)
        for log in logs:
            data = log["data"]
            key = f"{data.get('class', '')}.{data.get('function', '')}"
            function_times[key].append(data["duration_ms"])
        
        return {
            "avg_times": {k: sum(v)/len(v) for k, v in function_times.items()},
            "total_calls": {k: len(v) for k, v in function_times.items()},
            "max_times": {k: max(v) for k, v in function_times.items()}
        }---END-OF-FILE---


"co_ai\__init__.py"
---START-OF-FILE---
"""
AI Co-Scientist Package

A modular pipeline for hypothesis generation, evaluation, and summarization using DSPy, pgvector, and Ollama.
"""
__version__ = "0.1.0"
---END-OF-FILE---


"co_ai\cli.py"
---START-OF-FILE---
import logging

import click

from co_ai.utils import extract_resources


@click.group()
def cli():
    """AI Co-Scientist CLI"""
    pass


@cli.command()
def init():
    """Initialize config and prompt files from embedded resources"""
    try:
        extract_resources()
        click.echo("[+] Successfully extracted configs and prompts")
    except Exception as e:
        click.echo(f"[-] Failed to extract resources: {e}")
        raise


if __name__ == "__main__":
    cli()---END-OF-FILE---


"co_ai\constants.py"
---START-OF-FILE---
# co_ai/constants.py

# ==== Context Keys ====
AGENT = "Agent"
API_BASE = "api_base"
API_KEY = "api_key"
BATCH_SIZE = "batch_size"
CONTEXT = "context"
DEFAULT = "default"
DETAILS = "details"
EVOLVED = "evolved"
FEEDBACK = "feedback"
FILE = "file"
GOAL = "goal"
GOAL_TYPE = "goal_type"
GOAL_TEXT = "goal_text"
HYPOTHESES = "hypotheses"
DATABASE_MATCHES = "database_matches"
TEXT = "text"
LITERATURE = "literature"
LOOKAHEAD = "lookahead"
MODEL = "model"
NAME = "name"
PIPELINE = "pipeline"
PROXIMITY = "proximity"
RANKING = "ranking"
REFLECTION = "reflection"
REVIEW = "review"
REVIEWS = "reviews"
RUN_ID = "run_id"
PIPELINE_RUN_ID="pipeline_run_id"
SCORE = "score"
SOURCE = "source"
STAGE = "stage"
STRATEGY = "strategy"
VERSION = "version"

# ==== Config Keys ====
DATABASE = "database"
EVENT = "event"
INPUT_KEY = "input_key"
OUTPUT_KEY = "output_key"
PROMPT_DIR = "prompt_dir"
PROMPT_FILE = "prompt_file"
PROMPT_MATCH_RE = "prompt_match_re"
PROMPT_MODE = "prompt_mode"
PROMPT_PATH = "prompt_path"
SAVE_CONTEXT = "save_context"
SAVE_PROMPT = "save_prompt"
SKIP_IF_COMPLETED = "skip_if_completed"
---END-OF-FILE---


"co_ai\main.py"
---START-OF-FILE---
# main.py
import asyncio
import json
import logging
from datetime import datetime

import hydra
import yaml
from omegaconf import DictConfig, OmegaConf

from co_ai.logs import JSONLogger
from co_ai.memory import MemoryTool
from co_ai.supervisor import Supervisor
from co_ai.utils import generate_run_id, get_log_file_path


@hydra.main(version_base=None, config_path="../config", config_name="config")
def run(cfg: DictConfig):
    async def main():
        print(f"Initial Config:\n{OmegaConf.to_yaml(cfg)}")

        # Setup logger and memory
        run_id = generate_run_id(cfg.goal.goal_text if "goal" in cfg else "batch")
        log_path = get_log_file_path(run_id, cfg)
        logger = JSONLogger(log_path=log_path)
        memory = MemoryTool(cfg=cfg.db, logger=logger)

        supervisor = Supervisor(cfg=cfg, memory=memory, logger=logger)

        # âœ… Batch Mode: input_file provided
        if "input_file" in cfg and cfg.input_file:
            print(f"ðŸ“‚ Batch mode: Loading from file: {cfg.input_file}")
            result = await supervisor.run_pipeline_config({"input_file": cfg.input_file})
            print(f"âœ… Batch run completed for file: {cfg.input_file}: {str(result)[:100]}")
            return

        # âœ… Single goal mode
        print(f"ðŸŸ¢ Running pipeline with run_id={run_id}")
        print(f"ðŸ§  Goal: {cfg.goal}")
        print(f"ðŸ“ Config source: {str(cfg)[:100]}...")

        goal = OmegaConf.to_container(cfg.goal, resolve=True)
        context = {
            "goal": goal,
            "run_id": run_id,
        }

        result = await supervisor.run_pipeline_config(context)

        save_json_result(log_path, result)

        if cfg.report.generate_report:
            supervisor.generate_report(result, run_id=run_id)

    asyncio.run(main())


def save_yaml_result(log_path: str, result: dict):
    report_path = log_path.replace(".jsonl", ".yaml")
    with open(report_path, "w", encoding="utf-8") as f:
        yaml.dump(result, f, allow_unicode=True, sort_keys=False)
    print(f"âœ… Result saved to: {report_path}")

def default_serializer(obj):
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")

def save_json_result(log_path: str, result: dict):
    report_path = log_path.replace(".jsonl", "_report.json")
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=default_serializer)
    print(f"âœ… JSON result saved to: {report_path}")

if __name__ == "__main__":
    # Suppress HTTPX logs
    logging.getLogger("httpx").setLevel(logging.WARNING)
    # Suppress LiteLLM logs
    logging.getLogger("LiteLLM").setLevel(logging.WARNING)

    run()
---END-OF-FILE---


"co_ai\supervisor.py"
---START-OF-FILE---
# co_ai/supervisor.py

import json
import os
from datetime import datetime, timezone
from uuid import uuid4

import hydra
from omegaconf import DictConfig, OmegaConf

from co_ai.constants import (GOAL, NAME, PIPELINE, PIPELINE_RUN_ID, PROMPT_DIR,
                             RUN_ID, SAVE_CONTEXT, SKIP_IF_COMPLETED, STAGE)
from co_ai.logs.json_logger import JSONLogger
from co_ai.memory import MemoryTool
from co_ai.reports import ReportFormatter
from co_ai.rules.symbolic_rule_applier import SymbolicRuleApplier
from co_ai.registry import AgentRegistry

class PipelineStage:
    def __init__(self, name: str, config: dict, stage_dict: dict):
        self.name = name
        self.cls = config.get("cls", "")
        self.enabled = config.get("enabled", True)
        self.iterations = config.get("iterations", 1)
        self.stage_dict = stage_dict

class SingleAgentPipeline:
    def __init__(self, agent_name, config):
        self.agent = AgentRegistry(config).get(agent_name)
        self.goal_input = config.input_path

    def run(self):
        pass
        # goals = load_goal_list(self.goal_input)
        # for goal in goals:
        #     result = self.agent.run(goal)
            # wrap it in pipeline logs, score evals, etc.


class Supervisor:
    def __init__(self, cfg, memory=None, logger=None):
        self.cfg = cfg
        self.memory = memory or MemoryTool(cfg=cfg.db, logger=logger)
        self.logger = logger or JSONLogger(log_path=cfg.logger.log_path)
        self.logger.log("SupervisorInit", {"cfg": cfg})
        self.rule_applier = SymbolicRuleApplier(cfg, self.memory, self.logger)
        # Parse pipeline stages from config
        self.pipeline_stages = self._parse_pipeline_stages(cfg.pipeline.stages)

    def _parse_pipeline_stages(
        self, stage_configs: list[dict[str, any]]
    ) -> list[PipelineStage]:
        """Parse and validate pipeline stages from config."""
        stages = []
        for stage_config in stage_configs:
            name = stage_config.name
            if not stage_config.enabled:
                print(f"Skipping disabled stage: {name}")
                continue
            stage_dict = self.cfg.agents[name]
            self.logger.log("StageContext", {"stage_dict": stage_dict})
            stages.append(PipelineStage(name, stage_config, stage_dict))
        return stages

    async def run_pipeline_config(self, input_data: dict) -> dict:
        """
        Run all stages defined in config.
        Each stage loads its class dynamically via hydra.utils.get_class()
        """
        self.logger.log("PipelineStart", input_data)
        input_file = input_data.get("input_file", self.cfg.get("input_file", None))

        if input_file and os.path.exists(input_file):
            self.logger.log("BatchProcessingStart", {"file": input_file})
            with open(input_file, "r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    goal_dict = json.loads(line)
                    goal_orm = self.memory.goals.get_or_create(goal_dict)

                    run_id = goal_orm.get("id", f"goal_{i}")
                    context = {
                        GOAL: goal_dict,
                        RUN_ID: run_id,
                        "prompt_dir": self.cfg.paths.prompts,
                        PIPELINE: [stage.name for stage in self.pipeline_stages],
                    }
                    try:
                        await self._run_pipeline_stages(context)
                    except Exception as e:
                        self.logger.log(
                            "BatchItemFailed",
                            {"index": i, "run_id": run_id, "error": str(e)},
                        )
            self.logger.log("BatchProcessingComplete", {"file": input_file})
            return {"status": "completed_batch", "input_file": input_file}

        goal_dict = self.get_goal(input_data)
        run_id = str(uuid4())
        pipeline_list = [stage.name for stage in self.pipeline_stages]

        context = input_data.copy()
        context.update(
            {
                RUN_ID: run_id,
                PIPELINE: pipeline_list,
                PROMPT_DIR: self.cfg.paths.prompts,
                "goal": goal_dict,
            }
        )

        # Create and store PipelineRun
        pipeline_run_data = {
            "name": self.cfg.get("pipeline", {}).get(NAME, "UnnamedPipelineRun"),
            "tag":  self.cfg.get("pipeline", {}).get("tag", "default"),
            "description": self.cfg.get("pipeline", {}).get("description", ""),
            "goal_id": goal_dict.get("id"),
            "run_id": run_id,
            "pipeline": pipeline_list,  # Should be list of strings like ["generation", "judge"]
            "strategy": context.get("strategy"),
            "model_name": self.cfg.get("model.name", "unknown"),
            "run_config": OmegaConf.to_container(self.cfg),
            "created_at": datetime.now(timezone.utc).isoformat(),
        }

        # Insert into DB
        run_id = self.memory.pipeline_runs.insert(pipeline_run_data)
        context[PIPELINE_RUN_ID] = run_id

        # Now allow lookahead or other steps to adjust context
        context = await self.maybe_adjust_pipeline(context)
        context = self.rule_applier.apply(context)
        return await self._run_pipeline_stages(context)

    def _parse_pipeline_stages_from_list(
        self, stage_names: list[str]
    ) -> list[PipelineStage]:
        return [
            PipelineStage(name, self.cfg.pipeline.stages[name], self.cfg.agents[name])
            for name in stage_names
            if name in self.cfg.agents
        ]

    async def _run_pipeline_stages(self, context: dict) -> dict:
        for stage in self.pipeline_stages:
            if not stage.enabled:
                self.logger.log(
                    "PipelineStageSkipped",
                    {STAGE: stage.name, "reason": "disabled_in_config"},
                )
                continue

            cls = hydra.utils.get_class(stage.cls)
            stage_dict = OmegaConf.to_container(stage.stage_dict, resolve=True)
            self.rule_applier.apply_to_agent(stage_dict, context)

            saved_context = self.load_context(stage_dict, run_id=context.get(RUN_ID))
            if saved_context:
                self.logger.log(
                    "PipelineStageSkipped",
                    {STAGE: stage.name, "reason": "context_loaded"},
                )
                context = {**context, **saved_context}
                continue

            agent = cls(cfg=stage_dict, memory=self.memory, logger=self.logger)

            self.logger.log("PipelineStageStart", {STAGE: stage.name})

            for i in range(stage.iterations):
                self.logger.log(
                    "PipelineIterationStart", {STAGE: stage.name, "iteration": i + 1}
                )
                context = await agent.run(context)
                self.rule_applier.track_pipeline_stage(stage_dict, context)
                self.logger.log(
                    "PipelineIterationEnd", {STAGE: stage.name, "iteration": i + 1}
                )

            self.save_context(stage_dict, context)
            self.logger.log("PipelineStageEnd", {STAGE: stage.name})
            self.logger.log(
                "ContextAfterStage",
                {STAGE: stage.name, "context_keys": list(context.keys())},
            )

            # After final stage
            if self.cfg.get("post_judgment", {}).get("enabled", False):
                judge_cfg = OmegaConf.to_container(self.cfg.post_judgment, resolve=True)
                stage_dict = OmegaConf.to_container(
                    self.cfg.agents.pipeline_judge, resolve=True
                )
                judge_cls = hydra.utils.get_class(judge_cfg["cls"])
                judge_agent = judge_cls(
                    cfg=stage_dict, memory=self.memory, logger=self.logger
                )
                context = await judge_agent.run(context)
                self.rule_applier.track_pipeline_stage(stage_dict, context)

        return context

    def generate_report(self, context: dict[str, any], run_id: str) -> str:
        """Generate a report based on the pipeline context."""
        formatter = ReportFormatter(self.cfg.report.path)
        report = formatter.format_report(context)
        # self.memory.report.log(
        #     run_id, str(context.get("goal")), report, self.cfg.report.path
        # )
        self.logger.log(
            "ReportGenerated", {RUN_ID: run_id, "report_snippet": report[:100]}
        )
        return report

    def save_context(self, cfg: DictConfig, context: dict):
        if self.memory and cfg.get(SAVE_CONTEXT, False):
            run_id = context.get(RUN_ID)
            name = cfg.get(NAME, "NoAgentNameInConfig")
            self.memory.context.save(run_id, name, context, cfg)
            self.logger.log(
                "ContextSaved",
                {NAME: name, RUN_ID: run_id, "context_keys": list(context.keys())},
            )

    def load_context(self, cfg: DictConfig, run_id: str):
        if self.memory and cfg.get(SKIP_IF_COMPLETED, False):
            name = cfg.get(NAME, None)
            if name and self.memory.context.has_completed(run_id, name):
                saved_context = self.memory.context.load(run_id, name)
                if saved_context:
                    self.logger.log("ContextLoaded", {RUN_ID: run_id, NAME: name})
                    return saved_context
        return None

    async def maybe_adjust_pipeline(self, context: dict) -> dict:
        """
        Optionally run DOTSPlanner and/or LookaheadAgent to revise or select the pipeline.
        """
        goal = context.get("goal", {})

        # === RUN DOTS PLANNER FIRST (STRATEGY PLANNER) ===
        if self.cfg.get("planner", {}).get("enabled", False):
            try:
                planner_cfg = OmegaConf.to_container(self.cfg.planner, resolve=True)
                planner_cls = hydra.utils.get_class(planner_cfg["cls"])
                planner = planner_cls(
                    cfg=planner_cfg, memory=self.memory, logger=self.logger
                )

                context = await planner.run(context)

                if "suggested_pipeline" in context:
                    suggested = context["suggested_pipeline"]
                    self.logger.log(
                        "PipelineUpdatedByDOTSPlanner",
                        {
                            "strategy": context.get("strategy", "unknown"),
                            "suggested": suggested,
                        },
                    )
                    self.pipeline_stages = self._parse_pipeline_stages_from_list(
                        suggested
                    )
            except Exception as e:
                self.logger.log("DOTSPlannerFailed", {"error": str(e)})

        # === RUN LOOKAHEAD SECOND (OPTIONAL REFLECTIVE OVERRIDE) ===
        if not self.cfg.get("dynamic", {}).get("lookahead_enabled", False):
            return context

        try:
            lookahead_cfg = OmegaConf.to_container(self.cfg.dynamic, resolve=True)
            stage_dict = OmegaConf.to_container(self.cfg.agents.lookahead, resolve=True)
            stage_dict = self.rule_applier.apply_to_agent(stage_dict, context)
            agent_cls = hydra.utils.get_class(lookahead_cfg["cls"])
            lookahead_agent = agent_cls(
                cfg=stage_dict, memory=self.memory, logger=self.logger
            )

            self.logger.log("LookaheadStart", {"goal": goal})
            context[PIPELINE] = [stage.name for stage in self.pipeline_stages]
            context["agent_registry"] = OmegaConf.to_container(
                OmegaConf.load("config/agent_registry.yaml")["agents"]
            )
            updated_context = await lookahead_agent.run(context)

            if "suggested_pipeline" in updated_context:
                suggested = updated_context["suggested_pipeline"]
                self.logger.log(
                    "PipelineUpdatedByLookahead",
                    {
                        "original": [stage.name for stage in self.pipeline_stages],
                        "suggested": suggested,
                    },
                )
                self.pipeline_stages = self._parse_pipeline_stages_from_list(suggested)
            return updated_context

        except Exception as e:
            self.logger.log("LookaheadFailed", {"error": str(e)})
            return context

    async def rerun_pipeline(self, run_id: str) -> dict:
        """
        Re-run a previously stored pipeline run by its run_id.
        """
        self.logger.log("PipelineRerunStart", {"run_id": run_id})

        # Step 1: Load pipeline run
        pipeline_run = self.memory.pipeline_runs.get_by_run_id(run_id)
        if not pipeline_run:
            raise ValueError(f"No pipeline run found with run_id={run_id}")

        # Step 2: Load goal object
        goal = self.memory.goals.get_by_id(pipeline_run.goal_id)
        if not goal:
            raise ValueError(f"No goal found with goal_id={pipeline_run.goal_id}")

        # Step 3: Build context
        context = {
            "goal": goal,
            RUN_ID: run_id,
            PIPELINE: pipeline_run.pipeline,
            "strategy": pipeline_run.strategy,
            "model_config": pipeline_run.run_config,
            PROMPT_DIR: self.cfg.paths.prompts,
        }

        # Optional: override pipeline stages to match recorded run
        self.pipeline_stages = self._parse_pipeline_stages_from_list(
            pipeline_run.pipeline
        )

        # Optional: reapply lookahead suggestion or symbolic context (or skip it for pure repeatability)
        # context["lookahead"] = pipeline_run.lookahead_context
        # context["symbolic_suggestion"] = pipeline_run.symbolic_suggestion

        # Step 4: Run
        context = await self._run_pipeline_stages(context)

        # Step 5: Generate report (optional)
        self.generate_report(context, run_id)

        self.logger.log("PipelineRerunComplete", {"run_id": run_id})
        return context

    def analyze_pipeline_deltas(self, goal_id: int):
        from co_ai.analysis.reflection_delta import compare_pipeline_runs

        deltas = compare_pipeline_runs(self.memory, goal_id)
        for delta in deltas:
            self.logger.log("ReflectionDeltaComputed", delta)

    def get_goal(self, input_data: dict) -> dict:
        goal_dict = input_data.get("goal")
        if not goal_dict:
            raise ValueError("Missing 'goal' key in input_data")
        goal_orm = self.memory.goals.get_or_create(goal_dict)

        merged = goal_orm.to_dict()
        for key, value in goal_dict.items():
            if value is not None:
                if key in merged and merged[key] != value:
                    self.logger.log("GoalContextOverride", {
                        "field": key,
                        "original": merged[key],
                        "override": value,
                        "note": "Overriding goal field from context"
                    })
                merged[key] = value
        return merged
---END-OF-FILE---


