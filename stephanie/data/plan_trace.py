# stephanie/data/plan_trace.py
import json
import os
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

from stephanie.data.score_bundle import ScoreBundle


@dataclass
class ExecutionStep:
    """
    Represents a single step in the execution of a reasoning plan.
    This can be generated by an executor like EpistemicPlanExecutorAgent.
    """
    step_id: Union[str, int]  # Unique identifier for the step (e.g., index, name)
    step_type: str  # The type of the step (e.g., "action", "observation")
    description: str  # A textual description of what this step does
    output_text: str  # The textual output or result of this step

    # The scores assigned to this step's output by various scorers (SICQL, EBT, etc.)
    # against the original goal. 
    scores: Optional[ScoreBundle] 

    input_text: Optional[str] = None  # Optional input text for this step, if applicable
    agent_name: Optional[str] = None  # Name of the agent that executed this step, if applicable
    plan_trace_id: Optional[int] = None  # Foreign key to the PlanTrace this step belongs to
    step_order: Optional[int] = None  # Foreign key to the PlanTrace this step belongs to
    # Optional: Embedding of the output_text. Can be computed on demand if not stored.

    # Optional: Any other metadata specific to this step
    attributes: Optional[Dict[str, Any]] = field(default_factory=dict) 
    
    # Optional: Any other metadata specific to this step
    extra_data: Optional[Dict[str, Any]] = field(default_factory=dict) 
   
    agent_config: Optional[Dict] = None
    input_type: Optional[str] = None
    output_type: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    duration: Optional[float] = None
    error: Optional[Dict[str, Any]] = None
    output_keys: Optional[List[str]] = None
    output_size: Optional[int] = None
    policy_logits: Optional[List[float]] = None
    uncertainty: Optional[float] = None
    entropy: Optional[float] = None
    zsa: Optional[Union[List[float], Dict]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        result = {
            "step_id": self.step_id,
            "step_order": self.step_order,
            "step_type": self.step_type,
            "description": self.description,
            "input_text": self.input_text,
            "output_text": self.output_text,
            "agent_name": self.agent_name,
            "input_type": self.input_type,
            "output_type": self.output_type,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "duration": self.duration,
            "output_keys": self.output_keys,
            "output_size": self.output_size,
            "policy_logits": self.policy_logits,
            "uncertainty": self.uncertainty,
            "entropy": self.entropy
        }
        
        # Handle agent_config safely
        if self.agent_config:
            result["agent_config"] = self.agent_config
            
        # Handle error information
        if self.error:
            result["error"] = self.error
            
        # Handle scores
        if self.scores:
            result["scores"] = self.scores.to_dict() if hasattr(self.scores, "to_dict") else self.scores
            
        # Handle zsa (can be complex tensor data)
        if self.zsa is not None:
            if isinstance(self.zsa, list):
                result["zsa"] = self.zsa
            elif hasattr(self.zsa, "tolist"):
                result["zsa"] = self.zsa.tolist()
            else:
                result["zsa"] = str(self.zsa)
                
        return result

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ExecutionStep":
        """Create from dictionary"""
        return cls(
            step_id=data["step_id"],
            step_order=data["step_order"],
            step_type=data["step_type"],
            description=data["description"],
            input_text=data.get("input_text"),
            output_text=data.get("output_text"),
            agent_name=data.get("agent_name"),
            agent_config=data.get("agent_config"),
            input_type=data.get("input_type"),
            output_type=data.get("output_type"),
            start_time=data.get("start_time"),
            end_time=data.get("end_time"),
            duration=data.get("duration"),
            error=data.get("error"),
            scores=data.get("scores"),
            output_keys=data.get("output_keys"),
            output_size=data.get("output_size"),
            policy_logits=data.get("policy_logits"),
            uncertainty=data.get("uncertainty"),
            entropy=data.get("entropy"),
            zsa=data.get("zsa")
        )


@dataclass
class PlanTrace:
    """
    Represents the complete execution trace of a reasoning plan.
    This is the primary input for the EpistemicTraceEncoder and subsequently 
    the Epistemic Plan HRM model.
    """
    # --- Core Identifiers ---
    trace_id: str # Unique identifier for this specific trace/execution
    
    # --- Initial Context ---
    goal_text: str # The original goal or query
    goal_id: int
    input_data: Dict[str, Any] # Any initial data or variables provided to the plan
    
    # --- Plan Definition (Optional but useful for context) ---
    # This could be a representation of the DSPy program or pipeline used.
    # A simple string signature or a more structured representation.
    plan_signature: str 

    # --- Execution Details ---
    execution_steps: List[ExecutionStep] # The sequence of steps executed
    
    # --- Final Outcome ---
    final_output_text: str # The final output produced by the plan
    # The scores assigned to the final output by various scorers.
    final_scores: Optional[ScoreBundle] = None

    # --- Target for Epistemic Plan HRM Training ---
    # This is the label the HRM model will try to predict.
    # It represents the "epistemic quality" of this reasoning process.
    target_epistemic_quality: Optional[float] = None 
    # Source of the target quality score (e.g., "llm_judgment", "proxy_metric_avg_sicql_q")
    target_epistemic_quality_source: Optional[str] = None 

    # --- Metadata ---
    created_at: str = "" # ISO format timestamp
    # Any other execution metadata (e.g., time taken, DSPy optimizer version)
    extra_data: Optional[Dict[str, Any]] = field(default_factory=dict) 


    def to_dict(self) -> dict:
        result = {
            "trace_id": self.trace_id,
            "goal_text": self.goal_text,
            "goal_id": self.goal_id,
            "input_data": self.input_data,
            "plan_signature": self.plan_signature,
            "execution_steps": [step.to_dict() for step in self.execution_steps],
            "final_output_text": self.final_output_text,
            "final_scores": self.final_scores.to_dict() if self.final_scores else None,
            "target_epistemic_quality": self.target_epistemic_quality,
            "target_epistemic_quality_source": self.target_epistemic_quality_source,
            "created_at": self.created_at,
            "extra_data": self.extra_data,
        }
        return result

    def get_target_quality(self) -> float:
        if self.has_target_quality():
            return float(self.target_epistemic_quality)
        raise ValueError(f"Trace {self.trace_id} is missing 'target_epistemic_quality'")

    def has_target_quality(self) -> float:
        return self.target_epistemic_quality is not None

    # --- Utility Methods ---
    def get_all_text_outputs(self) -> List[str]:
        """Get a list of all text outputs, including intermediate steps and final output."""
        texts = [step.output_text for step in self.execution_steps]
        texts.append(self.final_output_text)
        return texts

    def get_all_score_bundles(self) -> List[ScoreBundle]:
        """Get a list of all ScoreBundles, including intermediate steps and final output."""
        bundles = [step.scores for step in self.execution_steps]
        bundles.append(self.final_scores)
        return bundles

    def to_markdown(self) -> str:
        lines = [f"## Plan Trace: {self.trace_id}", f"**Goal:** {self.goal_text}\n"]
        for step in self.execution_steps:
            step_id_str = str(step.step_id) if step.step_id is not None else "N/A"
            lines.append(f"### Step {step_id_str}: {step.description} ({step.step_type}) ")
            lines.append(f"Output: `{step.output_text}`")
            lines.append(step.scores.to_report(f"Step {step_id_str}: Scores"))
        lines.append(f"\n**Final Output:** `{self.final_output_text}`")
        lines.append("Final Scores:")
        lines.append(self.final_scores.to_report("Trace Final Scores") if self.final_scores else "No final scores available.")
        return "\n".join(lines)

    def save_as_markdown(self, reports_dir: str = "reports") -> str:
        os.makedirs(reports_dir, exist_ok=True)
        markdown_text = self.to_markdown()
        safe_trace_id = "".join(c for c in self.trace_id if c.isalnum() or c in (' ', '-', '_')).rstrip()
        filename = f"{safe_trace_id}.md"
        filepath = os.path.join(reports_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(markdown_text)
        return filepath

    def save_as_json(self, dir_path: str = "reports/json") -> str:
        os.makedirs(dir_path, exist_ok=True)
        filename = f"{self.trace_id}.json"
        path = os.path.join(dir_path, filename)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, indent=2)

        print(f"PlanTraceSavedAsJSON path: {path}")

        return path

    @classmethod
    def from_dict(cls, data: dict) -> "PlanTrace":
        from stephanie.data.score_bundle import ScoreBundle

        execution_steps = [
            ExecutionStep(
                step_id=step["step_id"],
                step_type=step.get("step_type", "action"),  # Default to "action"
                agent_name=step.get("agent_name"),
                description=step["description"],
                output_text=step["output_text"],
                input_text=step.get("input_text"),
                scores=ScoreBundle.from_dict(step["scores"]),
                plan_trace_id=step.get("plan_trace_id"),
                step_order=step.get("step_order"),
                extra_data=step.get("extra_data", {}),
                agent_config=step.get("agent_config"),
                input_type=step.get("input_type"),
                output_type=step.get("output_type"),
                start_time=step.get("start_time"),
                end_time=step.get("end_time"),
                duration=step.get("duration"),
                error=step.get("error"), 
                output_keys=step.get("output_keys"),
                output_size=step.get("output_size"),
                policy_logits=step.get("policy_logits"),
                uncertainty=step.get("uncertainty"),
                entropy=step.get("entropy"),
                zsa=step.get("zsa"),
                    
            )
            for step in data["execution_steps"]
        ]

        return cls(
            trace_id=data["trace_id"],
            goal_text=data["goal_text"],
            goal_id=data["goal_id"],
            input_data=data["input_data"],
            plan_signature=data["plan_signature"],
            execution_steps=execution_steps,
            final_output_text=data["final_output_text"],
            final_scores=ScoreBundle.from_dict(data["final_scores"]),
            target_epistemic_quality=data.get("target_epistemic_quality"),
            target_epistemic_quality_source=data.get("target_epistemic_quality_source"),
            created_at=data.get("created_at", ""),
            extra_data=data.get("extra_data", {}),
        )