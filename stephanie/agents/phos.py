# stephanie/agents/phos.py
from __future__ import annotations
import numpy as np
import re
from datetime import datetime
import json
from pathlib import Path
from typing import List, Dict, Any
from stephanie.agents.base_agent import BaseAgent
from stephanie.services.zeromodel_service import ZeroModelService
from stephanie.analysis.vpm_differential_analyzer import (
    VPMDifferentialAnalyzer,
)
from stephanie.services.scoring_service import ScoringService
from stephanie.constants import PIPELINE_RUN_ID
from stephanie.scoring.scorable import ScorableType
from stephanie.services.workers.metrics_worker import MetricsWorker
from stephanie.services.workers.vpm_worker import VPMWorker
from stephanie.utils.emit_broadcaster import EmitBroadcaster
import time
from sqlalchemy import text
import logging
from tqdm import tqdm
import asyncio
from typing import Optional
from stephanie.agents.agentic_tree_search import (
    SolutionNode,
)  # assuming you have this
from stephanie.scoring.scorable import Scorable, ScorableFactory


_logger = logging.getLogger(__name__)


class PhosAgent(BaseAgent):
    """
    PhÅsAgent â€” Illuminates semantic structure by generating contrastive VPMs.

    It:
    - Retrieves ATS outputs for a given goal.
    - Clusters them into good/mixed/bad using embedding similarity.
    - Scores and visualizes them using ZeroModel.
    - Performs differential analysis between categories.
    """

    def __init__(self, cfg, memory, container, logger):
        super().__init__(cfg, memory, container, logger)
        self.embedding_store = memory.embedding
        self.zm: ZeroModelService = container.get("zeromodel")
        self.scorer: ScoringService = container.get("scoring")
        self.analyzer = VPMDifferentialAnalyzer(output_dir="vpm_phos")
        self.ref_goal_id = cfg.get("reference_goal_id", 83)
        self.metrics_worker = MetricsWorker(cfg, memory, container, logger)
        self.vpm_worker = VPMWorker(cfg, memory, container, logger)
        self.run_id = 0
        self._goal_text = ""
        self._published_nodes = set()  # to avoid duplicates
        self.metric_names = []

        self.emit_cb = EmitBroadcaster(
            self._emit_to_logger, self._timeline_sink
        )

    async def run(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Flow:
        1. Load scorables (good/mixed/opposite)
        2. Ensure embeddings exist
        3. Generate timelines for each band
        4. Run differential analysis (epistemic field)
        5. Return structured results in context
        I"""
        asyncio.create_task(self.metrics_worker.start())
        asyncio.create_task(self.vpm_worker.start())

        # give the bus time to register subscriptions
        await asyncio.sleep(0.5)

        self.run_id = context.get(PIPELINE_RUN_ID)
        # Open timeline session (direct, in-process)
        goal_text = context["goal"]["goal_text"]
        self._goal_text = goal_text
        # 1 get gols standard runs
        good_rows = await self._gather_runs(self.ref_goal_id)
        good = [
            self._make_scorable(r["response"], similarity=1.0, label="good")
            for r in good_rows
        ]
        self.ensure_embedding(good)

        # 2 get unrelated runs for contrast
        medium_rows = (
            self.memory.embedding.search_scorables_in_similarity_band(
                goal_text,
                ScorableType.RESPONSE,
                lower=0.25,
                upper=0.75,
                top_k=300,
            )
        )
        medium = [
            self._make_scorable(
                r["text"], similarity=r["similarity"], label="medium"
            )
            for r in medium_rows
        ]
        self.ensure_embedding(medium)
        # --- 3. OPPOSITE: inverse semantic
        opposite_rows = self.memory.embedding.search_unrelated_scorables(
            goal_text, ScorableType.RESPONSE, top_k=300
        )
        opposite = [
            self._make_scorable(
                r["text"], similarity=r["score"], label="opposite"
            )
            for r in opposite_rows
        ]
        self.ensure_embedding(opposite)

        datasets = {
            "good": good,
            "mixed": medium,
            "opposite": opposite,
        }

        timeline_gifs = await self._simulate_band_processing(datasets, context)

        diffs = self._analyze_vpms(
            {
                "good": timeline_gifs.get("good"),
                "medium": timeline_gifs.get("medium"),
                "opposite": timeline_gifs.get("opposite"),
            }
        )

        context["phos_outputs"] = diffs
        return context

    # -------------------------------------

    def ensure_embedding(self, scorables: List[Scorable]) -> List[int]:
        """Ensure that the embedding for the given items is available."""
        ids = []
        skipped, updated = 0, 0
        for scorable in tqdm(
            scorables,
            desc="Backfilling prompt response embeddings",
            unit="responses",
        ):
            # Step 2: Check if embedding already exists in the embedding store
            exists = self.memory.scorable_embeddings.get_by_scorable(
                scorable_id=str(scorable.id),
                scorable_type=scorable.target_type,
                embedding_type=self.memory.embedding.name,
            )
            if exists:
                ids.append(exists.id)
                skipped += 1
                continue

            # Step 3: Choose text for embedding
            scorable = ScorableFactory.from_orm(scorable, mode="response_only")

            # Step 4: Generate embedding

            # Step 5: Insert into store
            embedding_id = self.memory.scorable_embeddings.get_or_create(
                scorable
            )
            updated += 1
            ids.append(embedding_id)
        _logger.debug(
            "Embedding backfill complete: %s skipped, %s added.",
            skipped,
            updated,
        )
        return ids

    async def _gather_runs(self, goal_id: int) -> List[Dict]:
        """Pull ATS runs matching or near the goal."""
        q = text(f"""
            SELECT id, response_text, *
            FROM prompts
            WHERE goal_id = {goal_id}
            GROUP BY pipeline_run_id, id
            ORDER BY id DESC
            LIMIT 300;
        """)

        # Use the sessionmaker to create a new session
        with self.memory.session() as session:
            rows = session.execute(q).fetchall()

        return [
            {"response": self._strip_think_blocks(r.response_text)}
            for r in rows
        ]

    def _analyze_vpms(self, vpms: Dict[str, Any]):
        """Run contrastive analysis on actual matrices returned by timeline_finalize."""
        good = vpms.get("good", {})
        mixed = vpms.get("medium", {})
        bad = vpms.get("opposite", {})

        # Safely extract matrices only if the entry is a dict
        good_mat = good.get("matrix") if isinstance(good, dict) else None
        mixed_mat = mixed.get("matrix") if isinstance(mixed, dict) else None
        bad_mat = bad.get("matrix") if isinstance(bad, dict) else None

        results = {}
        output_dir = Path(f"data/vpms/phos_diffs/run_{self.run_id}")
        output_dir.mkdir(parents=True, exist_ok=True)

        if good_mat is not None and bad_mat is not None:
            meta = self.zm.generate_epistemic_field(
                pos_matrices=[good_mat],
                neg_matrices=[bad_mat],
                output_dir=str(output_dir / "good_vs_bad"),
                aggregate=True,
                metric_names=self.metric_names,
            )
            results["good_vs_bad"] = meta
            _logger.debug(
                "Î”Mass=%s, Overlap=%s",
                meta["delta_mass"],
                meta["overlap_score"],
            )
            # ðŸ§  Analyze the differential field
            diff_matrix = meta.get("diff_matrix")
            metric_names = meta.get("metric_names_reordered", [])
            ranked_metrics = self.zm.analyze_differential_field(
                diff_matrix, metric_names, output_dir / "good_vs_bad"
            )
            self.logger.log("metrics", ranked_metrics)

        if good_mat is not None and mixed_mat is not None:
            meta = self.zm.generate_epistemic_field(
                pos_matrices=[good_mat],
                neg_matrices=[mixed_mat],
                output_dir=str(output_dir / "good_vs_mixed"),
                aggregate=True,
            )
            results["good_vs_mixed"] = meta
            _logger.debug(
                "Î”Mass=%s, Overlap=%s",
                meta["delta_mass"],
                meta["overlap_score"],
            )

        return results

    def _strip_think_blocks(self, text: str) -> str:
        if not text:
            return text
        """Remove <think>...</think> sections from text."""
        return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

    def _make_scorable(
        self, text: str, similarity: float, label: str
    ) -> Scorable:
        """
        Create a Scorable with similarity stored in its meta data.
        """
        sc = ScorableFactory.from_text(
            text=text,
            target_type=ScorableType.RESPONSE,
            meta={"similarity": similarity, "source_label": label},
        )
        return sc

    # --------------------------------------------------------------------------- #
    # ðŸ§  PLAN PROCESSOR â€” uses same logic as AgenticTreeSearch._process_plan
    # --------------------------------------------------------------------------- #
    async def _process_plan_for_phos(
        self,
        index: int,
        scorable: Scorable,
        label: str,
        context: Dict[str, Any],
        parent_node: Optional[SolutionNode] = None,
        node_type: str = "draft",
    ) -> SolutionNode:
        """
        Execute a text (scorable) as a plan and emit results to ZeroModel.
        Used to simulate the AgenticTreeSearch behavior for contrastive sets.
        """
        # 1. Compute the evaluation (metric + vector)

        plan = scorable.text
        ts = time.time()

        # 2. Create a minimal SolutionNode-like dict for emission
        node_id = self._make_numeric_id(self.run_id, label, index)
        _logger.debug(f"Processing plan {node_id} ({label}): {plan[:60]}...")
        node = {
            "id": node_id,
            "plan": plan,
            "summary": plan[:75],
            "output": plan,
            "timestamp": ts,
            "type": node_type,
            "label": label,
            "run_id": self.run_id,
        }

        # 4. Emit directly to ZeroModel timeline (if in-process mode)
        try:
            await self._timeline_sink("node", node)
        except Exception as e:
            _logger.error("[PhosAgent] Emit failed (%s): %s", label, e)

        return node

    async def _simulate_band_processing(
        self,
        datasets: Dict[str, List[Dict[str, Any]]],
        context: Dict[str, Any],
    ) -> Dict[str, str]:
        """
        Simulate AgenticTreeSearch execution for each contrastive dataset
        and produce three ZeroModel timelines (good / medium / opposite).
        """
        results = {}
        for label, scorables in datasets.items():
            run_id = self.run_id
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_run = str(self.run_id)[:8]
            out_path = f"vpm_phos_{timestamp}_run{safe_run}_{label}.gif"

            # âœ… Guard against empty or invalid bands early
            if not scorables:
                _logger.warning(
                    "[PhosAgent] âš ï¸ No scorables found for %s, skipping band.",
                    label
                )
                continue

            total = len(scorables)

            if total == 0:
                _logger.warning(
                    "[PhosAgent] âš ï¸ No scorables found for %s", label
                )
                continue

            # 1ï¸âƒ£ Open a fresh timeline stream
            try:
                self.zm.timeline_open(run_id=run_id)
                _logger.debug(
                    "Timeline opened for %s (%s scorables)", label, total
                )
            except Exception as e:
                _logger.warning(
                    "[PhosAgent] Timeline open failed (%s): %s", label, e
                )

            # 2ï¸âƒ£ Process each scorable with visible progress bar
            with tqdm(
                total=total, desc=f"[PhosAgent] {label} band", unit="node"
            ) as pbar:
                for idx, scorable in enumerate(scorables):
                    await self._process_plan_for_phos(
                        index=idx,
                        scorable=scorable,
                        label=label,
                        context=context,
                    )

                    pbar.update(1)

                    # Structured progress event for dashboards / metrics
                    if idx % 10 == 0 or idx == total - 1:
                        progress_ratio = (idx + 1) / total
                        info = json.dumps(
                            {
                                "event": "PhosProgress",
                                "label": label,
                                "run_id": self.run_id,
                                "completed": idx + 1,
                                "total": total,
                                "percent": round(progress_ratio * 100, 1),
                            }
                        )
                        if info:
                            _logger.debug(
                                "[PhosAgent] Progress update: %s", info
                            )
                    await asyncio.sleep(
                        0.01
                    )  # small delay for event sequencing

            # 3ï¸âƒ£ Close the stream and render to GIF
            try:
                final_res = await self.zm.timeline_finalize(
                    run_id=run_id, out_path=out_path
                )

                # âœ… Only store valid results that actually contain a matrix
                if (
                    final_res
                    and isinstance(final_res, dict)
                    and final_res.get("matrix") is not None
                ):
                    results[label] = final_res
                    self.metric_names = final_res.get("metric_names", [])
                    _logger.debug(
                        "[PhosAgent] âœ… Timeline closed for %s â†’ %s",
                        label,
                        out_path,
                    )
                else:
                    _logger.warning(
                        "[PhosAgent] âš ï¸ No valid matrix for %s, skipping analysis.",
                        label,
                    )
            except Exception as e:
                _logger.warning(
                    "[PhosAgent] Timeline close failed (%s): %s", label, e
                )

        return results

    async def _emit(self, event: str, payload: Dict[str, Any]) -> None:
        """
        Emit event to ZeroModel + EmitBroadcaster (if configured).
        """
        run_id = payload.get("run_id")
        # 1. Send to ZeroModel timeline directly
        try:
            if event == "node":
                node = payload.get("node", payload)
                node_id = node.get("id")
                key = (self.run_id, node_id)
                if key in self._published_nodes:
                    return  # already published, skip to avoid loop
                self._published_nodes.add(key)

                vector = payload.get("vector", {})
                metrics_columns = list(vector.keys())
                if len(metrics_columns):
                    metrics_values = list(vector.values())
                    self.zm.timeline_append_row(
                        run_id=run_id,
                        metrics_columns=metrics_columns,
                        metrics_values=metrics_values,
                    )
            elif event == "report":
                await self.zm.timeline_finalize(run_id)
        except Exception as e:
            self.logger.warning(
                f"[PhosAgent] ZeroModel timeline emit failed: {e}"
            )

        # 2. Notify EmitBroadcaster
        if self.emit_cb:
            try:
                await self.emit_cb(event, payload)
            except Exception as e:
                self.logger.debug(f"[PhosAgent] EmitBroadcaster failed: {e}")

    async def _timeline_sink(
        self, event: str, payload: Dict[str, Any]
    ) -> None:
        """Centralized event sink for ZeroModel + metrics bus."""
        if not self.run_id:
            return

        try:
            if event == "node":
                node = payload.get("node", payload)
                # emit into ZeroModelâ€™s in-process timeline if available
                try:
                    await self._emit("node", node)
                except Exception as e:
                    self.logger.warning(
                        f"[PhosAgent] ZeroModel emit failed: {e}"
                    )

                # publish metrics job for async worker
                _logger.debug(
                    f"[PhosAgent] -> 'arena.metrics.request' job for node={node}"
                )

                await self.memory.bus.publish(
                    subject="arena.metrics.request",
                    payload={
                        "run_id": self.run_id,
                        "node_id": node.get("id"),
                        "parent_id": node.get("parent_id"),
                        "action_type": node.get("type", "draft"),
                        "goal_text": self._goal_text,
                        "prompt_text": node.get("plan"),
                        "best_metric": node.get("metric"),
                        "bug": bool(node.get("bug", False)),
                        "ts_enqueued": time.time(),
                    },
                )

            elif event == "report":
                await self.memory.bus.publish(
                    subject="arena.ats.report",
                    payload={"run_id": self.run_id},
                )

        except Exception as e:
            _logger.warning("[PhosAgent] Timeline sink error: %s", e)

    def _make_numeric_id(self, run_id: Any, label: str, index: int) -> int:
        """
        Build a sortable integer ID:
            [run_code][label_code][index:04d]
        Ensures ascending order by run â†’ label â†’ index.
        """
        # 1ï¸âƒ£ derive a 4-digit run code from the run_id (UUID-safe)
        if isinstance(run_id, str):
            run_hash = abs(hash(run_id)) % 10_000
        else:
            run_hash = int(run_id) % 10_000

        # 2ï¸âƒ£ map label to a stable small integer
        label_code = {"good": 1, "medium": 2, "opposite": 3}.get(label, 9)

        # 3ï¸âƒ£ combine into a single sortable integer
        numeric_id = int(f"{run_hash:04d}{label_code}{index:04d}")
        return numeric_id

    def _emit_to_logger(self, event: str, payload: Dict[str, Any]) -> None:
        _logger.debug(f"Phos::{event} --> {payload}")
