# stephanie/agents/inference/document_mrq_inference.py
import os

import torch
import torch.nn.functional as F

from stephanie.agents.base_agent import BaseAgent
from stephanie.data.score_bundle import ScoreBundle
from stephanie.data.score_result import ScoreResult
from stephanie.evaluator.hypothesis_value_predictor import \
    HypothesisValuePredictor
from stephanie.scoring.model.in_context_q import InContextQModel
from stephanie.scoring.model.mrq_model import MRQModel
from stephanie.scoring.model.policy_head import PolicyHead
from stephanie.scoring.model.q_head import QHead
from stephanie.scoring.model.text_encoder import TextEncoder
from stephanie.scoring.model.v_head import VHead
from stephanie.scoring.scorable_factory import ScorableFactory, TargetType
from stephanie.scoring.scoring_manager import ScoringManager
from stephanie.scoring.transforms.regression_tuner import RegressionTuner
from stephanie.utils.file_utils import load_json
from stephanie.utils.model_locator import ModelLocator


class MRQInferenceAgent(BaseAgent):
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)
        self.dim = memory.embedding.dim
        self.hdim = memory.embedding.hdim
        self.cfg = cfg
        self.model_path = cfg.get("model_path", "models")
        self.use_sicql = cfg.get("use_sicql_style", False)
        self.model_type = "sicql" if self.use_sicql else "mrq"
        self.evaluator = "sicql" if self.use_sicql else "mrq"
        self.target_type = cfg.get("target_type", "document")
        self.model_version = cfg.get("model_version", "v1")
        self.embedding_type = self.memory.embedding.name
        self.dimensions = cfg.get("dimensions", [])
        self.models = {}
        self.model_meta = {} 
        self.tuners = {}

        self.logger.log(
            "MRQInferenceAgentInitialized",
            {
                "model_type": self.model_type,
                "target_type": self.target_type,
                "dimensions": self.dimensions,
                "device": str(self.device),
                "model_version": self.model_version,
                "embedding_type": self.embedding_type,
                "use_sicql": self.use_sicql,
            },
        )

    async def run(self, context: dict) -> dict:
        goal_text = context.get("goal", {}).get("goal_text")
        results = []
        docs = context.get(self.input_key, [])
        self.load_models(self.dimensions)

        for doc in docs:
            doc_id = doc.get("id")
            self.logger.log("MRQScoringStarted", {"document_id": doc_id})
            scorable = ScorableFactory.from_dict(doc, TargetType.DOCUMENT)
            dimension_scores = {}
            score_results = []

            for dim, model in self.models.items():
                if self.use_sicql:
                    prompt_emb = torch.tensor(
                        self.memory.embedding.get_or_create(goal_text),
                        device=self.device,
                    ).unsqueeze(0)
                    output_emb = torch.tensor(
                        self.memory.embedding.get_or_create(scorable.text),
                        device=self.device,
                    ).unsqueeze(0)
                    result = model(prompt_emb, output_emb)

                    q_value = result["q_value"].item()
                    v_value = result["state_value"].item()
                    policy_logits = (
                        result["action_logits"].cpu().detach().numpy().tolist()
                    )

                    if (
                        isinstance(policy_logits, list)
                        and len(policy_logits) == 1
                    ):
                        if isinstance(policy_logits[0], list):
                            # [[0.1166]] â†’ [0.1166]
                            policy_logits = policy_logits[0]

                    print(f"Policy logits for {dim}: {policy_logits}")

                    # Calculate uncertainty (|Q - V|)
                    uncertainty = abs(q_value - v_value)

                    # Calculate entropy from policy logits
                    policy_tensor = torch.tensor(policy_logits)
                    action_probs = F.softmax(policy_tensor, dim=-1)
                    entropy = -torch.sum(
                        action_probs * torch.log(action_probs + 1e-8)
                    ).item()

                    # Calculate advantage
                    advantage = q_value - v_value

                else:
                    q_value = model.predict(goal_text, scorable.text)

                meta = self.model_meta.get(dim, {"min": 0, "max": 100})
                if dim in self.tuners:
                    scaled_score = self.tuners[dim].transform(q_value)
                else:
                    normalized = torch.sigmoid(torch.tensor(q_value)).item()
                    scaled_score = (
                        normalized * (meta["max_value"] - meta["min_value"])
                        + meta["min_value"]
                    )

                scaled_score = max(
                    min(scaled_score, meta["max_value"]), meta["min_value"]
                )
                final_score = round(scaled_score, 4)
                dimension_scores[dim] = final_score

                attributes = {
                    "raw_q_value": round(q_value, 4),
                    "raw_v_value": round(v_value, 4)
                    if self.use_sicql
                    else None,
                    "policy_logits": policy_logits if self.use_sicql else None,
                    "scaled_score": round(scaled_score, 4),
                    "advantage": round(advantage, 4)
                    if self.use_sicql
                    else None,
                    "uncertainty": round(uncertainty, 4)
                    if self.use_sicql
                    else None,
                    "entropy": round(entropy, 4) if self.use_sicql else None,
                }

                score_results.append(
                    ScoreResult(
                        dimension=dim,
                        score=final_score,
                        rationale=f"Q={round(q_value, 4)}",
                        weight=1.0,
                        source=self.name,
                        attributes=attributes,
                    )
                )

                self.logger.log(
                    "MRQScoreComputed",
                    {
                        "document_id": doc_id,
                        "dimension": dim,
                        "q_value": round(q_value, 4),
                        "final_score": final_score,
                    },
                )

            score_bundle = ScoreBundle(
                results={r.dimension: r for r in score_results}
            )
            model_name = (
                f"{self.target_type}_{self.model_type}_{self.model_version}"
            )

            ScoringManager.save_score_to_memory(
                score_bundle,
                scorable,
                context,
                self.cfg,
                self.memory,
                self.logger,
                source="mrq",
                model_name=model_name,
                evaluator_name=self.model_type,
            )

            results.append(
                {
                    "scorable": scorable.to_dict(),
                    "scores": dimension_scores,
                    "score_bundle": score_bundle.to_dict(),
                }
            )

            self.logger.log(
                "MRQScoringFinished",
                {
                    "document_id": doc_id,
                    "scores": dimension_scores,
                    "dimensions_scored": list(dimension_scores.keys()),
                },
            )

        context[self.output_key] = results
        self.logger.log(
            "MRQInferenceCompleted", {"total_documents_scored": len(results)}
        )
        return context

    def load_models(self, dimensions: list):
        self.models = {}
        self.model_meta = {}
        self.tuners = {}

        for dim in dimensions:
            locator = ModelLocator(
                root_dir=self.model_path,
                embedding_type=self.embedding_type,
                model_type=self.model_type,
                target_type=self.target_type,
                dimension=dim,
                version=self.model_version,
            )

            encoder = TextEncoder(self.dim, self.hdim)
            if self.use_sicql:
                q_head = QHead(zsa_dim=self.dim, hdim=self.hdim).to(
                    self.device
                )
                v_head = VHead(zsa_dim=self.dim, hdim=self.hdim).to(
                    self.device
                )
                pi_head = PolicyHead(
                    zsa_dim=self.dim, hdim=self.hdim, num_actions=3
                ).to(self.device)

                encoder.load_state_dict(
                    torch.load(
                        locator.encoder_file(), map_location=self.device
                    )
                )
                q_head.load_state_dict(
                    torch.load(locator.q_head_file(), map_location=self.device)
                )
                v_head.load_state_dict(
                    torch.load(locator.v_head_file(), map_location=self.device)
                )
                pi_head.load_state_dict(
                    torch.load(
                        locator.pi_head_file(), map_location=self.device
                    )
                )

                model = InContextQModel(
                    encoder=encoder,
                    q_head=q_head,
                    v_head=v_head,
                    pi_head=pi_head,
                    embedding_store=self.memory.embedding,
                    device=self.device,
                )
                self.models[dim] = model

                meta = (
                    load_json(locator.meta_file())
                    if os.path.exists(locator.meta_file())
                    else {"min_value": 0, "max_value": 100}
                )
                self.model_meta[dim] = meta

                tuner_path = locator.tuner_file()
                if os.path.exists(tuner_path):
                    tuner = RegressionTuner(dimension=dim)
                    tuner.load(tuner_path)
                    self.tuners[dim] = tuner
            else:
                predictor = HypothesisValuePredictor(self.dim, self.hdim)
                model = MRQModel(
                    encoder,
                    predictor,
                    self.memory.embedding,
                    device=self.device,
                )
                model.load_weights(
                    locator.encoder_file(), locator.model_file()
                )
                self.models[dim] = model
                meta = (
                    load_json(locator.meta_file())
                    if os.path.exists(locator.meta_file())
                    else {"min": 0, "max": 100}
                )
                tuner_path = locator.tuner_file()
                self.model_meta[dim] = meta

                if os.path.exists(tuner_path):
                    tuner = RegressionTuner(dimension=dim)
                    tuner.load(tuner_path)
                    self.tuners[dim] = tuner

        self.logger.log("AllMRQModelsLoaded", {"dimensions": dimensions})
