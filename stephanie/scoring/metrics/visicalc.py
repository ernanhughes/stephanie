# stephanie/scoring/metrics/visicalc.py
"""
Unified cognitive analytics engine for measuring thought evolution.

This module provides VisiCalc, a structured analyzer that turns sequences of scored items
(e.g., Scorables in a graph) into interpretable reports, feature vectors, and quality scores.

Designed for:
  - Measuring ΔV in Blossom expansions
  - Comparing graph states in Nexus
  - Generating blog/UI visuals ("filmstrip" diagnostics)
  - Agent decision-making based on cognitive health

Core concepts:
  - VisiCalcEpisode: raw (items × metrics) score matrix + names
  - VisiCalcReport: global + regional statistics over a frontier metric
  - VisiCalc: high-level wrapper that builds a report + feature vector from rows/matrices
"""

from __future__ import annotations

import csv
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image

from stephanie.utils.json_sanitize import dumps_safe

log = logging.getLogger(__name__)


# =========================================================================== #
# 1. Core Data Structures
# =========================================================================== #


@dataclass
class VisiCalcRegion:
    """One contiguous row region in a VisiCalc report."""

    index: int
    start_row: int
    end_row: int
    row_count: int
    frontier_frac: float
    low_frac: float
    high_frac: float
    mean_frontier_value: float

    def to_dict(self) -> Dict[str, Any]:
        return {
            "index": self.index,
            "start_row": self.start_row,
            "end_row": self.end_row,
            "row_count": self.row_count,
            "frontier_frac": self.frontier_frac,
            "low_frac": self.low_frac,
            "high_frac": self.high_frac,
            "mean_frontier_value": self.mean_frontier_value,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any], index_fallback: int = 0) -> "VisiCalcRegion":
        start_row = data.get("start_row", data.get("row_start", 0))
        end_row = data.get("end_row", data.get("row_end", 0))
        row_count = data.get("row_count", data.get("num_rows", 0))
        frontier_frac = data.get("frontier_frac", data.get("frontier_fraction", 0.0))
        return cls(
            index=int(data.get("index", index_fallback)),
            start_row=int(start_row),
            end_row=int(end_row),
            row_count=int(row_count),
            frontier_frac=float(frontier_frac),
            low_frac=float(data.get("low_frac", 0.0)),
            high_frac=float(data.get("high_frac", 0.0)),
            mean_frontier_value=float(data.get("mean_frontier_value", 0.0) or 0.0),
        )

    def __str__(self) -> str:
        return (
            f"[R{self.index}] {self.start_row}-{self.end_row} "
            f"| F={self.frontier_frac:.3f} L={self.low_frac:.3f} H={self.high_frac:.3f}"
        )


@dataclass
class VisiCalcReport:
    """
    Full diagnostic report generated by VisiCalc.

    Represents global + regional statistics over a scored item sequence for a single
    "frontier" metric (e.g., quality, risk, etc.).
    """

    frontier_metric: str
    frontier_low: float
    frontier_high: float
    row_region_splits: int
    regions: List[VisiCalcRegion]
    global_low_frac: float
    global_high_frac: float
    global_frontier_frac: float = 0.0
    global_mean: float = 0.0
    global_std: float = 0.0
    global_min: float = 0.0
    global_max: float = 0.0
    entropy: float = 0.0
    sparsity_level_e3: float = 0.0
    sparsity_level_e2: float = 0.0

    def validate(self) -> None:
        assert 0.0 <= self.frontier_low < self.frontier_high <= 1.0, \
            f"Invalid band [{self.frontier_low}, {self.frontier_high}]"
        assert len(self.regions) == self.row_region_splits, \
            f"regions count mismatch: {len(self.regions)} vs {self.row_region_splits}"
        for r in self.regions:
            assert 0 <= r.start_row <= r.end_row
            assert r.row_count == r.end_row - r.start_row
            assert 0.0 <= r.frontier_frac <= 1.0
        log.debug("✅ VisiCalcReport validated: '%s'", self.frontier_metric)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "frontier": {
                "metric": self.frontier_metric,
                "low": self.frontier_low,
                "high": self.frontier_high,
                "row_region_splits": self.row_region_splits,
            },
            "global": {
                "frontier_frac": self.global_frontier_frac,
                "low_frac": self.global_low_frac,
                "high_frac": self.global_high_frac,
                "mean": self.global_mean,
                "std": self.global_std,
                "min": self.global_min,
                "max": self.global_max,
                "entropy": self.entropy,
                "sparsity_level_e3": self.sparsity_level_e3,
                "sparsity_level_e2": self.sparsity_level_e2,
            },
            "regions": [r.to_dict() for r in self.regions],
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "VisiCalcReport":
        frontier = data.get("frontier", {}) or {}
        global_stats = data.get("global", {}) or {}
        region_items = data.get("regions", []) or []
        regions = [
            VisiCalcRegion.from_dict(r, index_fallback=i)
            for i, r in enumerate(region_items)
        ]
        report = cls(
            frontier_metric=str(frontier.get("metric", "")),
            frontier_low=float(frontier.get("low", 0.0)),
            frontier_high=float(frontier.get("high", 1.0)),
            row_region_splits=int(frontier.get("row_region_splits", len(regions))),
            regions=regions,
            global_low_frac=float(global_stats.get("low_frac", 0.0)),
            global_high_frac=float(global_stats.get("high_frac", 0.0)),
            global_frontier_frac=float(global_stats.get("frontier_frac", 0.0)),
            global_mean=float(global_stats.get("mean", 0.0)),
            global_std=float(global_stats.get("std", 0.0)),
            global_min=float(global_stats.get("min", 0.0)),
            global_max=float(global_stats.get("max", 0.0)),
            entropy=float(global_stats.get("entropy", 0.0)),
            sparsity_level_e3=float(global_stats.get("sparsity_level_e3", 0.0)),
            sparsity_level_e2=float(global_stats.get("sparsity_level_e2", 0.0)),
        )
        report.validate()
        return report

    def save_json(self, path: Union[Path, str]) -> Path:
        p = Path(path)
        p.parent.mkdir(parents=True, exist_ok=True)
        with p.open("w", encoding="utf-8") as f:
            f.write(dumps_safe(self.to_dict(), indent=2))
        log.info("Saved VisiCalcReport → %s", p)
        return p

    def save_csv(self, path: Union[Path, str]) -> Path:
        p = Path(path)
        p.parent.mkdir(parents=True, exist_ok=True)

        fieldnames = [
            "level", "frontier_metric", "frontier_low", "frontier_high",
            "row_region_splits",
            "global_frontier_frac", "global_low_frac", "global_high_frac",
            "global_mean", "global_std", "global_min", "global_max",
            "entropy", "sparsity_level_e3", "sparsity_level_e2",
            "region_index", "region_start_row", "region_end_row", "region_row_count",
            "region_frontier_frac", "region_low_frac", "region_high_frac",
            "region_mean_frontier_value",
        ]

        with p.open("w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

            # Global row
            writer.writerow({
                "level": "global",
                "frontier_metric": self.frontier_metric,
                "frontier_low": self.frontier_low,
                "frontier_high": self.frontier_high,
                "row_region_splits": self.row_region_splits,
                "global_frontier_frac": self.global_frontier_frac,
                "global_low_frac": self.global_low_frac,
                "global_high_frac": self.global_high_frac,
                "global_mean": self.global_mean,
                "global_std": self.global_std,
                "global_min": self.global_min,
                "global_max": self.global_max,
                "entropy": self.entropy,
                "sparsity_level_e3": self.sparsity_level_e3,
                "sparsity_level_e2": self.sparsity_level_e2,
                **{f"region_{k}": None for k in (
                    "index", "start_row", "end_row", "row_count",
                    "frontier_frac", "low_frac", "high_frac", "mean_frontier_value"
                )}
            })

            # Region rows
            for r in self.regions:
                writer.writerow({
                    "level": "region",
                    "frontier_metric": self.frontier_metric,
                    "frontier_low": self.frontier_low,
                    "frontier_high": self.frontier_high,
                    "row_region_splits": self.row_region_splits,
                    "global_frontier_frac": self.global_frontier_frac,
                    "global_low_frac": self.global_low_frac,
                    "global_high_frac": self.global_high_frac,
                    "global_mean": self.global_mean,
                    "global_std": self.global_std,
                    "global_min": self.global_min,
                    "global_max": self.global_max,
                    "entropy": self.entropy,
                    "sparsity_level_e3": self.sparsity_level_e3,
                    "sparsity_level_e2": self.sparsity_level_e2,
                    "region_index": r.index,
                    "region_start_row": r.start_row,
                    "region_end_row": r.end_row,
                    "region_row_count": r.row_count,
                    "region_frontier_frac": r.frontier_frac,
                    "region_low_frac": r.low_frac,
                    "region_high_frac": r.high_frac,
                    "region_mean_frontier_value": r.mean_frontier_value,
                })

        log.info("Saved VisiCalcReport CSV → %s", p)
        return p

    def quality_score(self, low_weight: float = 0.5, high_weight: float = 0.5) -> float:
        """
        Simple scalar quality score:
          - reward frontier + high coverage,
          - penalize low coverage
        """
        return (
            self.global_frontier_frac +
            high_weight * self.global_high_frac -
            low_weight * self.global_low_frac
        )

    def diff(self, baseline: "VisiCalcReport") -> Dict[str, Any]:
        """Delta vs baseline report."""
        assert self.frontier_metric == baseline.frontier_metric
        return {
            "frontier_metric": self.frontier_metric,
            "band": [self.frontier_low, self.frontier_high],
            "target_splits": self.row_region_splits,
            "baseline_splits": baseline.row_region_splits,
            "global_delta": {
                "mean": self.global_mean - baseline.global_mean,
                "std": self.global_std - baseline.global_std,
                "min": self.global_min - baseline.global_min,
                "max": self.global_max - baseline.global_max,
                "frontier_frac": self.global_frontier_frac - baseline.global_frontier_frac,
                "low_frac": self.global_low_frac - baseline.global_low_frac,
                "high_frac": self.global_high_frac - baseline.global_high_frac,
                "entropy": self.entropy - baseline.entropy,
                "sparsity_e3": self.sparsity_level_e3 - baseline.sparsity_level_e3,
                "sparsity_e2": self.sparsity_level_e2 - baseline.sparsity_level_e2,
            },
            "regions": [
                {
                    "index": i,
                    "target_span": [tr.start_row, tr.end_row],
                    "baseline_span": [br.start_row, br.end_row],
                    "row_count_delta": tr.row_count - br.row_count,
                    "frontier_frac_delta": tr.frontier_frac - br.frontier_frac,
                    "low_frac_delta": tr.low_frac - br.low_frac,
                    "high_frac_delta": tr.high_frac - br.high_frac,
                    "mean_frontier_value_delta": tr.mean_frontier_value - br.mean_frontier_value,
                }
                for i, (tr, br) in enumerate(zip(self.regions, baseline.regions))
            ]
        }


@dataclass
class VisiCalcEpisode:
    """
    Generic 'visual calculation' episode.

    Typically created from:
      - a score matrix (rows = items/time, cols = metrics)
      - metric names
      - item ids (e.g., documents, steps, tiles)
    """

    episode_id: str
    scores: np.ndarray               # shape: (N_items, N_metrics)
    metric_names: List[str]
    item_ids: List[str]
    meta: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self) -> None:
        scores = np.asarray(self.scores)
        if scores.ndim != 2:
            raise ValueError(f"scores must be 2D, got shape {scores.shape}")
        n_items, n_metrics = scores.shape
        if len(self.metric_names) != n_metrics:
            raise ValueError(
                f"metric_names length {len(self.metric_names)} != scores.shape[1] {n_metrics}"
            )
        if len(self.item_ids) != n_items:
            raise ValueError(
                f"item_ids length {len(self.item_ids)} != scores.shape[0] {n_items}"
            )
        self.scores = scores

    def summary(self, frontier_band: Tuple[float, float] = (0.25, 0.75)) -> Dict[str, Any]:
        return episode_summary(self, frontier_band=frontier_band)

    def features(self, frontier_band: Tuple[float, float] = (0.25, 0.75)) -> Tuple[np.ndarray, List[str]]:
        return episode_features(self, frontier_band=frontier_band)


# =========================================================================== #
# 2. Utilities (normalization, entropy, frontier)
# =========================================================================== #


def normalize_scores(scores: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    """
    Normalize each metric column to [0,1] across items.
    If a column is constant, it becomes 0.5.
    """
    scores = np.asarray(scores, dtype=np.float64)
    if scores.ndim != 2:
        raise ValueError(f"Expected 2D scores, got {scores.shape}")

    out = np.zeros_like(scores, dtype=np.float64)
    for j in range(scores.shape[1]):
        col = scores[:, j]
        cmin = float(col.min())
        cmax = float(col.max())
        if cmax - cmin < eps:
            out[:, j] = 0.5
        else:
            out[:, j] = (col - cmin) / (cmax - cmin)
    return out

def scores_to_vpm_uint8(
    scores: np.ndarray,
    *,
    per_metric_normalize: bool = True,
    eps: float = 1e-8,
) -> np.ndarray:
    """
    Convert a 2D scores matrix into a [0,255] uint8 image.

    - per_metric_normalize=True:
        normalize each metric column to [0,1] (like normalize_scores)
    - per_metric_normalize=False:
        global min–max across the whole matrix.
    """
    arr = np.asarray(scores, dtype=np.float64)
    if arr.ndim != 2:
        raise ValueError(f"scores_to_vpm_uint8: expected 2D, got shape {arr.shape}")

    if per_metric_normalize:
        arr = normalize_scores(arr, eps=eps)
    else:
        cmin = float(np.nanmin(arr))
        cmax = float(np.nanmax(arr))
        if not np.isfinite(cmin) or not np.isfinite(cmax):
            arr = np.nan_to_num(arr, nan=0.0, posinf=1.0, neginf=0.0)
            cmin = float(arr.min())
            cmax = float(arr.max())
        if cmax - cmin < eps:
            arr = np.zeros_like(arr) + 0.5
        else:
            arr = (arr - cmin) / (cmax - cmin)

    # Clean up NaNs/Infs and clip to [0,1]
    arr = np.nan_to_num(arr, nan=0.0, posinf=1.0, neginf=0.0)
    arr = np.clip(arr, 0.0, 1.0)

    img = (arr * 255.0).round().astype(np.uint8)  # shape: (N_rows, N_metrics)
    return img


def compute_frontier_mask(
    scores: np.ndarray,
    low: float = 0.25,
    high: float = 0.75,
) -> np.ndarray:
    """
    Frontier band: entries whose value lies in [low, high].
    Returns a boolean mask of same shape as scores.
    """
    scores = np.asarray(scores, dtype=np.float64)
    if scores.ndim != 2:
        raise ValueError(f"Expected 2D scores, got {scores.shape}")
    if not (0.0 <= low <= high <= 1.0):
        raise ValueError(f"Invalid band [{low}, {high}], must be in [0,1] with low <= high")

    return (scores >= low) & (scores <= high)


def row_entropy(prob_row: np.ndarray, eps: float = 1e-8) -> float:
    """
    Shannon entropy of a row of non-negative values, treated as a distribution.
    We renormalize the row to sum to 1.
    """
    p = np.asarray(prob_row, dtype=np.float64)
    p = np.maximum(p, 0.0)
    s = float(p.sum())
    if s < eps:
        return 0.0
    p = p / s
    mask = p > eps
    return float(-np.sum(p[mask] * np.log(p[mask] + eps)))


def episode_summary(
    episode: VisiCalcEpisode,
    frontier_band: Tuple[float, float] = (0.25, 0.75),
) -> Dict[str, Any]:
    """
    Produce a dictionary of summary stats for an episode.
    You can log this or turn it into features later.
    """
    scores_norm = normalize_scores(episode.scores)
    low, high = frontier_band
    frontier_mask = compute_frontier_mask(scores_norm, low=low, high=high)

    N, M = scores_norm.shape

    # Per-metric stats
    means = scores_norm.mean(axis=0)
    stds = scores_norm.std(axis=0)

    # Frontier coverage
    frontier_fraction = frontier_mask.mean()          # global fraction
    frontier_per_metric = frontier_mask.mean(axis=0)  # per metric

    # Row entropies (treat each row as distribution over metrics)
    row_entropies = np.array([row_entropy(scores_norm[i, :]) for i in range(N)])
    mean_row_entropy = float(row_entropies.mean()) if len(row_entropies) else 0.0
    std_row_entropy = float(row_entropies.std()) if len(row_entropies) else 0.0

    return {
        "episode_id": episode.episode_id,
        "n_items": N,
        "n_metrics": M,
        "metric_names": list(episode.metric_names),
        "mean_per_metric": means,
        "std_per_metric": stds,
        "frontier_fraction": float(frontier_fraction),
        "frontier_per_metric": frontier_per_metric,
        "mean_row_entropy": mean_row_entropy,
        "std_row_entropy": std_row_entropy,
        "frontier_band": (low, high),
    }


def episode_features(
    episode: VisiCalcEpisode,
    frontier_band: Tuple[float, float] = (0.25, 0.75),
) -> Tuple[np.ndarray, List[str]]:
    """
    Turn an episode into a fixed-length feature vector + feature names.

    Current design (for M metrics):
      - mean_per_metric (M)
      - std_per_metric (M)
      - frontier_per_metric (M)
      - frontier_fraction (1)
      - mean_row_entropy (1)
      - std_row_entropy (1)
    => total dim = 3M + 3
    """
    summary = episode_summary(episode, frontier_band=frontier_band)
    metric_names = summary["metric_names"]

    mean_per_metric = np.asarray(summary["mean_per_metric"], dtype=np.float64)
    std_per_metric = np.asarray(summary["std_per_metric"], dtype=np.float64)
    frontier_per_metric = np.asarray(summary["frontier_per_metric"], dtype=np.float64)

    feats = np.concatenate(
        [
            mean_per_metric,
            std_per_metric,
            frontier_per_metric,
            np.array(
                [
                    summary["frontier_fraction"],
                    summary["mean_row_entropy"],
                    summary["std_row_entropy"],
                ],
                dtype=np.float64,
            ),
        ],
        axis=0,
    )

    # Feature names
    feat_names: List[str] = []
    for name in metric_names:
        feat_names.append(f"mean[{name}]")
    for name in metric_names:
        feat_names.append(f"std[{name}]")
    for name in metric_names:
        feat_names.append(f"frontier[{name}]")
    feat_names.extend(
        [
            "frontier_global",
            "mean_row_entropy",
            "std_row_entropy",
        ]
    )

    return feats.astype(np.float32), feat_names


# =========================================================================== #
# 3. Core computation: compute_visicalc_report
# =========================================================================== #


def compute_visicalc_report(
    vpm: np.ndarray,
    metric_names: Sequence[str],
    frontier_metric: str,
    row_region_splits: int,
    frontier_low: float = 0.25,
    frontier_high: float = 0.75,
) -> VisiCalcReport:
    """
    Pure computation: build a VisiCalcReport from a raw score matrix.

    vpm:
        (N_items, N_metrics) score matrix, ideally in [0,1]
    metric_names:
        length-N_metrics list of metric names
    frontier_metric:
        which metric to treat as the "frontier" dimension
    row_region_splits:
        how many contiguous row regions to divide the items into
    frontier_low/high:
        band defining the frontier for coverage stats
    """
    vpm = np.asarray(vpm, dtype=np.float64)
    if vpm.ndim != 2:
        raise ValueError(f"vpm must be 2D, got shape {vpm.shape}")

    num_rows, num_metrics = vpm.shape
    metric_names = list(metric_names)

    if num_rows == 0 or num_metrics == 0:
        report = VisiCalcReport(
            frontier_metric=frontier_metric,
            frontier_low=frontier_low,
            frontier_high=frontier_high,
            row_region_splits=row_region_splits,
            regions=[],
            global_frontier_frac=0.0,
            global_low_frac=0.0,
            global_high_frac=0.0,
        )
        report.validate()
        return report

    # Select frontier column
    try:
        fm_idx = metric_names.index(frontier_metric)
    except ValueError:
        log.warning("Frontier metric '%s' not found; using first metric '%s'",
                    frontier_metric, metric_names[0])
        fm_idx = 0
        frontier_metric = metric_names[0]

    frontier_vals = vpm[:, fm_idx]
    low, high = float(frontier_low), float(frontier_high)

    if not (0.0 <= low < high <= 1.0):
        raise ValueError(f"Invalid frontier band [{low}, {high}]")

    # Global stats over the entire matrix
    g_mean = float(vpm.mean())
    g_std = float(vpm.std())
    g_min = float(vpm.min())
    g_max = float(vpm.max())

    band_mask = (frontier_vals >= low) & (frontier_vals <= high)
    global_frontier_frac = float(band_mask.mean())
    global_low_frac = float((frontier_vals < low).mean())
    global_high_frac = float((frontier_vals > high).mean())

    # Sparsity (approximate "how much of the matrix is near zero")
    sparsity_e3 = float(np.mean(vpm <= 1e-3))
    sparsity_e2 = float(np.mean(vpm <= 1e-2))

    # Entropy of the scalar distribution
    hist, _ = np.histogram(vpm, bins=32, range=(0.0, 1.0), density=True)
    hist = hist[hist > 0]
    if len(hist) > 0:
        entropy = float(-np.sum(hist * np.log(hist)) / np.log(len(hist)))
    else:
        entropy = 0.0

    # Row region bounds: split rows as evenly as possible
    splits = int(row_region_splits)
    base, rem = divmod(num_rows, splits)
    bounds: List[Tuple[int, int]] = []
    start = 0
    for i in range(splits):
        end = start + base + (1 if i < rem else 0)
        bounds.append((start, end))
        start = end

    regions: List[VisiCalcRegion] = []
    for i, (rs, re) in enumerate(bounds):
        if rs >= re:
            regions.append(
                VisiCalcRegion(
                    index=i,
                    start_row=rs,
                    end_row=re,
                    row_count=0,
                    frontier_frac=0.0,
                    low_frac=0.0,
                    high_frac=0.0,
                    mean_frontier_value=0.0,
                )
            )
            continue

        seg_vals = frontier_vals[rs:re]
        seg_mask = band_mask[rs:re]
        regions.append(
            VisiCalcRegion(
                index=i,
                start_row=rs,
                end_row=re,
                row_count=re - rs,
                frontier_frac=float(seg_mask.mean()),
                low_frac=float((seg_vals < low).mean()),
                high_frac=float((seg_vals > high).mean()),
                mean_frontier_value=float(seg_vals.mean()),
            )
        )

    report = VisiCalcReport(
        frontier_metric=frontier_metric,
        frontier_low=low,
        frontier_high=high,
        row_region_splits=splits,
        regions=regions,
        global_frontier_frac=global_frontier_frac,
        global_low_frac=global_low_frac,
        global_high_frac=global_high_frac,
        global_mean=g_mean,
        global_std=g_std,
        global_min=g_min,
        global_max=g_max,
        entropy=entropy,
        sparsity_level_e3=sparsity_e3,
        sparsity_level_e2=sparsity_e2,
    )
    report.validate()
    return report


# =========================================================================== #
# 4. Main Class: VisiCalc
# =========================================================================== #


@dataclass
class VisiCalc:
    """
    Unified analyzer for cognitive metrics over sequences of scored items (e.g., Scorables).

    Turns a list of scored items into:
      - a VisiCalcReport (structured summary)
      - a fixed-length feature vector (for ML models)
      - quality score
      - diff vs another state

    Use Cases:
      - Compare graph states before/after Blossom expansion
      - Measure reasoning coherence in Nexus episodes
      - Track idea evolution in filmstrip visualizations
    """

    episode_id: str
    scores: np.ndarray           # (N_items, N_metrics)
    metric_names: List[str]
    item_ids: List[str]
    frontier_metric: str
    row_region_splits: int = 3
    frontier_low: float = 0.25
    frontier_high: float = 0.75
    meta: Dict[str, Any] = field(default_factory=dict)

    # Computed during __post_init__
    report: Optional[VisiCalcReport] = None
    features: Optional[np.ndarray] = None
    feature_names: Optional[List[str]] = None

    def __post_init__(self):
        scores = np.asarray(self.scores)
        if scores.ndim != 2:
            raise ValueError(f"scores must be 2D, got shape {scores.shape}")
        if len(self.metric_names) != scores.shape[1]:
            raise ValueError("metric_names length mismatch")
        if len(self.item_ids) != scores.shape[0]:
            raise ValueError("item_ids length mismatch")

        self.scores = scores

        # Build derived outputs
        self.report = compute_visicalc_report(
            vpm=self.scores,
            metric_names=self.metric_names,
            frontier_metric=self.frontier_metric,
            row_region_splits=self.row_region_splits,
            frontier_low=self.frontier_low,
            frontier_high=self.frontier_high,
        )
        episode = VisiCalcEpisode(
            episode_id=self.episode_id,
            scores=self.scores,
            metric_names=self.metric_names,
            item_ids=self.item_ids,
            meta=self.meta,
        )
        feats, names = episode_features(episode, frontier_band=(self.frontier_low, self.frontier_high))
        self.features = feats
        self.feature_names = names

    # =============================
    # Public API
    # =============================

    @classmethod
    def from_rows(
        cls,
        *,
        episode_id: str,
        rows: List[Dict[str, Any]],
        frontier_metric: str,
        row_region_splits: int = 3,
        frontier_low: float = 0.25,
        frontier_high: float = 0.75,
        meta: Optional[Dict[str, Any]] = None,
    ) -> "VisiCalc":
        """
        Build from ScorableProcessor-style rows.

        Each row must have:
          - 'scorable_id'
          - 'metrics_columns': List[str]
          - 'metrics_values': List[float]
        """
        if not rows:
            raise ValueError("No rows provided")

        first = rows[0]
        metric_names = list(first.get("metrics_columns") or [])
        if not metric_names:
            raise ValueError("First row missing 'metrics_columns'")

        matrix_rows: List[List[float]] = []
        item_ids: List[str] = []

        for row in rows:
            vals = row.get("metrics_values")
            if not vals:
                continue
            mapping = dict(zip(row.get("metrics_columns", []), vals))
            vec = [float(mapping.get(m, 0.0)) for m in metric_names]
            matrix_rows.append(vec)
            item_ids.append(str(row.get("scorable_id", "unknown")))

        if not matrix_rows:
            raise ValueError("No valid rows with metrics_values")

        X = np.array(matrix_rows, dtype=np.float32)

        fm = frontier_metric if frontier_metric in metric_names else metric_names[0]

        return cls(
            episode_id=episode_id,
            scores=X,
            metric_names=metric_names,
            item_ids=item_ids,
            frontier_metric=fm,
            row_region_splits=row_region_splits,
            frontier_low=frontier_low,
            frontier_high=frontier_high,
            meta=meta or {},
        )
    
    @classmethod
    def from_matrix(
        cls,
        *,
        episode_id: str,
        scores: Union[np.ndarray, Sequence[Sequence[float]]],
        metric_names: Sequence[str],
        item_ids: Sequence[str],
        frontier_metric: Optional[str] = None,
        row_region_splits: int = 3,
        frontier_low: float = 0.25,
        frontier_high: float = 0.75,
        meta: Optional[Dict[str, Any]] = None,
    ) -> "VisiCalc":
        """
        Build a VisiCalc instance directly from a score matrix.

        Args:
            episode_id:
                Logical id for this episode (e.g. "visicalc:targeted").
            scores:
                2D array-like of shape (N_items, N_metrics).
            metric_names:
                Length-N_metrics sequence of metric names (column order).
            item_ids:
                Length-N_items sequence of ids for each row (e.g. scorable_ids).
            frontier_metric:
                Which metric to treat as the 'frontier' dimension.
                If None or not found, falls back to the first metric.
            row_region_splits:
                How many contiguous row regions to divide items into.
            frontier_low / frontier_high:
                Frontier band in normalized [0, 1] space.
            meta:
                Optional extra metadata to carry along.

        Returns:
            VisiCalc instance with .report, .features, .feature_names populated.
        """
        X = np.asarray(scores, dtype=np.float32)
        if X.ndim != 2:
            raise ValueError(f"VisiCalc.from_matrix: scores must be 2D, got shape {X.shape}")

        n_items, n_metrics = X.shape
        metric_names_list = list(metric_names)
        item_ids_list = [str(i) for i in item_ids]

        if len(metric_names_list) != n_metrics:
            raise ValueError(
                f"VisiCalc.from_matrix: metric_names length {len(metric_names_list)} "
                f"!= scores.shape[1] {n_metrics}"
            )

        if len(item_ids_list) != n_items:
            raise ValueError(
                f"VisiCalc.from_matrix: item_ids length {len(item_ids_list)} "
                f"!= scores.shape[0] {n_items}"
            )

        # Resolve frontier metric
        fm = frontier_metric
        if not fm or fm not in metric_names_list:
            if fm is not None:
                log.warning(
                    "VisiCalc.from_matrix: requested frontier_metric=%r not in metric_names=%r; "
                    "falling back to first metric",
                    fm,
                    metric_names_list,
                )
            fm = metric_names_list[0]

        return cls(
            episode_id=episode_id,
            scores=X,
            metric_names=metric_names_list,
            item_ids=item_ids_list,
            frontier_metric=fm,
            row_region_splits=row_region_splits,
            frontier_low=float(frontier_low),
            frontier_high=float(frontier_high),
            meta=meta or {},
        )


    def quality(self, low_weight: float = 0.5, high_weight: float = 0.5) -> float:
        """Scalar quality score for this episode."""
        return self.report.quality_score(low_weight=low_weight, high_weight=high_weight)

    def diff(self, baseline: "VisiCalc") -> Dict[str, Any]:
        """
        Compare this state to a baseline.

        Returns a dict with:
          - 'frontier_metric'
          - 'band'
          - 'target_splits' / 'baseline_splits'
          - 'global_delta'
          - 'regions'
          - 'quality_delta'
          - 'feature_sim'
        """
        report_delta = self.report.diff(baseline.report)

        # Add extra top-level deltas
        quality_delta = self.quality() - baseline.quality()
        # cos-sim between feature vectors
        num = float(np.dot(self.features, baseline.features))
        den = float(np.linalg.norm(self.features) * np.linalg.norm(baseline.features) + 1e-8)
        feature_sim = num / den if den > 0 else 0.0

        report_delta["quality_delta"] = quality_delta
        report_delta["feature_sim"] = feature_sim
        report_delta["target_episode_id"] = self.episode_id
        report_delta["baseline_episode_id"] = baseline.episode_id
        return report_delta

    def save_report(self, path: Union[Path, str]) -> Path:
        """Save just the report to JSON."""
        return self.report.save_json(path)

    def to_vpm_array(
        self,
        *,
        per_metric_normalize: bool = True,
    ) -> np.ndarray:
        """
        Return a uint8 VPM array (N_items × N_metrics) suitable for saving
        as a PNG or feeding into ZeroModel/Tiny vision models.
        """
        return scores_to_vpm_uint8(
            self.scores,
            per_metric_normalize=per_metric_normalize,
        )

    def save_vpm_png(
        self,
        path: Union[Path, str],
        *,
        per_metric_normalize: bool = True,
        mode: str = "L",
    ) -> Path:
        """
        Save the episode's scores as a VPM PNG.

        - mode="L": grayscale (items = vertical axis, metrics = horizontal).
        - mode="RGB": replicate grayscale into 3 channels.
        """
        img_arr = self.to_vpm_array(per_metric_normalize=per_metric_normalize)
        p = Path(path)
        p.parent.mkdir(parents=True, exist_ok=True)

        if mode.upper() == "L":
            img = Image.fromarray(img_arr, mode="L")
        else:
            if img_arr.ndim == 2:
                rgb = np.stack([img_arr] * 3, axis=-1)
            else:
                rgb = img_arr
            img = Image.fromarray(rgb, mode="RGB")

        img.save(p)
        log.info(
            "VisiCalc: saved VPM PNG → %s (shape=%s, mode=%s, per_metric_normalize=%s)",
            p,
            img_arr.shape,
            mode,
            per_metric_normalize,
        )
        return p

    def to_dict(self) -> Dict[str, Any]:
        """Full serializable snapshot of this VisiCalc state."""
        return {
            "episode_id": self.episode_id,
            "meta": self.meta,
            "n_items": len(self.item_ids),
            "n_metrics": len(self.metric_names),
            "metric_names": self.metric_names,
            "frontier_metric": self.frontier_metric,
            "row_region_splits": self.row_region_splits,
            "frontier_low": self.frontier_low,
            "frontier_high": self.frontier_high,
            "quality": self.quality(),
            "report": self.report.to_dict(),
            "features_shape": self.features.shape,
            "feature_names": self.feature_names,
        }

    def pretty(self) -> str:
        """Pretty JSON string for logging / dashboards."""
        return dumps_safe(self.to_dict(), indent=2)


# =========================================================================== #
# 5. Backward compatibility / small helpers
# =========================================================================== #


def graph_quality_from_report(report: Any, **kwargs) -> float:
    """
    Convenience shim:
      - if 'report' is a VisiCalcReport, call its .quality_score()
      - if it's a dict, reconstruct a VisiCalcReport first
    """
    if isinstance(report, VisiCalcReport):
        return report.quality_score(**kwargs)
    return VisiCalcReport.from_dict(report).quality_score(**kwargs)
